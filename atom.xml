<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一期一会</title>
  <icon>https://www.gravatar.com/avatar/82f6d8814fca7e2e77b918b4c5e2f486</icon>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.jonnydu.me/"/>
  <updated>2020-05-01T14:24:32.469Z</updated>
  <id>http://blog.jonnydu.me/</id>
  
  <author>
    <name>jonnydu</name>
    <email>jonnydu.uestc@gmail.com</email>
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>hexo博客迁移新电脑</title>
    <link href="http://blog.jonnydu.me/2019/10/07/hexo-move-to-new-pc/"/>
    <id>http://blog.jonnydu.me/2019/10/07/hexo-move-to-new-pc/</id>
    <published>2019-10-07T12:51:53.000Z</published>
    <updated>2020-05-01T14:24:32.469Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍如何将hexo博客迁移到新电脑。</p><a id="more"></a><p>刚入手一台mac，之前的windows要慢慢休息了，包括在上面搭建的hexo博客环境也要开始搬家到mac。</p><h3 id="总体思路"><a href="#总体思路" class="headerlink" title="总体思路"></a>总体思路</h3><p>迁移之路总体包含三步：</p><ul><li>首先在新电脑上安装<code>hexo</code>，并初始化根目录；</li><li>然后生成新的<code>SSH key</code>，并将其添加到<code>github</code>；</li><li>将旧电脑的文件、配置通过github或其他方式复制到新电脑的对应目录。</li></ul><h3 id="具体操作"><a href="#具体操作" class="headerlink" title="具体操作"></a>具体操作</h3><h4 id="新电脑安装hexo依赖环境"><a href="#新电脑安装hexo依赖环境" class="headerlink" title="新电脑安装hexo依赖环境"></a>新电脑安装<code>hexo</code>依赖环境</h4><ol><li><strong>安装node/git</strong></li></ol><ul><li>使用homebrew安装，<code>brew install git</code>，<code>brew install node</code></li><li>每次使用homwbrew安装时，它总是会先更新自己：<code>updating homebrew</code>，由于某些不可描述的原因，这个过程默认比较慢，可以搞个科学上网和国内的镜像加下速。</li></ul><ol start="2"><li><strong>安装hexo</strong></li></ol><ul><li>使用node的npm安装hexo：<code>npm install hexo</code></li></ul><ol start="3"><li><strong>初始化hexo</strong></li></ol><ul><li>新建一个名为hexo的文件夹作为根目录，然后在该目录下初始化：<code>hexo init</code></li><li>测试是否成功，执行<code>hexo s</code>，然后在浏览器打开：<code>localhost:4000</code>，查看本地博客。</li></ul><h4 id="github验证配置"><a href="#github验证配置" class="headerlink" title="github验证配置"></a>github验证配置</h4><ol><li><strong>生成SSH key</strong></li></ol><ul><li>生成SSH key：<code>ssh-keygen -t rsa xxx@xxx.com</code></li></ul><ol start="2"><li><strong>添加SSH key至github</strong></li></ol><ul><li><code>cd ~/.ssh</code>进入.ssh文件夹，然后打开里面的id_rsa.pub文件夹，里面的内容就是SSH key，复制全部内容；</li><li>网页打开github的设置：<code>Settings-&gt;SSH and GPG keys</code>，点击按钮<code>New SSH key</code>，然后在输入框输入上一步复制的内容；</li><li>保存后，github会给注册邮箱发送一个验证链接，需登录邮箱验证，否则后续的部署<code>hexo d</code>会一直不成功；</li><li>最后测试一下是否配置成功：<code>ssh -T git@github.com</code></li></ul><h4 id="文件配置转移"><a href="#文件配置转移" class="headerlink" title="文件配置转移"></a>文件配置转移</h4><p>需要转移的文件和配置包括：</p><ul><li>_config.yml</li><li>scaffolds</li><li>source</li><li>themes</li></ul><p>文件配置转移有两种方式：</p><ul><li>通过U盘或移动硬盘拷贝转移<blockquote><p>这种方式比较简单粗暴，直接拷贝，但是缺点也很明显，本地存储，每次换设备都得迁移</p></blockquote></li><li>通过github分支转移<blockquote><p>这种方式比较现代化，通过github建立分支来分别保存静态和动态文件，建议选择该方式。参考博客：<a href="https://blog.csdn.net/white_idiot/article/details/80685990" target="_blank" rel="noopener">创建Git分支将Hexo博客迁移到其它电脑</a><br>按照这种方式配置后，github的分支图如下，master分支存放<code>hexo d</code>生成的静态文件，而分支存放提交的文章markdown等文件，<br><img src="http://ww1.sinaimg.cn/large/c54778a7gy1ged8je7towj20hc04at8p.jpg" alt="hexo-github-branch.jpg"></p></blockquote></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍如何将hexo博客迁移到新电脑。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="hexo" scheme="http://blog.jonnydu.me/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>k8s实践系列---从Docker到k8s</title>
    <link href="http://blog.jonnydu.me/2019/08/12/k8s-practice/"/>
    <id>http://blog.jonnydu.me/2019/08/12/k8s-practice/</id>
    <published>2019-08-12T14:56:20.000Z</published>
    <updated>2020-04-30T14:16:30.281Z</updated>
    
    <content type="html"><![CDATA[<p>在容器化和云原生技术大行其道的时代，特推出《k8s实践》系列文章，就云原生实践过程中的一些经验与大家分享。本文为第一篇：从Docker到k8s。</p><a id="more"></a><p>[TOC]</p><h2 id="传统模式存在的问题"><a href="#传统模式存在的问题" class="headerlink" title="传统模式存在的问题"></a>传统模式存在的问题</h2><p>传统基于虚拟机构建的服务发布、管控平台主要存在以下问题：<br><img src="http://km.oa.com/files/photos/pictures/201906/1561283957_74_w1208_h497.png" alt="传统模式的问题"><br>1、发布繁琐：服务发布所涉及的代码编译–&gt;应用打包–&gt;版本升级的整个过程，都需要开发人员介入，缺乏一套自动化的流水线；<br>2、人工扩缩容：在一些大型运营活动、突发事件等流量突增的场景下，需要开发及运维人员介入进行手动扩缩容，整个过程显得比较笨拙和滞后；<br>3、故障无自愈：线上服务或节点出现故障时，平台无有效的自愈能力；<br>4、机器裁撤：物理机裁撤时，不能做到优雅的迁移线上服务；<br>5、资源利用率低：由于虚拟机对操作系统进行了虚拟化，这使得虚拟机的隔离性好，但资源利用率比较低；<br>6、环境依赖：比如常见的gcc版本不匹配导致的代码编译不过，程序运行异常等问题；<br>这些问题极大地降低了我们的开发和运维效率，甚至是影响线上服务质量，因此，我们迫切地需要更加智能、自动化的服务管控平台。</p><hr><h2 id="Docker初出茅庐"><a href="#Docker初出茅庐" class="headerlink" title="Docker初出茅庐"></a>Docker初出茅庐</h2><h3 id="Docker的核心贡献"><a href="#Docker的核心贡献" class="headerlink" title="Docker的核心贡献"></a>Docker的核心贡献</h3><p>Docker本质上只是一个使用容器技术实现的“沙盒”而已，通过操作系统中的Namespace和Cgroups机制为每一个应用创建一个独立的“沙盒”，然后在“沙盒”中启动这些应用进程。但Docker能迅速崛起并占领容器市场的根本原因在于Docker镜像。在Docker之前，PaaS(Platform as a Service)上部署应用最麻烦的问题在于应用程序的打包，整个过程毫无章法可循，基本上靠不断试错。很多时候在本地运行正常的应用，却需要做很多修改和配置才能在PaaS里运行起来。<br>Docker镜像解决的核心就是打包这个根本性问题。所谓Docker镜像，本质上就是一个压缩包，但是这个压缩包里的内容，比PaaS的可执行文件+启停脚本要丰富很多，实际上，大多数Docker镜像是直接由一个完整操作系统的所有文件和目录组成的，所以这个压缩包里的环境和本地开发、测试的环境是完全一样的。因此，Docker这种便利的打包机制，保证了服务本地环境和云端环境的高度一致，避免了用户通过不断试错来调试环境的痛苦过程。</p><h3 id="Docker依赖的核心技术"><a href="#Docker依赖的核心技术" class="headerlink" title="Docker依赖的核心技术"></a>Docker依赖的核心技术</h3><h4 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a>Namespace</h4><p>Namespace是Linux提供的一种内核级别的资源隔离机制，是Linux创建新进程的一个可选参数，每个Namespace下的资源相对于其他Namespace都是透明、不可见的。目前Linux系统提供了IPC、PID、Mount、UTS、Network、User、Cgroup等七个资源隔离类型。<br><img src="http://km.oa.com/files/photos/pictures/201906/1561284030_18_w683_h385.png" alt="Namespace"><br>Namespace是Docker容器中用来实现“隔离”的技术手段，Namespace技术实际上是修改了进程看待整个计算机的“视图”，即是被操作系统限制了只能看到某些指定的内容，但对于宿主机来说这些被隔离的进程与其他进程是没有差别的。所以，docker的最基本原理实际上就是在创建容器进程时，指定了这个进程所需要的一组Namespace参数，这样容器就只能看到当前Namespace所限定的资源、文件、设备、配置等。因此，这也印证了docker容器的本质其实是一种特殊的进程。<br>比如我们使用<code>docker exec -it 4ddf5638583c /bin/bash</code>命令进入到<code>4ddf5638583c</code>这一docker容器中，其原理是通过把<code>/bin/bash</code>进程加入到<code>4ddf5638583c</code>容器进程的Namespace中，这样我们在shell中看到的就是<code>4ddf5638583c</code>容器进程的视图。</p><h4 id="Cgroups"><a href="#Cgroups" class="headerlink" title="Cgroups"></a>Cgroups</h4><p>既然Docker容器进程只是宿主机上的一个应用进程，那必然存在和宿主机上的其他进程竞争资源的问题。这个问题可以通过Linux Cgroups来解决。Linux Cgroups的全称是Linux Control Group。它最主要的作用是限制一个进程组能够使用的资源上限，包括CPU、内存、磁盘、网络带宽等等。<br>在Linux中Cgroups暴露给用户的接口是文件系统，它以文件和目录的形式组织在操作系统的<code>/sys/fs/cgroup</code>路径下，使用<code>mount -t cgroup</code>可以查看Cgroups的挂载目录，同时可以看到<code>/sys/fs/cgroup</code>目录下有很多子目录如<code>cpu</code>、<code>cpuset</code>、<code>memory</code>等，这些表示当前可用被Cgroups限制的资源类型。<br>下面通过一个简单的测试来了解Cgroups是如何限制进程资源使用的。<br><img src="http://km.oa.com/files/photos/pictures/201906/1561284053_34_w1476_h341.png" alt="Cgroups"><br>1、在<code>/sys/fs/cgroup/cpu</code>目录下创建一个名为<code>mytest</code>的控制组；<br>2、初始时<code>cpu.cfs_period_us=1</code>,代表CPU时间片使用没有上限，测试中的进程为shell命令<code>while : ; do : ; done &amp;</code>，使用top命令发现当前进程直接把CPU跑到了100%；<br>3、然后修改<code>cpu.cfs_quota_us</code>文件，让进程只能占用30%的CPU时间片<code>echo 30000 &gt; cpu.cfs_quota_us</code>（默认<code>cpu.cfs_period_us</code>值为100000表示100ms，设置30000表示30ms占比30%）；<br><img src="http://km.oa.com/files/photos/pictures/201906/1561284075_47_w839_h84.png" alt=""><br>4、再次使用top命令查看，发现CPU利用率为30%。       </p><h4 id="rootfs"><a href="#rootfs" class="headerlink" title="rootfs"></a>rootfs</h4><p>为了让容器的根目录看起来更真实，我们一般会在这个容器的根目录下挂载一个完整操作系统的文件系统，比如Ubuntu 16.04的镜像。这样在容器启动之后，我们在容器里通过执行<code>“ls /”</code>查看根目录下的内容，就是Ubuntu 16.04的所有目录和文件。<br>这个挂载在容器根目录、用来为容器进程提供隔离后执行环境的文件系统，就是“容器镜像”，也被称为“rootfs（根文件系统）”。rootfs只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。在Linux系统中，这两部分是分开存放的，所以容器镜像其实是打包了操作系统相关的一些文件，并且会对一些不必要的内容进行裁剪，所以docker镜像本身是不大的。此外，由于rootfs里打包的不只是服务，而是整个操作系统的文件和目录，这意味着服务以及它运行所需的环境依赖都被打包在一起，从而解决了环境依赖问题。<br>在rootfs的基础上，Docker的镜像设计创造性地提出了使用多个增量rootfs联合挂载一个完整rootfs的方案，即镜像层的概念。用户制作镜像的每一步操作，都会生成一个层，也就是一个增量rootfs，然后通过联合文件系统（Union File System）将多个不同位置的目录联合挂载到同一个目录。<br><img src="http://km.oa.com/files/photos/pictures/201906/1561355016_97_w659_h333.png" alt=""><br>一个容器的rootfs主要由三部分组成：<br>1、只读层：如上图中rootfs最下面的四层，可以看到，它们的挂载方式都是只读的（<code>readonly+whiteout</code>）。<br>2、可读写层：如上图中rootfs最上面的一层，它的挂载方式为<code>read write</code>，在没有写入文件之前，这个目录是空的，而一旦在容器里做了写操作，我们修改所产生的文件就会已增量的方式出现在可读写层中。此外，为了实现对可读层中文件的删除操作，Union FS会在可读写层创建一个whiteout文件，把只读层里的文件“遮挡”起来，这样当两个层被联合挂载的时候，只读层的文件就被“删除”了。<br>3、Init层是Docker项目单独生成的一个内部层，夹在只读层和可读写层之间，专门用来存放/etc/hosts、/etc/resolv.conf等信息。因为用户往往需要在启动容器时写入一些指定的值如hostname，所以需要在可读写层对它们进行修改，但是这些修改往往只对当前容器有效，我们不希望执行docker commit时这些信息连同可读写层一起被提交。因此Docker的做法是在修改了这些文件之后，以一个单独的层挂载出来。         </p><h3 id="Docker小结"><a href="#Docker小结" class="headerlink" title="Docker小结"></a>Docker小结</h3><p>总的来说，一个docker容器，实际上是由一个Linux Namespace、Linux Cgroups和rootfs三种技术构建出来的进程隔离环境。更直观地讲，一个正在运行的Linux容器，可以被“一分为二”地看待：      </p><ol><li>一组联合挂载在<code>/var/lib/docker/aufs/mnt</code>上的rootfs，这一部分我们成为docker镜像，是docker容器的静态视图；</li><li>一个由Namespace+Cgroups构成的隔离环境，这一部分我们称为“容器运行时”（Container Runtime），是容器的动态视图。                 </li></ol><hr><h2 id="从Docker到Kubernetes"><a href="#从Docker到Kubernetes" class="headerlink" title="从Docker到Kubernetes"></a>从Docker到Kubernetes</h2><h3 id="容器编排与部署问题"><a href="#容器编排与部署问题" class="headerlink" title="容器编排与部署问题"></a>容器编排与部署问题</h3><p>通过上述对Docker的介绍不难发现，Docker对于个人开发者开发“单机版”容器化应用来说，确实已经足够好用了，无论是资源利用率还是开发效率都大为改善。但是如果在企业级应用场景中，很快就会因为容器规模的增加导致Docker手动操作方式的弊端不断被放大。因此，如何批量创建、调度和管理容器就成了制约Docker技术在企业级应用场景的主要障碍，即容器的编排与部署问题。</p><h3 id="Kubernetes简介及优势分析"><a href="#Kubernetes简介及优势分析" class="headerlink" title="Kubernetes简介及优势分析"></a>Kubernetes简介及优势分析</h3><p>Kubernetes（简称k8s）是由Google开源的容器编排工具，是Google大规模容器管理系统Borg的开源版本实现，吸收借鉴了Google过去十年间在生产环境上所学到的经验与教训。Docker开发公司在容器编排上提供了Swarm解决方案，但k8s更灵活，功能更强大，社区发展更活跃，已经成为企业级容器编排工具的首选。<br><img src="http://km.oa.com/files/photos/pictures/201906/1561284411_46_w342_h312.png" alt="k8s"><br>简单来说，k8s是一个管理跨主机容器化应用的系统，实现了包括编排、部署、高可用管理和弹性伸缩在内的一系列基础功能并将其封装为一套简单易用的RESTful API（或是基于RESTful API封装的命令行工具）对外提供服务。k8s通过各种调度器和控制器进程来维护容器集群一直处于用户所期望的运行状态，并基于此建立了一套完整的集群自愈机制，包括容器的自动重启、自动调度和自动备份等。<br>但是，以上的这些功能不是k8s的核心竞争力，因为类似于Fleet或者Mesos这样的平台也提供了相差无几的能力。k8s着重瞄准并解决的问题是运行在大规模集群中的各任务之间的复杂关系处理，而这也被认为是作业编排和管理系统最困难的地方。因此，k8s项目从一开始就没有像同时期的各种“容器云”项目那样，把Docker作为整个架构的核心，而仅仅把它作为最底层的一个容器运行时实现，k8s主要针对的服务对象是由多个容器组合而成的复杂应用，如弹性、分布式的服务架构，为此，k8s引入了专门队容器进行分组管理的Pod，从而建立了一套将非容器化应用平滑地迁入到容器云上的机制。可以说，整个k8s的设计思想都是建立在Pod这个可以视作单个容器的“容器组”展开的，当容器组代替容器成为系统的部署和调度粒度时，所构建出来的应用组织方式就和Fleet和Mesos有很大的区别。<br>因此，k8s最核心的设计思想和优势是，以统一的方式来定义和处理各任务之间的复杂关系，并且为将来支持更多种类的关系留有余地。            </p><h3 id="在Kubernetes上部署第一个应用"><a href="#在Kubernetes上部署第一个应用" class="headerlink" title="在Kubernetes上部署第一个应用"></a>在Kubernetes上部署第一个应用</h3><p>最后，我们通过一个简单的示例来看看如何在k8s集群上部署应用。本文不对k8s集群的搭建做阐述，假设应用部署时已有自建或云厂商提供的容器云平台。在示例中，我们希望k8s来完成Nginx容器镜像的部署和管理，此外，我们还需要平台帮我们运行两个完全相同的Nginx容器副本，以负载均衡的方式共同对外提供服务。<br>下面的yaml配置文件定义了一个Deployment（标准无状态应用，帮助我们创建和管理完全相同的各实例Pod，本系列后续文章会详述），其中包含两个Nginx Pod：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-deployment</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span><span class="comment"># 副本数为2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx-server</span>    <span class="comment"># 这个 Deployment 管理着那些拥有这个标签的 Pod </span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx-server</span>  <span class="comment"># 为所有 Pod 都打上这个标签</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.7.9</span><span class="comment"># nginx镜像</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><p>接着创建这个Deployment：    </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f nginx-deployment.yaml</span><br><span class="line">$ kubectl get deployments</span><br><span class="line">NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx-deployment   2         2         2            2           5s</span><br></pre></td></tr></table></figure><p>这样，两个完全相同的Nginx容器副本就被启动了，可以看到，这个Deployment管理着两个Nginx Pod：    </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods</span><br><span class="line">NAME                                READY     STATUS    RESTARTS   AGE</span><br><span class="line">nginx-deployment-2558053219-0g590   1&#x2F;1       Running   0          2m</span><br><span class="line">nginx-deployment-2558053219-220xk   1&#x2F;1       Running   0          2m</span><br></pre></td></tr></table></figure><p>从此，我们可以通过管理这个Deployment来实现多副本、服务高可用、版本更新与回滚、自动扩缩容等等能力。</p><hr><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文为《k8s实践》系列文章第一篇：从Docker到k8s，主要介绍了传统基于虚拟机构建的服务发布和管控平台存在的问题、Docker的核心贡献和关键技术以及从Docker到Kubernetes的必然演进。         </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在容器化和云原生技术大行其道的时代，特推出《k8s实践》系列文章，就云原生实践过程中的一些经验与大家分享。本文为第一篇：从Docker到k8s。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="k8s" scheme="http://blog.jonnydu.me/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>k8s知识图谱</title>
    <link href="http://blog.jonnydu.me/2019/07/27/k8s-knowledge/"/>
    <id>http://blog.jonnydu.me/2019/07/27/k8s-knowledge/</id>
    <published>2019-07-27T11:26:20.000Z</published>
    <updated>2020-04-30T14:16:30.274Z</updated>
    
    <content type="html"><![CDATA[<p>k8s技能升级中</p><a id="more"></a><p><img src="http://km.oa.com/files/photos/pictures/201905/1557659219_89_w2614_h1825.png" alt=""><br>地址：<a href="https://www.processon.com/view/link/5cd7fb4ee4b07a8b07682787#map" target="_blank" rel="noopener">https://www.processon.com/view/link/5cd7fb4ee4b07a8b07682787#map</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;k8s技能升级中&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="k8s" scheme="http://blog.jonnydu.me/tags/k8s/"/>
    
  </entry>
  
  <entry>
    <title>Logging、Metrics和Tracing</title>
    <link href="http://blog.jonnydu.me/2018/12/15/Logging-Tracing-Metrics/"/>
    <id>http://blog.jonnydu.me/2018/12/15/Logging-Tracing-Metrics/</id>
    <published>2018-12-15T11:26:20.000Z</published>
    <updated>2020-04-30T14:16:30.261Z</updated>
    
    <content type="html"><![CDATA[<p>近来研究了一系列面向DevOps的诊断与分析系统，并在部门结合业务场景逐步落地。本文记录了该过程中的一些思考和总结。</p><a id="more"></a><h3 id="三类系统的诞生"><a href="#三类系统的诞生" class="headerlink" title="三类系统的诞生"></a>三类系统的诞生</h3><p>随着DevOps、微服务、容器等理念的逐步落地和发展，传统软件架构正在发生着较大的变化，其主要体现在：</p><ul><li>应用架构开始从单体系统逐步转变为微服务，其中的业务逻辑随之会变成微服务之间的调用与请求。</li><li>从资源的角度来，服务器这个物理单元正在淡化，取而代之的是看不见摸不着的虚拟资源模式。</li></ul><p><img src="http://km.oa.com/files/photos/pictures/201810/1538896485_13_w694_h347.png" alt=""></p><p>以上两个变化使得分布式系统的运维与监控变得越来越复杂。为了应对这一趋势，诞生了一系列面向DevOps的诊断与监控系统，包括三大类：</p><ul><li>集中式日志系统（<strong>Logging</strong>）</li><li>集中式度量系统（<strong>Metrics</strong>）</li><li>分布式追踪系统（<strong>Tracing</strong>）</li></ul><h3 id="一些疑问"><a href="#一些疑问" class="headerlink" title="一些疑问"></a>一些疑问</h3><p>对Logging、Metrics和Tracing三种系统有了初步认识和了解之后，我对它们的关系产生了疑问：</p><ul><li>Logging和Metrics的区别是什么？</li><li>什么时候需要Metrics？</li><li>Tracing和Logging之间有什么关联？</li><li>三种系统各自解决的主要问题是什么？有无交集？</li></ul><h3 id="一些思考"><a href="#一些思考" class="headerlink" title="一些思考"></a>一些思考</h3><p><img src="http://km.oa.com/files/photos/pictures/201810/1538897894_21_w392_h400.jpg" alt=""></p><ul><li>Logging是以<strong>事件</strong>为元数据，记录某个时刻（离散时间）发生的事件，比如将应用的调试或错误信息发送到ElasticSearch，跟踪时间信息通过消息队列处理发送到数据仓库等。它的特点是大多数情况下记录的数据很分散，并且相互独立，也许是错误信息，也许仅仅是记录当前的事件状态等，这是Logging的关注属性。</li><li>Metrics有其特有的属性—<strong>可聚合性</strong>。比如我们想知道服务请求的QPS，用户的登录次数等指标，这时我们需要将一部分事件进行聚合或计数，这就是Metrics。它是一段时间内某个度量（计数器或直方图）的元数据。</li><li>在记录Tracing数据的时候，或多或少会有Logging的数据，比如在Tracing中的链路数据指标（rpc调用，请求处理时间是多少等），同样会被记录在Logging中，但Tracing同样也有自己独特的属性—<strong>请求范围</strong>，即可以绑定到系统单个“事务”对象的生命周期的任何元数据。例如一次HTTP请求的tracing，我们只需要关注请求到达时的节点状态，接收方和处理耗时等，不用关心具体的系统日志、错误信息等Logging事件。</li></ul><p>因此，根据上述的描述，我们可以标记上图中三者的重叠部分。<br><img src="http://km.oa.com/files/photos/pictures/201810/1538899025_24_w676_h400.png" alt=""><br>当然，大量被诊断的应用具有分布式的能力，逻辑处理在单次请求的范围内完成。因此，讨论追踪的上下文是有意义的。但是，我们注意到，并不是所有的诊断系统都绑定在请求的生命周期上的。他们可能是逻辑组件诊断信息、处理过程的生命周期明细。所以，不是所有的metrics和logging都可以塞进tracing系统的概念中，至少在不经过数据处理是不行的。<br>此外，上图中我们还能看到，在三个功能域中，metric倾向于更节省资源，因为它会“天然”压缩数据。相反，日志倾向于无限增加，会频繁的超出预期的容量，所以，图中纵向绘制了容量的需求趋势，metrics低到logging高。</p><h3 id="业界代表性系统"><a href="#业界代表性系统" class="headerlink" title="业界代表性系统"></a>业界代表性系统</h3><ul><li><strong>ELK</strong><ul><li>简介：专注于<strong>Logging</strong>领域，由ElasticSearch、Logstash和Kibana三款软件组成的一整套解决方案，提供了日志的收集、传输、存储和分析等功能。该架构由Logstash分布于各个节点上搜集相关日志、数据，并经过分析、过滤后发送给远端服务器上的ElasticSearch进行存储，用户通过直观地配置Kibana对日志进行可视化查询、生成报表等。该架构现已在部门内落地。</li><li>官网：<a href="https://www.elastic.co/cn/elk-stack" target="_blank" rel="noopener">https://www.elastic.co/cn/elk-stack</a></li></ul></li><li><strong>Prometheus</strong><ul><li>简介：主要聚焦<strong>Metrics</strong>领域（正在集成更多的tracing功能），既适用于面向服务器等硬件指标的监控，也适用于高动态的面向服务架构的监控。Prometheus的基本原理是通过HTTP协议周期性抓取被监控组件的状态，任意组件只要提供对应的HTTP接口就可以介入监控，不需要任何SDK或者其他的集成过程。</li><li>官网：<a href="https://prometheus.io/" target="_blank" rel="noopener">https://prometheus.io/</a></li></ul></li><li><strong>Zipkin</strong><ul><li>简介：专注于<strong>Tracing</strong>领域，根据Google Dapper论文设计的全链路监控系统，由Twitter公司开发。每个服务向Zipkin报告及时数据，Zipkin会根据调用关系通过UI生成依赖关系图，显示有多少跟踪请求通过每个服务，该系统让开发者可通过Web轻松的收集和分析数据，例如用户每次请求服务的处理时间，可方便的检测系统中的瓶颈。</li><li>官网：<a href="https://zipkin.io/" target="_blank" rel="noopener">https://zipkin.io/</a></li></ul></li></ul><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://peter.bourgon.org/blog/2017/02/21/metrics-tracing-and-logging.html" target="_blank" rel="noopener">https://peter.bourgon.org/blog/2017/02/21/metrics-tracing-and-logging.html</a><br><a href="https://yq.aliyun.com/articles/394183" target="_blank" rel="noopener">https://yq.aliyun.com/articles/394183</a><br><a href="https://techbeacon.com/monitoring-demystified-top-logging-tracing-metrics-resources" target="_blank" rel="noopener">https://techbeacon.com/monitoring-demystified-top-logging-tracing-metrics-resources</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;近来研究了一系列面向DevOps的诊断与分析系统，并在部门结合业务场景逐步落地。本文记录了该过程中的一些思考和总结。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="DevOps" scheme="http://blog.jonnydu.me/tags/DevOps/"/>
    
  </entry>
  
  <entry>
    <title>golang中channel的属性分析</title>
    <link href="http://blog.jonnydu.me/2018/09/15/golang-channel/"/>
    <id>http://blog.jonnydu.me/2018/09/15/golang-channel/</id>
    <published>2018-09-15T11:26:20.000Z</published>
    <updated>2020-04-30T14:16:30.262Z</updated>
    
    <content type="html"><![CDATA[<p>我将golang中的channel理解为信号，即channel允许一个goroutine向另一个goroutine发出某些特定事件的信号。</p><a id="more"></a><p><strong>信号是理解golang中使用channel做一切工作的核心。</strong><br>本文将从channel的三个关键属性出发来理解信号机制是如何工作的。这三个属性分别是：<br><strong>1、可靠性传递<br>2、状态<br>3、是否含有数据</strong><br>下面将结合图示和代码分别对其进行分析。</p><h2 id="可靠性传递"><a href="#可靠性传递" class="headerlink" title="可靠性传递"></a>可靠性传递</h2><p>属性“可靠性传递”是基于信号传递中的一个常见问题：“我如何得知一个goroutine发出的特定信号已被正确接收？”</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="number">2</span>p := &lt;-ch</span><br><span class="line"><span class="number">3</span> &#125; ()</span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">5</span> ch &lt;- <span class="string">"paper"</span></span><br></pre></td></tr></table></figure><p>以上面的代码为例，我们可以根据发送信号的goroutine（第5行）是否需要（<strong>立即</strong>）得到接收信号的goroutine（第2行）的<strong>确认</strong>（Guaranteed）来将channel划分为Unbuffered（无缓冲的）和Buffered（带缓冲的）两种类型，它们针对“可靠性传递”这一属性提供了不同表现。如下图所示。<br><img src="http://km.oa.com/files/photos/pictures/201806/1529824533_88_w681_h152.jpg" alt=""><br>“带确认的可靠性传递”在很多场景下是非常重要的，这里先做简单说明，后文还将更详细地介绍。</p><h2 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h2><p>channel的行为受其当前所处状态的影响，channel可能处于的状态包括open，closed和nil。<br>下面的代码实例中展示了三种可能的状态：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> ch <span class="keyword">chan</span> <span class="keyword">string</span><span class="comment">//nil state</span></span><br><span class="line">ch = <span class="literal">nil</span><span class="comment">//nil state</span></span><br><span class="line">ch := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">string</span>)   <span class="comment">//open state</span></span><br><span class="line"><span class="built_in">close</span>(ch)<span class="comment">//closed state</span></span><br></pre></td></tr></table></figure><p>channel所处的状态决定了对其进行send和receive操作时的表现（如前文所述，我们将channel理解为信号的行为，因此将操作称为send和receive，而不说read和write）。如下图所示。<br><img src="http://km.oa.com/files/photos/pictures/201806/1529825902_47_w908_h227.jpg" alt=""><br>（1）当一个channel处于nil状态时，任何试图对channel进行的send/receive操作都将被阻塞；<br>（2）当一个channel处于open状态时，信号可以在channel中被发送和接收；<br>（3）当一个channel处于closed状态时，不能再发送信号，<strong>但仍可能接收信号</strong>。</p><h2 id="是否含有数据"><a href="#是否含有数据" class="headerlink" title="是否含有数据"></a>是否含有数据</h2><p>最后一个需要考虑的属性是信号中是否含有数据。<br>当信号中含有数据时（<code>ch &lt;- &quot;paper&quot;</code>），通常发生在：<br>（1）一个goroutine需要开启一项新任务；<br>（2）一个goroutine需要反馈执行结果。<br>而当信号中不含有数据时（<code>close(ch)</code>），通常发生在：<br>（1）一个goroutine需要被停止；<br>（2）一个goroutine需要反馈其已经完成或关闭了处理。</p><h3 id="含有数据的信号"><a href="#含有数据的信号" class="headerlink" title="含有数据的信号"></a>含有数据的信号</h3><p>当使用含有数据的信号时，我们可以根据需要得到的确认类型来选择channel的配置，可选的三个配置分别是Unbuffered、Buffered&gt;1和Buffered=1，其特点如下图所示。<br><img src="http://km.oa.com/files/photos/pictures/201806/1529827175_43_w908_h152.jpg" alt=""><br>（1）Unbuffered channel：提供了可靠性确认，确认发送的信号已被接收，因为信号的接收发生在信号发送完成之前；<br>（2）Buffered&gt;1：不能保证发送的信号已经被接收，因为信号的发送在信号接收完成之前结束；<br>（3）Buffered=1：提供了延迟确认，可以保证先前发送的信号已经被接收，因为第一个信号的接收发生在第二个信号发送完成之前。</p><h3 id="不含有数据的信号"><a href="#不含有数据的信号" class="headerlink" title="不含有数据的信号"></a>不含有数据的信号</h3><p>不含有数据的信号主要表现为取消操作，它允许一个goroutine向另外的goroutine发出信号来停止或继续它们正在做的事情。该操作可以通过Buffered和Unbuffered两种channel实现，但相对来说Buffered channel更常见一些。<br>不含有数据信号操作的一个常见例子是内置函数close。如前文所述，我们仍然可以在close后的channel上接收信号，并且不会阻塞，总是会返回。</p><h2 id="实际应用场景"><a href="#实际应用场景" class="headerlink" title="实际应用场景"></a>实际应用场景</h2><p>基于以上对channel三个关键属性的总结，接下来通过实际代码中的一些场景来对channel做进一步理解。在下面的几个例子中，我们将goroutine抽象成人，将channel抽象成人与人之间的信号传递。</p><h3 id="带确认-含有数据-Unbuffered-Channel"><a href="#带确认-含有数据-Unbuffered-Channel" class="headerlink" title="带确认+含有数据+Unbuffered Channel"></a>带确认+含有数据+Unbuffered Channel</h3><p>我们通过两个实际场景来说明该类型Channel的工作机制。</p><h4 id="场景1：等待任务"><a href="#场景1：等待任务" class="headerlink" title="场景1：等待任务"></a>场景1：等待任务</h4><p>想象一位经理面试一位新雇员的场景。该场景下，经理希望雇员做一些试题以检测其工作能力，雇员需要拿到经理递去的正式答题纸后方可开始作答。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">01</span> <span class="function"><span class="keyword">func</span> <span class="title">waitForTask</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="number">02</span> ch := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="number">03</span> </span><br><span class="line"><span class="number">04</span> <span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="number">05</span>p := &lt;-ch</span><br><span class="line"><span class="number">06</span><span class="comment">//新雇员开始答题</span></span><br><span class="line"><span class="number">07</span><span class="comment">//新雇员答题后离开</span></span><br><span class="line"><span class="number">08</span> &#125;()</span><br><span class="line"><span class="number">09</span> time.Sleep(time.Duration(rand.Intn(<span class="number">500</span>)) * time.Millisecond)</span><br><span class="line"><span class="number">10</span> ch &lt;- <span class="string">"paper"</span></span><br><span class="line"><span class="number">11</span> &#125;</span><br></pre></td></tr></table></figure><p>在第02行，程序创建了一个带string数据的Unbuffered channel，然后在第04行，新雇员在开始答题之前被告知在05行等待经理的正式答题纸，这将使得雇员在等待时阻塞。而一旦雇员收到公司的正式答题纸，他就可以开始作答，并在完成之后离开。而经理与雇员在并发工作，当准备好正式答题纸之后，经理向雇员发出信号，即第10行的数据“paper”，由于使用的是Unbuffered channel，可以保证一旦发送操作完成后，新雇员就立即收到了数据“paper”。</p><h4 id="场景2：等待结果"><a href="#场景2：等待结果" class="headerlink" title="场景2：等待结果"></a>场景2：等待结果</h4><p>在接下来这个场景中，情况发生了变化。这一次，经理希望新雇员在被雇佣时立即执行答题（所有必备材料已准备好），而经历需要等待雇员的答题结果，待其完成之后，再进行自己的工作。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">01</span> <span class="function"><span class="keyword">func</span> <span class="title">waitForResult</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="number">02</span>ch := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">string</span>)</span><br><span class="line"><span class="number">03</span></span><br><span class="line"><span class="number">04</span><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="number">05</span>time.Sleep(time.Duration(rand.Intn(<span class="number">500</span>)) * time.Millisecond)  <span class="comment">//雇员答题</span></span><br><span class="line"><span class="number">06</span></span><br><span class="line"><span class="number">07</span>ch &lt;- <span class="string">"paper"</span><span class="comment">//雇员完成答题，并离开</span></span><br><span class="line"><span class="number">08</span>&#125;()</span><br><span class="line"><span class="number">09</span>ch := &lt;-ch</span><br><span class="line"><span class="number">10</span> &#125;</span><br></pre></td></tr></table></figure><p>在第02行，程序创建了一个带string数据的Unbuffered channel，然后在第04行，新雇员开始面试并立即投入答题。而经理在09行等待着其答题结果。一旦答题在05行结束，新雇员通过07行的数据发送告知经理，由于这是Unbuffered channel，接收发生在发送完成之前，因此新雇员保证经理已经接收到答卷。在这种场景下，经理不知道新雇员需要多长时间才能完成答题。</p><h3 id="无确认-含有数据-Buffered-gt-1-Channel"><a href="#无确认-含有数据-Buffered-gt-1-Channel" class="headerlink" title="无确认+含有数据+Buffered&gt;1 Channel"></a>无确认+含有数据+Buffered&gt;1 Channel</h3><p>有时我们不需要知道被发送的信号是否被（立即）接收，比如下面的两种场景。</p><h4 id="扇出"><a href="#扇出" class="headerlink" title="扇出"></a>扇出</h4><p>扇出模式允许经理让多名员工同时工作并完成报告，经理需要确保自己有足够的空间来容纳这么多的报告。<br>想象一位经理面试一个员工团队的场景。经理希望团队中的每位员工都执行一个单独的任务，当员工完成任务时，他们需要为经理提供一份报告，并放置在经理办公桌上的盒子里。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">01</span> <span class="function"><span class="keyword">func</span> <span class="title">fanOut</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="number">02</span>emps := <span class="number">20</span></span><br><span class="line"><span class="number">03</span>ch := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">string</span>, emps)</span><br><span class="line"><span class="number">04</span></span><br><span class="line"><span class="number">05</span><span class="keyword">for</span> e := <span class="number">0</span>; e &lt; emps; e++ &#123;</span><br><span class="line"><span class="number">06</span><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="number">07</span>time.Sleep(time.Duration(rand.Intn(<span class="number">200</span>)) * time.Millisecond)</span><br><span class="line"><span class="number">08</span>ch &lt;- <span class="string">"paper"</span></span><br><span class="line"><span class="number">09</span>&#125;() </span><br><span class="line"><span class="number">10</span>&#125;</span><br><span class="line"><span class="number">11</span></span><br><span class="line"><span class="number">12</span><span class="keyword">for</span> emps &gt; <span class="number">0</span> &#123;</span><br><span class="line"><span class="number">13</span>p := &lt;-ch</span><br><span class="line"><span class="number">14</span>fmt.Println(p)</span><br><span class="line"><span class="number">15</span>emps--</span><br><span class="line"><span class="number">16</span>&#125;</span><br><span class="line"><span class="number">17</span> &#125;</span><br></pre></td></tr></table></figure><p>在第03行，程序创建了一个带string数据的Buffered channel，这一次，该channel的缓冲区大小设定为20。在第05行至10行之间，20名新雇员立即开始工作，经理不知道每位员工完成任务需要多少时间（第07行）。然后在第08行，员工完成任务后发送报告，但该发送过程并不阻塞等待接收。因为盒子里每位员工都有空间，channel上的发送只与其他在同一时间发送报告的员工竞争。第12行至16行之间的代码表示经理等待所有20名员工完成他们的工作并发送报告。一旦收到报告，将在第14行打印，并且本地计数器变量递减。</p><h4 id="Drop"><a href="#Drop" class="headerlink" title="Drop"></a>Drop</h4><p>Drop模式允许经理在雇员有空闲时继续布置任务，即雇员有一个能力值，在能力值范围之内可以继续接收任务，超出能力范围则只能Drop。<br>想象一位经理安排员工完成任务的场景。经理与员工之间没有直接接触，而是通过一个存放任务的箱子沟通，如果经理能将新工作（任务书）放进箱子里，表明员工还能继续承担任务；如果不能执行发送，表明箱子已满，新的工作只能被丢弃。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">01</span> <span class="function"><span class="keyword">func</span> <span class="title">selectDrop</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="number">02</span><span class="keyword">const</span> <span class="built_in">cap</span> = <span class="number">5</span></span><br><span class="line"><span class="number">03</span>ch := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">string</span>, <span class="built_in">cap</span>)</span><br><span class="line"><span class="number">04</span></span><br><span class="line"><span class="number">05</span><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="number">06</span><span class="keyword">for</span> p := <span class="keyword">range</span> ch &#123;</span><br><span class="line"><span class="number">07</span>fmt.Println(<span class="string">"employee : received : "</span>, p)</span><br><span class="line"><span class="number">08</span>&#125;</span><br><span class="line"><span class="number">09</span>&#125;()</span><br><span class="line"><span class="number">10</span></span><br><span class="line"><span class="number">11</span><span class="keyword">const</span> work = <span class="number">20</span></span><br><span class="line"><span class="number">12</span><span class="keyword">for</span> w := <span class="number">20</span>; w &lt; work; w++ &#123;</span><br><span class="line"><span class="number">13</span><span class="keyword">select</span> &#123;</span><br><span class="line"><span class="number">14</span><span class="keyword">case</span> ch &lt;- <span class="string">"paper"</span>:</span><br><span class="line"><span class="number">15</span>fmt.Println(<span class="string">"manager : send ack"</span>)</span><br><span class="line"><span class="number">16</span><span class="keyword">default</span>:</span><br><span class="line"><span class="number">17</span>fmt.Println(<span class="string">"manager : drop"</span>)</span><br><span class="line"><span class="number">18</span>&#125;</span><br><span class="line"><span class="number">19</span>&#125;</span><br><span class="line"><span class="number">20</span><span class="built_in">close</span>(ch)</span><br><span class="line"><span class="number">21</span> &#125;</span><br></pre></td></tr></table></figure><p>在第03行，程序创建了一个带string数据的Buffered channel，这一次，该channel的缓冲区大小设定为5。在第05行至09行之间，经理雇佣一名新员工来处理工作，<code>for range</code>用于信道接收的范围，每次收到’paper’，就在07行处理。在第11至20行之间，经理尝试向员工发送20张’paper’，这里使用SELECT语句实现，如果缓冲区没有更多的空间（<code>default</code>），发送将被阻塞，在第17行放弃发送。最后，在第21行，内置函数close被调用关闭channel，这将在没有数据的情况下向完成工作的员工发送信号。</p><h3 id="延迟确认-含有数据-Buffered-1-Channel"><a href="#延迟确认-含有数据-Buffered-1-Channel" class="headerlink" title="延迟确认+含有数据+Buffered=1 Channel"></a>延迟确认+含有数据+Buffered=1 Channel</h3><p>有些时候在发送信号之前需要知道对端是否接收到发送的前一个信号，比如下面的“等待接收任务”场景。</p><h4 id="等待接收任务"><a href="#等待接收任务" class="headerlink" title="等待接收任务"></a>等待接收任务</h4><p>该场景下，经理雇佣了一位新员工，并一个接一个地为他安排任务。但是，雇员必须在完成一项任务之后才能开始新的任务。<br>此时，如果经理和雇员都按照预定的节奏进行，那么他们不需要额外的互相等待。每次经理发送信号的时候，缓冲区都是空的，每次雇员要开始新的任务时，缓冲区都是满的。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">01</span> <span class="function"><span class="keyword">func</span> <span class="title">waitForTasks</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="number">02</span>ch := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">string</span>, <span class="number">1</span>)</span><br><span class="line"><span class="number">03</span></span><br><span class="line"><span class="number">04</span><span class="keyword">go</span> <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="number">05</span><span class="keyword">for</span> p := <span class="keyword">range</span> ch &#123;</span><br><span class="line"><span class="number">06</span>fmt.Println(<span class="string">"employee : working :"</span>, p)</span><br><span class="line"><span class="number">07</span>&#125;</span><br><span class="line"><span class="number">08</span>&#125;()</span><br><span class="line"><span class="number">09</span></span><br><span class="line"><span class="number">10</span><span class="keyword">const</span> work = <span class="number">10</span></span><br><span class="line"><span class="number">11</span><span class="keyword">for</span> w := <span class="number">0</span>; w &lt; work; w ++ &#123;</span><br><span class="line"><span class="number">12</span>ch &lt;- <span class="string">"paper"</span></span><br><span class="line"><span class="number">13</span>&#125;</span><br><span class="line"><span class="number">14</span></span><br><span class="line"><span class="number">15</span><span class="built_in">close</span>(ch)</span><br><span class="line"><span class="number">16</span> &#125;</span><br></pre></td></tr></table></figure><p>在第02行，程序创建了一个带string数据的大小为1的Buffered channel，在第04行至08行之间，一个新员工被雇佣来处理工作，<code>for range</code>用于channel接收信号，每次收到信号，就在第06行处理。<br>在第10行至13行之间，经理开始向员工安排任务，如果员工可以以尽可能快的速度完成，那么经理与员工之间的等待时间就会减少。但是每一次发送成功，经理都能保证其提交的最后一份工作正在被处理。<br>最后，在第15行，内置函数被调用关闭channel。这将向员工发出信号已经完成了工作。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>可靠性传递、信道状态和是否含有数据这三大属性对于理解和使用channel非常重要，本文通过一些实例代码和示意图对这些属性进行了分析，希望能帮助我们更好地掌握channel的工作机制以编写良好的并发程序。</p><h3 id="附"><a href="#附" class="headerlink" title="附"></a>附</h3><p><a href="http://colobu.com/2016/04/14/Golang-Channels/" target="_blank" rel="noopener">Go Channel详解</a><br><a href="https://www.ardanlabs.com/blog/2017/10/the-behavior-of-channels.html" target="_blank" rel="noopener">The Behavior of Channels</a><br><a href="http://legendtkl.com/2017/07/30/understanding-golang-channel/" target="_blank" rel="noopener">深入理解Go Channel</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我将golang中的channel理解为信号，即channel允许一个goroutine向另一个goroutine发出某些特定事件的信号。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="golang" scheme="http://blog.jonnydu.me/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>golang反射三法则</title>
    <link href="http://blog.jonnydu.me/2018/07/22/golang-reflection/"/>
    <id>http://blog.jonnydu.me/2018/07/22/golang-reflection/</id>
    <published>2018-07-22T11:26:20.000Z</published>
    <updated>2020-04-30T14:16:30.252Z</updated>
    
    <content type="html"><![CDATA[<p>近期在为bingo框架添砖加瓦，用到了golang的反射。这篇文章解决了我不少疑难杂症，特分享给大家。</p><a id="more"></a><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>在计算机科学里，反射是程序，特别是通过类型，查看并操纵其自身结构的能力。 它是元编程的一种形式，同时也是最容易让人误解的部分。<br>在本文中，我们试图解释反射在 Go 中是如何工作，希望能澄清对它的误解。每一种语言的发射模型都不同 （甚至许多语言根本不支持反射），不过既然这篇文章是关于 Go 的，因此在接下来的内容中， “反射”一词应理解成“Go 中的反射”。</p><h3 id="类型与接口"><a href="#类型与接口" class="headerlink" title="类型与接口"></a>类型与接口</h3><p>由于反射建立在类型系统之上，就让我先来复习一下 Go 语言里的类型吧。<br>Go是静态类型的语言。每个变量都有一种静态类型。换言之，他们的类型在 编译期就确定并且固定下来了。 比如 int 、 float32 、 *MyType 或 []byte 等等。如果我们定义了</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> MyInt <span class="keyword">int</span></span><br><span class="line"><span class="keyword">var</span> i <span class="keyword">int</span></span><br><span class="line"><span class="keyword">var</span> j MyInt</span><br></pre></td></tr></table></figure><p>那么i 的类型为 int ，而 j 的类型为 MyInt 。尽管变量 i 和 j 拥有相同的基本类型， 但它们的静态类型仍然不同，因此在它们未经转换前是不能相互赋值的。<br>在类型中，有一种重要的类别就是接口类型，它表示一个确定的方法集。只要某个具体值 （非接口）实现了某个接口中的方法，该接口类型的变量就能存储它。一对众所周知的例子就是 io.Reader 和 io.Writer ，即 io 包中的 Reader 和 Writer 类型：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Reader is the interface that wraps the basic Read method.</span></span><br><span class="line"><span class="keyword">type</span> Reader <span class="keyword">interface</span> &#123;</span><br><span class="line">    Read(p []<span class="keyword">byte</span>) (n <span class="keyword">int</span>, err error)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Writer is the interface that wraps the basic Write method.</span></span><br><span class="line"><span class="keyword">type</span> Writer <span class="keyword">interface</span> &#123;</span><br><span class="line">    Write(p []<span class="keyword">byte</span>) (n <span class="keyword">int</span>, err error)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>任何实现了 Read （或 Write ）方法及其签名的类型， 同时也就实现了 io.Reader （或 io.Writer ）接口。 就此而言，若某个值的类型拥有 Read 方法， io.Reader 类型的变量就能保存它：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> r io.Reader</span><br><span class="line">r = os.Stdin</span><br><span class="line">r = bufio.NewReader(r)</span><br><span class="line">r = <span class="built_in">new</span>(bytes.Buffer)</span><br></pre></td></tr></table></figure><p>有件事一定要明确，即无论 r 保存了什么具体的值， r 的类型总是 io.Reader ：Go是静态类型的，而 r 的静态类型为 io.Reader 。<br>一个非常重要的接口类型是空接口：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">interface</span>&#123;&#125;</span><br></pre></td></tr></table></figure><p>它表示空方法集。由于任何值都有零或多个方法，因此任何值都满足它。<br>有人说Go的接口是动态类型的，不过这是种误解。它们确实是静态类型的： 接口类型的变量总有着相同的静态类型，就算存储在其中的值的类型在运行时可能会改变， 它也总是满足该接口。<br>对于所有的这些，我们都必须严谨对待，因为反射和接口密切相关。</p><h3 id="接口的表示"><a href="#接口的表示" class="headerlink" title="接口的表示"></a>接口的表示</h3><p>Russ Cox 写过一篇题为<a href="http://research.swtch.com/2009/12/go-data-structures-interfaces.html" target="_blank" rel="noopener">Go中接口值的表示</a>的文章，我们就不在此赘述了。不过简单概括一下还是很有必要的。<br>接口类型的变量存储了一对内容：赋予该变量的具体值，以及该值的类型描述符。 更准确地说，其值是实现了该接口的具体数据条目，而其类型则描述了该条目的完整类型。 例如，在执行</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> r io.Reader</span><br><span class="line">tty, err := os.OpenFile(<span class="string">"/dev/tty"</span>, os.O_RDWR, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line">r = tty</span><br></pre></td></tr></table></figure><p>之后，如果按形式来描述，那么 r 就包含了 (值, 类型) 对，即 ( tty, *os.File )。注意，类型 *os.File 还实现了除 Read 以外的其它方法：尽管该接口值只提供了访问 Read 方法的能力，但其内部却携带了有关该值的所有类型信息。 这就是我们可以这样做的原因：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> w io.Writer</span><br><span class="line">w = r.(io.Writer)</span><br></pre></td></tr></table></figure><p>在此赋值语句中的表达式是一个类型断言：它断言 r 内的条目同时也实现了 io.Writer ，因此我们可以将它赋予 w 。 赋值后， w 将会包含一对 ( tty , *os.File )。 这与保存在 r 中的一致。接口的静态类型决定了哪些方法可通过接口变量调用， 即便其内部具体的值可能有更大的方法集。<br>接着，我们可以这样做：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> empty <span class="keyword">interface</span>&#123;&#125;</span><br><span class="line">empty = w</span><br></pre></td></tr></table></figure><p>而我们的空接口值 e 也将再次包含同样的一对 ( tty, *os.File )。 这很方便：空接口可保存任何值，并包含关于该值的所有信息。<br>（在这里我们无需类型断言，因为 w 肯定是满足空接口的。在本例中， 我们将一个值从 Reader 变成了 Writer ，由于 Writer 的方法并非 Reader 的子集，因此我们必须显式地使用类型断言。）<br>一个很重要的细节，就是接口内部的对总是 (值, 具体类型) 的形式，而不会是 (值, 接口类型) 的形式。接口不能保存接口值。<br>现在我们准备好聊聊反射了。</p><h3 id="反射法则一：反射是从接口值到反射对象"><a href="#反射法则一：反射是从接口值到反射对象" class="headerlink" title="反射法则一：反射是从接口值到反射对象"></a>反射法则一：反射是从接口值到反射对象</h3><p>从基本层面上看，反射只是一种检查存储在接口变量中的“类型-值对”的机制。 首先，我们需要了解reflect包中的两种类型：Type和Value。这两种类型可用来访问接口变量的内容。 还有两个简单的函数，叫做reflect.TypeOf 和 reflect.ValueOf ， 它们用于接口值中分别获取 reflect.Type 和 reflect.Value 。 同样，从 reflect.Value 也能很容易地获取 reflect.Type ， 不过让我们先保持 Value 和 Type 概念的独立性吧。<br>我们先从 TypeOf 开始：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">    <span class="string">"reflect"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">var</span> x <span class="keyword">float64</span> = <span class="number">3.4</span></span><br><span class="line">    fmt.Println(<span class="string">"type:"</span>, reflect.TypeOf(x))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此程序会打印出：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span>: <span class="keyword">float64</span></span><br></pre></td></tr></table></figure><p>你可能会问接口在哪，因为该程序看起来只是向 reflect.TypeOf 传递了一个 float64 类型的变量 x ，而不是一个接口值。但它就在那， reflect.TypeOf 的签名包含了一个空接口：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TypeOf 返回 interface&#123;&#125; 中的值的反射类型 Type。</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">TypeOf</span><span class="params">(i <span class="keyword">interface</span>&#123;&#125;)</span> <span class="title">Type</span></span></span><br></pre></td></tr></table></figure><p>当我们调用 reflect.TypeOf(x) 时， x 首先会被存储在一个空接口中， 然后它会作为实参被传入； reflect.TypeOf 通过解包该空接口来还原其类型信息。<br>当然， reflect.ValueOf 函数也会还原它的值（从这里开始， 我们会略过那些概念示例，而只关注于可执行的代码）：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> x <span class="keyword">float64</span> = <span class="number">3.4</span></span><br><span class="line">fmt.Println(<span class="string">"value:"</span>, reflect.ValueOf(x))</span><br></pre></td></tr></table></figure><p>会打印出</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">value: <span class="number">3.4</span></span><br></pre></td></tr></table></figure><p>reflect.Type 和 reflect.Value 都有许多方法来让我们检测并操作它们。 一个重要的例子就是 Value 拥有一个 Type 方法，它会返回 reflect.Value 的 Type 。另外就是 Type 和 Value 都有一个 Kind 方法，它会返回一个常量来表明条目的类型： Uint 、 Float64 或 Slice 等等。同样， Value 拥有像 Int 和 Float 这样的方法来让我们获取存储在内部的值 （作为 int64 和 float64 ）:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> x <span class="keyword">float64</span> = <span class="number">3.4</span></span><br><span class="line">v := reflect.ValueOf(x)</span><br><span class="line">fmt.Println(<span class="string">"type:"</span>, v.Type())</span><br><span class="line">fmt.Println(<span class="string">"kind is float64:"</span>, v.Kind() == reflect.Float64)</span><br><span class="line">fmt.Println(<span class="string">"value:"</span>, v.Float())</span><br></pre></td></tr></table></figure><p>会打印出</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span>: <span class="keyword">float64</span></span><br><span class="line">kind is <span class="keyword">float64</span>: <span class="literal">true</span></span><br><span class="line">value: <span class="number">3.4</span></span><br></pre></td></tr></table></figure><p>同样还有 SetInt 和 SetFloat 这样的方法，不过在使用它们之前， 我们需要理解其可设置性，该主题会在后面的第三条反射法则中讨论。<br>反射库有几点特性值得一提。首先，为了让 API 保持简单， Value 的“获取者”和“设置者”方法会在能够保存其值的最大类型上进行操作：例如 int64 就能用于所有的带符号整数。也就是说， Value 的 Int 方法会返回 int64 类型的值，而 SetInt 会接受 int64 类型的值；因此该值可能需要转换为它所涉及到的实际类型：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> x <span class="keyword">uint8</span> = <span class="string">'x'</span></span><br><span class="line">v := reflect.ValueOf(x)</span><br><span class="line">fmt.Println(<span class="string">"type:"</span>, v.Type())                            <span class="comment">// uint8.</span></span><br><span class="line">fmt.Println(<span class="string">"kind is uint8: "</span>, v.Kind() == reflect.Uint8) <span class="comment">// true.</span></span><br><span class="line">x = <span class="keyword">uint8</span>(v.Uint())                                       <span class="comment">// v.Uint returns a uint64.</span></span><br></pre></td></tr></table></figure><p>第二个特性就是反射对象的 Kind 描述了其基本类型，而给静态类型。 若反射对象包含了用户定义的整数类型的值，比如</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> MyInt <span class="keyword">int</span></span><br><span class="line"><span class="keyword">var</span> x MyInt = <span class="number">7</span></span><br><span class="line">v := reflect.ValueOf(x)</span><br></pre></td></tr></table></figure><p>那么 v 的 Kind 仍为 reflect.Int ，尽管 x 的静态类型为 MyInt 而非 int 。换句话说， Kind 无法区分 int 和 MyInt ，而 Type 则可以。</p><h3 id="反射法则二：从反射对象可反射出接口值"><a href="#反射法则二：从反射对象可反射出接口值" class="headerlink" title="反射法则二：从反射对象可反射出接口值"></a>反射法则二：从反射对象可反射出接口值</h3><p>如同物理中的反射现象那样，Go中的反射也会产生它自己的镜像。<br>给定一个 reflect.Value ，我们可以使用 Interface 方法还原其接口值；在效果上，该方法会将类型与值的信息打包成接口表示，并返回其结果：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Interface 将 v 的值返回成 interface&#123;&#125;。</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(v Value)</span> <span class="title">Interface</span><span class="params">()</span> <span class="title">interface</span></span>&#123;&#125;</span><br></pre></td></tr></table></figure><p>因此，我们可以通过</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y := v.Interface().(<span class="keyword">float64</span>) <span class="comment">// y will have type float64.</span></span><br><span class="line">fmt.Println(y)</span><br></pre></td></tr></table></figure><p>打印出反射对象 v 所表示的 float64 值。<br>不过我们可以做得更好。 fmt.Println 与 fmt.Printf 等都会将实参作为空接口值传递，它们会被包 fmt 进行内部解包， 就像我们刚做的那样。因此，正确地打印出 reflect.Value 内容的方法就是 将 Interface 方法的结果传至格式化打印功能：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fmt.Println(v.Interface())</span><br></pre></td></tr></table></figure><p>（为什么不是 fmt.Println(v) ？因为 v 是个 reflect.Value ，而我们想要的是它保存的具体值。）由于值的类型是 float64 ，如果需要的话，我们甚至可以使用浮点数格式化：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fmt.Printf(<span class="string">"value is %7.1e\n"</span>, v.Interface())</span><br></pre></td></tr></table></figure><p>然后就会得到</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">3.4e+00</span></span><br></pre></td></tr></table></figure><p>再次强调，这里无需将 v.Interface() 的结果类型断言为 float64 ， 因为空接口值中拥有具体值的类型信息，而 Printf 则会将它还原。<br>简单来说， Interface 方法就是 ValueOf 函数的镜像， 不过其结果总是静态类型 interface{} 。<br>重申一遍：从接口值可反射出反射对象，反之亦可。</p><h3 id="反射法则三：可设置性"><a href="#反射法则三：可设置性" class="headerlink" title="反射法则三：可设置性"></a>反射法则三：可设置性</h3><p>第三条法则是最微妙而令人困惑的，但如果我们从第一条法则开始，还是很容易理解的。<br>这些代码虽然不能工作，但很值得学习（自己也在这里踩过坑）。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> x <span class="keyword">float64</span> = <span class="number">3.4</span></span><br><span class="line">v := reflect.ValueOf(x)</span><br><span class="line">v.SetFloat(<span class="number">7.1</span>) <span class="comment">// Error: will panic.</span></span><br></pre></td></tr></table></figure><p>如果你运行这段代码，它就会报出神秘的恐慌信息：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">panic</span>: reflect.Value.SetFloat using unaddressable value</span><br></pre></td></tr></table></figure><p>其问题的根源不在于值 7.1 能不能寻址，而在于 v 不可设置。 可设置性是反射值 Value 的一种属性，而且并不是所有的反射值都拥有它。<br>Value 的 CanSet 方法会报告 Value 的可设置性。 在我们的例子中，</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> x <span class="keyword">float64</span> = <span class="number">3.4</span></span><br><span class="line">v := reflect.ValueOf(x)</span><br><span class="line">fmt.Println(<span class="string">"settability of v:"</span>, v.CanSet())</span><br></pre></td></tr></table></figure><p>会打印出</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">settability of v: <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>对不可设置的 Value 调用 Set 方法会产生错误，但什么是可设置性呢？<br>可设置性有点像可寻址性，不过它更加严格。它是反射对象能否修改其创建之初的真实值的一种属性。 可设置性决定了反射对象是否保存原始条目。当我们执行</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> x <span class="keyword">float64</span> = <span class="number">3.4</span></span><br><span class="line">v := reflect.ValueOf(x)</span><br></pre></td></tr></table></figure><p>后，就将 x 的一份副本传入了 reflect.ValueOf ， 因此该接口值也就作为传递给 reflect.ValueOf 的实参创建了一份 x 的副本，而非 x 本身。因此，假如语句</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">v.SetFloat(<span class="number">7.1</span>)</span><br></pre></td></tr></table></figure><p>能够成功执行，它也无法更新 x ，即便 v 看起来创建自 x 。就算它能够更新存储在该反射值中的 x 的副本， x 本身也不会受影响。这是令人困惑且毫无用处的，因此它是非法的。 而可设置性就是用于避免此类问题的属性。<br>这看起来很奇怪，事实却并非如此。它其实就是藏在奇特外表下的一种常见情况。 考虑将 x 传递给一个函数f(x)。<br>我们并不期望 f 能修改 x ，因为我们传入的是值 x 的副本，而非 x 本身。如果我们想让 f 直接修改 x ，就必须向该函数传入 x 的地址（即指向 x 的指针）：f(&amp;x)。<br>这即熟悉又简单，反射也是以相同的方式工作的。如果我们想要通过反射修改 x ，就必须向反射库提供要修改的值的指针。<br>让我们试试吧。首先像平时那样初始化 x ，接着创建指向它的反射值，叫做 p 。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> x <span class="keyword">float64</span> = <span class="number">3.4</span></span><br><span class="line">p := reflect.ValueOf(&amp;x) <span class="comment">// Note: take the address of x.</span></span><br><span class="line">fmt.Println(<span class="string">"type of p:"</span>, p.Type())</span><br><span class="line">fmt.Println(<span class="string">"settability of p:"</span>, p.CanSet())</span><br></pre></td></tr></table></figure><p>目前会输出</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> of p: *<span class="keyword">float64</span></span><br><span class="line">settability of p: <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>反射对象 p 并不是可设置的，不过我们也不想设置 p ， 而（实际上）是 *p 。为获得 p 指向的内容，我们调用 Value 的 Elem 方法，它会间接通过指针，并将结构保存到叫做 v 的反射值 Value 中：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">v := p.Elem()</span><br><span class="line">fmt.Println(<span class="string">"settability of v:"</span>, v.CanSet())</span><br></pre></td></tr></table></figure><p>现在 v 是可设置的反射对象，如输出所示：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">settability of v: <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>由于它代表 x ，因此最终我们可使用 v.SetFloat 来修改 x 的值：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v.SetFloat(<span class="number">7.1</span>)</span><br><span class="line">fmt.Println(v.Interface())</span><br><span class="line">fmt.Println(x)</span><br></pre></td></tr></table></figure><p>并得到期望的输出：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">7.1</span></span><br><span class="line"><span class="number">7.1</span></span><br></pre></td></tr></table></figure><p>反射可能很难理解，但语言做了它应该做的，尽管反射类型 Types 和值 Values 隐藏了发生的事情。只要记得反射值需要某些东西的地址来修改它所代表的东西。</p><h3 id="结构体"><a href="#结构体" class="headerlink" title="结构体"></a>结构体</h3><p>在我们前面的例子中， v 本身并不是指针，它只是从一个指针中获取的。 在使用反射修改结构体的字段时，这种情况经常出现。即，当我们有结构体的地址时， 就能修改它的字段。<br>下面这个简单的例子分析了结构体类型的值 t 。我们从它的地址创建了反射对象， 因为待会儿要修改它。接着我们将 typeOfT 设置为它的类型， 然后以直白的方法调用遍历其字段（详见 <a href="https://go-zh.org/pkg/reflect/" target="_blank" rel="noopener">reflect</a> 包）。 注意，我们从该结构体类型中提取了其字段名，但字段本身是一般的 reflect.Value 对象。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> T <span class="keyword">struct</span> &#123;</span><br><span class="line">    A <span class="keyword">int</span></span><br><span class="line">    B <span class="keyword">string</span></span><br><span class="line">&#125;</span><br><span class="line">t := T&#123;<span class="number">23</span>, <span class="string">"skidoo"</span>&#125;</span><br><span class="line">s := reflect.ValueOf(&amp;t).Elem()</span><br><span class="line">typeOfT := s.Type()</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; s.NumField(); i++ &#123;</span><br><span class="line">    f := s.Field(i)</span><br><span class="line">    fmt.Printf(<span class="string">"%d: %s %s = %v\n"</span>, i,</span><br><span class="line">        typeOfT.Field(i).Name, f.Type(), f.Interface())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此程序的输出为：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0</span>: A <span class="keyword">int</span> = <span class="number">23</span></span><br><span class="line"><span class="number">1</span>: B <span class="keyword">string</span> = skidoo</span><br></pre></td></tr></table></figure><p>这里还有一个关于可设置性的要点： T 的字段名必须大写（已导出）， 因为只有已导出的字段才是可设置的。<br>由于 s 包含了可设置的反射对象，因此我们可以修改该结构体的字段：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s.Field(<span class="number">0</span>).SetInt(<span class="number">77</span>)</span><br><span class="line">s.Field(<span class="number">1</span>).SetString(<span class="string">"Sunset Strip"</span>)</span><br><span class="line">fmt.Println(<span class="string">"t is now"</span>, t)</span><br></pre></td></tr></table></figure><p>其结果为：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t is now &#123;<span class="number">77</span> Sunset Strip&#125;</span><br></pre></td></tr></table></figure><p>若我们将此程序修改为 s 创建自 t 而非 &amp;t ，那么调用 SetInt 和 SetString 就会失败，因为 t 的字段不可设置。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>再次提示，反射法则如下：</p><ul><li>从接口值可反射出反射对象。</li><li>从反射对象可反射出接口值。</li><li>要修改反射对象，其值必须可设置。</li></ul><p>一旦你理解了Go中的这些反射法则，它就会变得更容易使用了，尽管它还是很微妙。 这是个强大的工具，因此除非有必要，否则应当避免或小心使用。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;近期在为bingo框架添砖加瓦，用到了golang的反射。这篇文章解决了我不少疑难杂症，特分享给大家。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="golang" scheme="http://blog.jonnydu.me/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>常见分布式唯一ID生成策略总结</title>
    <link href="http://blog.jonnydu.me/2018/07/22/distributed-unique-id/"/>
    <id>http://blog.jonnydu.me/2018/07/22/distributed-unique-id/</id>
    <published>2018-07-22T10:30:20.000Z</published>
    <updated>2020-04-30T14:16:30.257Z</updated>
    
    <content type="html"><![CDATA[<p>最近在基于Google Dapper论文搭建分布式跟踪系统，traceid作为一次用户请求的标识，需要在分布式系统中使用唯一ID，这里对常见的生成策略进行简单总结。</p><a id="more"></a><p>全局唯一的ID在很多分布式系统中都会遇到，比如本文引言中所述的分布式跟踪系统，使用traceid来标记一次完整的服务调用，一般在系统的边界（接入层）生成，并向调用链上的后继服务传递，实现对整个调用链的跟踪。<br>借此契机，对常见的分布式唯一ID生成策略进行总结。</p><h3 id="使用数据库的auto-increment生成"><a href="#使用数据库的auto-increment生成" class="headerlink" title="使用数据库的auto_increment生成"></a>使用数据库的auto_increment生成</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul><li>使用数据库现有的功能，比较简单</li><li>具有唯一性和递增性</li><li>id之间的步长是固定且可自定义的</li></ul><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul><li>可用性和扩展性较差。因为数据库常见架构是一主多从+读写分离，主库的写性能决定了ID的生成性能上限，并且难以扩展。</li></ul><h4 id="改进方案"><a href="#改进方案" class="headerlink" title="改进方案"></a>改进方案</h4><p>对于可用性较差的问题，可以通过“冗余主库”、“数据水平切分”等方法来改善。如下图所示，由1个写库变成3个写库，每个写库设置不同的auto_increment初始值，以及相同的步长，以保证每个数据库生成的ID各不相同。<br><img src="http://km.oa.com/files/photos/pictures//20180902//1535881731_49.png" alt=""><br>对于写压力大的问题，可以使用批量生成的方式来改善。如下图所示，数据库中只存储当前ID的最大值。假设ID生成服务每次批量获取5个ID，服务访问数据库，将当前ID的最大值设置为4，这样不需要每次访问数据库，ID生成服务就能依次派发0,1,2,3,4这些ID了。<br>这种方式下，ID生成服务存在单点问题，我们可以通过“主备”的方式来实现容灾和水平扩展，不过又会引发一致性问题。<br><img src="http://km.oa.com/files/photos/pictures//20180902//1535881973_11.png" alt=""></p><h3 id="使用Redis来生成"><a href="#使用Redis来生成" class="headerlink" title="使用Redis来生成"></a>使用Redis来生成</h3><p>使用数据库生成ID的性能较低，可以尝试使用Redis来改善。这主要源于Redis是单线程的，可以通过其原子操作INCR和INCRBY来获取全局唯一的ID。</p><h4 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h4><ul><li>灵活方便，性能优于数据库</li><li>数字ID天然排序，对需要排序的场景有帮助</li></ul><h4 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h4><ul><li>需要引入Redis组件，需要评估系统的复杂度</li></ul><h3 id="uuid"><a href="#uuid" class="headerlink" title="uuid"></a>uuid</h3><p>上述数据库，ID生成服务或是Redis的方法，业务方都需要进行一次远程调用，这对于时延敏感的业务影响较大。uuid是一种常见的本地生成ID的方法。<br>uuid是指一台机器在同一时间中生成的数字在所有机器中是唯一的，其主要由以下三部分组成：</p><ul><li>当前日期和时间</li><li>时钟序列</li><li>全局唯一的IEEE机器识别号</li></ul><p>标准的uuid格式为xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx(8-4-4-4-12)，包含32个16进制数字，以连字号分为五段。</p><h4 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h4><ul><li>性能高，本地生成，没有网络消耗</li><li>扩展性好，基本可以认为没有性能上限</li></ul><h4 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h4><ul><li>不易存储，uuid为16字节128位，通常以36长度的字符串表示，有些场景不适用。—-常见的优化方案为“转化为两个uint64整数存储”或者“折半存储”（折半后不能保证唯一性）</li><li>信息不安全：基于MAC地址生成uuid的算法可能会造成MAC地址泄露</li><li>无法保证递增</li></ul><h3 id="取当前毫秒-微妙数"><a href="#取当前毫秒-微妙数" class="headerlink" title="取当前毫秒/微妙数"></a>取当前毫秒/微妙数</h3><p>针对uuid无法保证趋势递增，以及作为字符串ID检索效率低的缺点，有人提出了取当前毫秒/微妙数的方案。</p><h4 id="优点-3"><a href="#优点-3" class="headerlink" title="优点"></a>优点</h4><ul><li>本地生成ID</li><li>生成ID递增</li><li>生成的ID是整数，建立索引后查询效率高</li></ul><h4 id="缺点-3"><a href="#缺点-3" class="headerlink" title="缺点"></a>缺点</h4><p>并发量超过1000/1000000个ID，会发生冲突。</p><h3 id="Twitter提出的Snowflake算法"><a href="#Twitter提出的Snowflake算法" class="headerlink" title="Twitter提出的Snowflake算法"></a>Twitter提出的Snowflake算法</h3><p>Snowflake是twitter提出的一种分布式ID生成算法，被广泛应用于分布式系统中，其核心思想是一个long型的id由以下四部分组成：</p><ul><li>1bit作为标识符 — 最高位是符号位，始终是0，代表正数</li><li>41bit作为毫秒数 — 41位的长度可以使用69年</li><li>10bit作为机器编号（5bit是数据中心，5bit是机器id） — 10位的长度可部署1024个节点</li><li>12bit作为毫秒内序列号 — 12位的长度支持单个节点每毫秒产生4096个ID号</li></ul><p>如下图所示，算法单节点每秒理论上最多可以产生1000*(2^12)个ID。<br><img src="http://km.oa.com/files/photos/pictures//20180902//1535885058_21.png" alt=""></p><h4 id="Snowflake的C-实现"><a href="#Snowflake的C-实现" class="headerlink" title="Snowflake的C++实现"></a>Snowflake的C++实现</h4><p>在实际系统中采用C++语言实现了Snowflake算法，类的定义为：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> SNOW_FLAKE_H</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> SNOW_FLAKE_H</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Snowflake</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Snowflake();</span><br><span class="line">    ~Snowflake();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">setEpoch</span><span class="params">(<span class="keyword">uint64_t</span> epoch)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">setMachine</span><span class="params">(<span class="keyword">int</span> machine)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">int64_t</span> <span class="title">generate</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">uint64_t</span> <span class="title">getTime</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">uint64_t</span> epoch_;</span><br><span class="line">    <span class="keyword">int</span> machine_;</span><br><span class="line">    <span class="keyword">int</span> sequence_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br></pre></td></tr></table></figure><p>具体实现代码如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"Snowflake.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/time.h&gt;</span></span></span><br><span class="line"></span><br><span class="line">Snowflake::Snowflake()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">this</span>-&gt;epoch_ = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">this</span>-&gt;time_ = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">this</span>-&gt;machine_ = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">this</span>-&gt;sequence_ = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Snowflake::~Snowflake()</span><br><span class="line">&#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//设置毫秒起始点</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Snowflake::setEpoch</span><span class="params">(<span class="keyword">uint64_t</span> epoch)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>-&gt;epoch_ = epoch;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Snowflake::setMachine</span><span class="params">(<span class="keyword">int</span> machine)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>-&gt;machine_ = machine;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//id生成函数</span></span><br><span class="line"><span class="function"><span class="keyword">int64_t</span> <span class="title">Snowflake::generate</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int64_t</span> value = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">//和开始时间戳的差值</span></span><br><span class="line">    <span class="keyword">uint64_t</span> time = getTime() - <span class="keyword">this</span>-&gt;epoch_;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//41bit,毫秒时间</span></span><br><span class="line">    value |= time &lt;&lt; <span class="number">22</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//10bit,机器编号</span></span><br><span class="line">    value |= <span class="keyword">this</span>-&gt;machine_ &amp; <span class="number">0x3FF</span> &lt;&lt; <span class="number">12</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//12bit,毫秒内序列号</span></span><br><span class="line">    value |= <span class="keyword">this</span>-&gt;sequence_++ &amp; <span class="number">0xFFF</span>;</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;sequnce_ == <span class="number">0x1000</span>) &#123;</span><br><span class="line">    <span class="keyword">this</span>-&gt;sequnce_ = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> value;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取当前毫秒数</span></span><br><span class="line"><span class="function"><span class="keyword">uint64_t</span> <span class="title">Snowflake::getTime</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">timeval</span> <span class="title">tv</span>;</span></span><br><span class="line">    gettimeofday(&amp;tv, <span class="literal">NULL</span>);</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> tv.tv_usec / <span class="number">1000</span> + tv.tv_sec * <span class="number">1000</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在基于Google Dapper论文搭建分布式跟踪系统，traceid作为一次用户请求的标识，需要在分布式系统中使用唯一ID，这里对常见的生成策略进行简单总结。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="分布式系统" scheme="http://blog.jonnydu.me/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>goroutine和golang调度器</title>
    <link href="http://blog.jonnydu.me/2018/04/28/goroutine_and_scheduler/"/>
    <id>http://blog.jonnydu.me/2018/04/28/goroutine_and_scheduler/</id>
    <published>2018-04-28T13:07:00.000Z</published>
    <updated>2020-04-30T14:16:30.249Z</updated>
    
    <content type="html"><![CDATA[<p>关于golang，自己听到的第一句话就是“原生支持并发”，而goroutine就是其具体实现。</p><a id="more"></a><h3 id="golang调度器"><a href="#golang调度器" class="headerlink" title="golang调度器"></a>golang调度器</h3><p>本节围绕“<strong>存在内核线程调度器的情况下，golang为什么要设计一个自己的调度器？</strong>”这一问题进行讨论。<br>我们知道，操作系统的内核调度器会将系统中的多个线程按照一定的算法调度到物理CPU上运行。最典型的例子如C/C++语言的并发实际上就是基于内核调度的，应用程序负责创建线程（比如linux下通过pthread库调用），内核负责调度。但是，这种传统方式存在一些不足：<br>1、<strong>代价较高</strong>：虽然线程的代价已经比进程小了很多，但仍占用不小的资源，并且线程切换也有不小的消耗，因此我们依然不能创建大量的线程。<br>2、常见的线程模型—N:1和1:1模型<strong>不能较好的兼顾</strong>1）充分利用多核CPU的优势；2）快速的上下文切换。<br>3、<strong>线程之间的通信不易掌握</strong>。虽然多线程之间的通信有多种方法，但使用起来都比较复杂、易错。比如常见的共享内存，各种加锁操作会使得程序开发和问题定位相当困难，甚至会出现死锁和挂掉等问题。<br>4、<strong>大量使用异步</strong>。比如很多网络服务程序，如上文所述，由于系统中不能创建大量线程，所以通常需要在线程中使用IO多路复用（如epoll），然后通过注册回调函数的机制实现各事件的异步调用。但是，异步调用的方式和我们通常的思维习惯不同，当系统中存在大量的回调函数时，会给程序的开发和维护带来较大困难。  </p><p><strong>基于以上问题，go采用了用户态轻量级线程（“goroutine”）的方式来解决</strong>。其主要特点可以归纳为：<br>1、<strong>用户态</strong>：goroutine在用户空间实现，其对于操作系统来说只是一个应用程序，操作系统甚至不知道有goroutine的存在，goroutine的调度需要自己完成。<br>2、<strong>轻量级</strong>：goroutine占用的资源很小，因此一个go程序可以创建成千上万个并发的goroutine。同时，goroutine之间的调度切换不需要陷入内核中，代价较低。将多个goroutine按照一定的算法调度到CPU上执行需要goroutine调度器。<br>3、倡导“<strong>通过通信来共享内存，而不是通过共享内存来通信</strong>”。go通过语法关键词提供了channel机制，用于goroutine之间的通信，实现了CSP并发模型。该并发范式逻辑简单清楚，系统有高正确性。</p><h3 id="golang调度模型"><a href="#golang调度模型" class="headerlink" title="golang调度模型"></a>golang调度模型</h3><p>golang试图通过M:N模型来兼顾N:1和1:1模型的优点，具体来说，每个用户线程可以对应多个内核线程，同时一个内核线程也可以对应多个用户线程。M:N模型的缺点是调度复杂，需要设计良好的调度器。  </p><p>如下图所示，goroutine调度器中使用了三种实体：（1）M：系统线程；（2）P：逻辑处理器（核）；（3）G：goroutine。图中，每个线程运行了一个goroutine，所以必须得维持一个上下文P。P的数量由启动时环境变量GOMAXPROCS决定，即在程序执行的过程中都只有GOMAXPROCS个gouroutine在同时运行。而灰色的goroutine在等待被调度，它们被维护在一个队列（runqueue）里。当一个go语句执行（即创建一个goroutine）时，就有一个新的goroutine被添加到runqueue队尾；当运行当前goroutine到调度点时，runqueue队头弹出一个goroutine进入运行。  </p><p><img src="https://i.imgur.com/TewfErj.png" alt=""></p><p>因此，当一个上下文（P）运行完要被调度的所有goroutine时，系统的稳定状态将被改变。此时如果各上下文的运行队列里的goroutine数目不均衡，将会导致一个上下文在执行完它的运行队列后就结束了，而系统中仍然有许多goroutine要执行。所以，调度系统需要维护一个全局队列，每个上下文能够从全局队列中获取goroutine，但如果全局队列中也没有goroutine了，上下文还能从其他上下文获取goroutine。该过程被称为“stealing”，goroutine通过“stealing”，确保每个上下文总是处于运行，同时保障所有线程都处于最大负载。<br>goroutine调度器模型践行了计算机界的一句名言：“<strong>计算机科学领域的绝大多数问题都可以采用增加一个间接的中间层解决</strong>”。每个G（goroutine）要想真正运行起来，首先需要被分配一个P（进入到P的local runqueue中），在G看来，P就是它所运行的“CPU”。但在go调度器看来，真正的“CPU”却是M，只有将P和M绑定才能让P的runqueue中的G真正运行起来。因此，goroutine调度器通过向G-M模型中增加一个中间层P，实现了高效、可扩展的调度。</p><h3 id="golang调度器跟踪"><a href="#golang调度器跟踪" class="headerlink" title="golang调度器跟踪"></a>golang调度器跟踪</h3><p>下面通过一个例子来查看go调度器的一些跟踪信息。 </p><p><img src="https://i.imgur.com/rKoVNN1.png" alt=""></p><p>在main函数中，通过一个for循环创建了10个goroutine，然后在第16行等待所有的goroutine完成任务。每个goroutine执行的work函数中先sleep 1秒，然后循环计数1e9次。完成后调用waitGroup的Done方法返回。<br>然后在linux环境下运行该程序，并设置GODEBUG环境变量和schedtrace参数观察调度器的输出信息，其结果如下图所示。</p><p><img src="https://i.imgur.com/03sGFDQ.png" alt=""></p><p>其中，xxxms代表自程序开始运行时的毫秒数，gomaxprocx代表配置的处理器数，即上述golang调度器模型中的P，threads代表运行时管理的线程数，idleprocs代表空闲的处理器数，idlethreads代表空闲的线程数，spinningthreads处于自旋状态的线程数（M正在寻找可运行的G）。runqueue=xxx代表在全局队列中的goroutine数。[xxx]代表本地run队列中的goroutine数。<br>而如果还想更深入的了解goroutine调度的细节，可以添加scheddetail参数，该参数下将打印输出处理器P、线程M和goroutine的细节，如下图所示。</p><p><img src="https://i.imgur.com/MhoDjYV.png" alt=""></p><p>可以看出，与上图相比，添加scheddetail参数后P、M、G实体的状态和摘要信息更加清晰，这里就不再一一描述，有兴趣的同学可以参考：<a href="https://software.intel.com/en-us/blogs/2014/05/10/debugging-performance-issues-in-go-programs" target="_blank" rel="noopener">https://software.intel.com/en-us/blogs/2014/05/10/debugging-performance-issues-in-go-programs</a></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>golang入门不久，对goroutine的设计及调度，高并发CSP模型等问题有较强的兴趣，本文从初学者的角度，简单学习、总结了goroutine和对应调度器的一些设计思路，希望借此能更好的理解go语言，后续还将对channel以及对应的CSP模型进行较深入的学习。</p><h3 id="附"><a href="#附" class="headerlink" title="附"></a>附</h3><p>本文参考：<br><a href="https://morsmachine.dk/netpoller" target="_blank" rel="noopener">https://morsmachine.dk/netpoller</a><br><a href="https://software.intel.com/en-us/blogs/2014/05/10/debugging-performance-issues-in-go-programs" target="_blank" rel="noopener">https://software.intel.com/en-us/blogs/2014/05/10/debugging-performance-issues-in-go-programs</a>   </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于golang，自己听到的第一句话就是“原生支持并发”，而goroutine就是其具体实现。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="golang" scheme="http://blog.jonnydu.me/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>Linux块IO浅析</title>
    <link href="http://blog.jonnydu.me/2017/05/05/Linux-Block-IO/"/>
    <id>http://blog.jonnydu.me/2017/05/05/Linux-Block-IO/</id>
    <published>2017-05-05T12:08:28.000Z</published>
    <updated>2020-04-30T14:16:30.276Z</updated>
    
    <content type="html"><![CDATA[<p>块存储，简单来说就是使用块设备为系统提供存储服务。块存储分多钟类型，有单机块存储（磁盘）、网络存储（NAS、SAN等），分布式存储（云硬盘）。通常块存储的表现形式就是一块设备，用户看到的就是类似于sda、sdb这样的逻辑设备。      </p><a id="more"></a> <p>可以说，生活中最常见的块存储设备就是硬盘了，接下来将以硬盘为例简要说明Linux块IO。   </p><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><p>类似于网络协议栈的分层结构，Linux内核块设备I/O总体结构图如下所示。从图中可以看出，以对磁盘的读请求为例，读操作会依次经过虚拟文件系统层（VFS），Cache层（Page Cache）、具体的文件系统层（比如xfs），通用块层（Generic Block Layer）、I/O调度层、块设备驱动层，最后到达物理块设备层。当然，这其中还包括绕开Page Cache的直接（Direct）I/O模式。下面对每一层做简要介绍。</p><p><img src="http://i.imgur.com/2ELr6pl.png" alt=""></p><h4 id="虚拟文件系统层"><a href="#虚拟文件系统层" class="headerlink" title="虚拟文件系统层"></a>虚拟文件系统层</h4><p>VFS虚拟文件系统是一种软件机制，扮演着文件系统管理者的角色，与它相关的数据结构只存在于物理内存当中。它的作用是屏蔽下层具体文件系统操作的差异，为上层的操作提供一个统一的接口。也正是由于VFS的存在，Linux中允许众多不同的文件系统共存并且对文件的操作可以跨文件系统而执行。<br>Linux中VFS主要依靠四个数据结构来描述相关信息,分别是超级块、索引节点、目录项和文件对象。<br>（1）超级块(Super Block)<br>超级块对象表示一个文件系统。它存储一个已安装的文件系统的控制信息，包括文件系统名称（比如Ext2）、文件系统的大小和状态、块设备的引用和元数据信息（比如空闲列表等等）。超级块一般存储在磁盘的特定扇区中，但是对于那些基于内存的文件系统（如proc、sysfs），超级块是在使用时创建在内存中。<br>（2）索引节点（Inode）<br>索引节点是VFS中的核心概念，它包含内核在操作文件或目录时需要的全部信息。<br>一个索引节点代表文件系统的一个文件（这里的文件不仅是指我们平时所认为的普通的文件，还包括目录、特殊设备文件等）。<br>索引节点和超级块一样是实际存储在磁盘上的，当被应用程序访问时才会在内存中创建。<br>（3）目录项（Dentry）<br>引入目录项对象的概念主要是出于方便查找文件的目的。不同于前面的两个对象，目录项对象没有对应的磁盘数据结构，只存在于内存中，一个路径的各个组成部分，不管是目录还是普通的文件，都是一个目录项对象。在路径/home/source/test.cpp中，目录/, home, source和文件test.cpp都对应一个目录项对象。VFS在查找的时候，根据一层层的目录项找到对应的目录项Inode，就可以找到最终的文件。<br>（4）文件对象（File）<br>文件对象描述的是进程已经打开的文件。因为一个文件可以被多个进程打开，所以一个文件可以存在多个文件对象。一个文件对应的文件对象可能不是惟一的，但是其对应的索引节点和目录项对象肯定是惟一的。 </p><h4 id="Page-Cache层"><a href="#Page-Cache层" class="headerlink" title="Page Cache层"></a>Page Cache层</h4><p>引入Page Cache层的目的是为了提高Linux操作系统对磁盘访问的性能。Cache层在内存中缓存了磁盘上的部分数据。当数据的请求到达时，如果在Cache中存在该数据且是最新的，则直接将数据传递给用户程序，免除了对底层磁盘的操作，提高了性能。<br>在Linux的实现中，文件Cache分为两个层面，一是Page Cache，另一个是Buffer Cache，前者主要用来作为文件系统上的文件数据的缓存使用，尤其是针对当进程对文件有read/write操作的时候。Buffer Cache则主要设计用来在系统对块设备进行读写的时候，对块进行数据缓存的系统来使用。关于二者的详细对比说明，请参考<a href="http://blog.dujiong.net" target="_blank" rel="noopener">一期一会</a><br>磁盘Cache有两大功能：预读和回写。预读其实就是利用了局部性原理，具体过程是：对于每个文件的第一个读请求，系统读入所请求的页面并读入紧随其后的少数几个页面（通常是三个页面），这时的预读称为同步预读。对于第二次读请求，如果所读页面不在Cache中，即不在前次预读的页中，则表明文件访问不是顺序访问，系统继续采用同步预读；如果所读页面在Cache中，则表明前次预读命中，操作系统把预读页的大小扩大一倍，此时预读过程是异步的，应用程序可以不等预读完成即可返回，只要后台慢慢读页面即可，这时的预读称为异步预读。任何接下来的读请求都会处于两种情况之一：第一种情况是所请求的页面处于预读的页面中，这时继续进行异步预读；第二种情况是所请求的页面处于预读页面之外，这时系统就要进行同步预读。<br>回写是通过暂时将数据存在Cache里，然后统一异步写到磁盘中。通过这种异步的数据I/O模式解决了程序中的计算速度和存储速度不匹配的鸿沟，减少了访问底层存储介质的次数，使存储系统的性能大大提高。</p><h4 id="文件系统层"><a href="#文件系统层" class="headerlink" title="文件系统层"></a>文件系统层</h4><p>VFS的下一层是具体的文件系统。比如Ext2、Ext3、xfs等。<br>限于个人知识水平，暂时不对具体的文件系统做介绍。待后面研究过后，再做分析。    </p><h4 id="通用块层"><a href="#通用块层" class="headerlink" title="通用块层"></a>通用块层</h4><p>通用块层的主要工作是：接收上层发出的磁盘请求，并最终发出I/O请求。该层隐藏了底层硬件块设备的特性，为块设备提供了一个通用的抽象视图。<br>对于VFS和具体的文件系统来说，块（Block）是基本的数据传输单元，当内核访问文件的数据时，它首先从磁盘读取一个块。但是对于磁盘来说，扇区是最小的可寻址单元，块设备无法对比它还小的单元进行寻址和操作。由于扇区是磁盘的最小可寻址单元，所以块不能比扇区还小，只能整数倍于扇区大小，即一个块对应磁盘上的一个或多个扇区。一般来说，块大小是2的整数倍，而且由于Page Cache层的最小单元是页（Page），所以块大小不能超过一页的长度。<br>大多数情况下，数据的传输通过DMA方式。旧的磁盘控制器，仅仅支持简单的DMA操作：，只能传输磁盘上相邻的扇区，即数据在内存中也是连续的。这是因为如果传输非连续的扇区，会导致磁盘花费更多的时间在寻址操作上。而现在的磁盘控制器支持“分散/聚合”DMA操作，这种模式下，数据传输可以在多个非连续的内存区域中进行。为了利用“分散/聚合”DMA操作，块设备驱动必须能处理被称为段（segments）的数据单元。一个段就是一个内存页面或一个页面的部分，它包含磁盘上相邻扇区的数据。<br>通用块层是粘合所有上层和底层的部分，一个页的磁盘数据布局如下所示：      </p><p><img src="http://i.imgur.com/ISbyMca.png" alt=""></p><h4 id="I-O调度层"><a href="#I-O调度层" class="headerlink" title="I/O调度层"></a>I/O调度层</h4><p>I/O调度层的功能是管理块设备的请求队列。即接收通用块层发出的I/O请求，缓存请求并试图合并相邻的请求，并根据设置好的调度算法，回调驱动层提供的请求处理函数，以处理具体的I/O请求。<br>如果简单地以内核产生请求的次序直接将请求发给块设备的话，那么块设备性能肯定让人难以接受，因为磁盘寻址是整个计算机中最慢的操作之一。为了优化寻址操作，内核不会一旦接收到I/O请求后，就按照请求的次序发起块I/O请求。为此Linux实现了几种I/O调度算法，算法基本思想就是通过合并和排序I/O请求队列中的请求，以此大大降低所需的磁盘寻道时间，从而提高整体I/O性能。<br>常见的I/O调度算法包括Noop调度算法（No Operation）、CFQ（完全公正排队I/O调度算法）、DeadLine（截止时间调度算法）、AS预测调度算法等。     </p><h4 id="块设备驱动层"><a href="#块设备驱动层" class="headerlink" title="块设备驱动层"></a>块设备驱动层</h4><p>驱动层中的驱动程序对应具体的物理块设备。它从上层中取出I/O请求，并根据该I/O请求中指定的信息，通过向具体块设备的设备控制器发送命令的方式，来操纵设备传输数据。</p><h3 id="磁盘I-O优化技巧"><a href="#磁盘I-O优化技巧" class="headerlink" title="磁盘I/O优化技巧"></a>磁盘I/O优化技巧</h3><p>前文已经叙述了Linux通过Cache以及排序合并I/O请求来提高系统的性能，其本质原因是磁盘随机读写慢，顺序读写快。本节介绍一些现有开源系统常见的针对磁盘I/O特性的优化技巧。       </p><h4 id="追加写"><a href="#追加写" class="headerlink" title="追加写"></a>追加写</h4><p>在进行系统设计时，良好的读性能和写性能往往不可兼得。在许多常见的开源系统中都是在优先保证写性能的前提下来优化读性能的。使系统拥有良好的写性能的一个常见方法就是采用追加写，每次将数据添加到文件。由于完全是顺序的，所以可以具有非常好的写操作性能。但是这种方式也存在一些缺点：从文件中读一些数据时将会需要更多的时间：需要倒序扫描，直到找到所需要的内容。<br>LevelDB所采用的LSM（日志结构合并树）是一种较好的追加写设计方案，LSM的思想是将整个磁盘看做一个日志，在日志中存放永久性数据及其索引，每次都添加到日志的末尾。并且通过将很多小文件的存取转换为连续的大批量传输，使得对于文件系统的大多数存取都是顺序的，从而提高磁盘I/O。其具体设计可参考<a href="http://blog.dujiong.net/2017/03/27/leveldb/" target="_blank" rel="noopener">LevelDB原理剖析</a></p><h4 id="文件合并和元数据优化"><a href="#文件合并和元数据优化" class="headerlink" title="文件合并和元数据优化"></a>文件合并和元数据优化</h4><p>目前的大多数文件系统，如XFS/Ext4、HDFS，在元数据管理、缓存管理等实现策略上都侧重大文件。这些文件系统在面临海量小文件时在性能和存储效率方面都大幅降低，根本原因是磁盘最适合顺序的大文件I/O读写模式，而非常不适合随机的小文件I/O读写模式。主要原因体现在元数据管理低效和数据布局低效：<br>（1）元数据管理低效：由于小文件数据内容较少，因此元数据的访问性能对小文件访问性能影响巨大。Ext2文件系统中Inode和Data Block分别保存在不同的物理位置上，一次读操作需要至少经过两次的独立访问。在海量小文件应用下，Inode的频繁访问，使得原本的并发访问转变为了海量的随机访问，大大降低了性能。另外，大量的小文件会快速耗尽Inode资源，导致磁盘尽管有大量Data Block剩余也无法存储文件，会浪费磁盘空间。<br>（2）数据布局低效：Ext2在Inode中使用多级指针来索引数据块。对于大文件，数据块的分配会尽量连续，这样会具有比较好的空间局部性。但是对于小文件，数据块可能零散分布在磁盘上的不同位置，并且会造成大量的磁盘碎片，不仅造成访问性能下降，还大量浪费了磁盘空间。数据块一般为1KB、2KB或4KB，对于小于4KB的小文件，Inode与数据的分开存储破坏了空间局部性，同时也造成了大量的随机I/O。<br>对于海量小文件应用，常见的I/O流程复杂也是造成磁盘性能不佳的原因。对于小文件，磁盘的读写所占用的时间较少，而用于文件的open()操作占用了绝大部分系统时间，导致磁盘有效服务时间非常低，磁盘性能低下。针对于问题的根源，优化的思路大体上分为：<br>（1）针对数据布局低效，采用小文件合并策略，将小文件合并为大文件；<br>（2）针对元数据管理低效，优化元数据的存储和管理。   </p><h5 id="小文件合并"><a href="#小文件合并" class="headerlink" title="小文件合并"></a>小文件合并</h5><p>小文件合并为大文件后，首先减少了大量元数据，提高了元数据的检索和查询效率，降低了文件读写的I/O操作延时。其次将可能连续访问的小文件一同合并存储，增加了文件之间的局部性，将原本小文件间的随机访问变为了顺序访问，大大提高了性能。同时，合并存储能够有效的减少小文件存储时所产生的磁盘碎片问题，提高了磁盘的利用率。最后，合并之后小文件的访问流程也有了很大的变化，由原来许多的open操作转变为了seek操作，定位到大文件具体的位置即可。如何寻址这个大文件中的小文件呢？其实就是利用一个旁路数据库来记录每个小文件在这个大文件中的偏移量和长度等信息。其实小文件合并的策略本质上就是通过分层的思想来存储元数据。中控节点存储一级元数据，也就是大文件与底层块的对应关系；数据节点存放二级元数据，也就是最终的用户文件在这些一级大块中的存储位置对应关系，经过两级寻址来读写数据。<br>TFS是采用小文件合并存储策略的例子。TFS默认Block大小为64M，每个块中会存储许多不同的小文件，但是这个块只占用一个Inode。假设一个Block为64M，数量级为1PB。那么NameServer上会有16.7M个Block。假设每个Block的元数据大小为0.1K，则占用内存不到2G。在TFS中，文件名中包含了Block ID和File ID，通过Block ID定位到具体的DataServer上，然后DataServer会根据本地记录的信息来得到File ID所在Block的偏移量，从而读取到正确的文件内容。    </p><h5 id="元数据管理优化"><a href="#元数据管理优化" class="headerlink" title="元数据管理优化"></a>元数据管理优化</h5><p>一般来说元数据信息包括名称、文件大小、设备标识符、用户标识符、用户组标识符等等，在小文件系统中可以对元数据信息进行精简，仅保存足够的信息即可。元数据精简可以减少元数据通信延时，同时相同容量的Cache能存储更多的元数据，从而提高元数据使用效率。另外可以在文件名中就包含元数据信息，从而减少一个元数据的查询操作。最后针对特别小的一些文件，可以采取元数据和数据并存的策略，将数据直接存储在元数据之中，通过减少一次寻址操作从而大大提高性能。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;块存储，简单来说就是使用块设备为系统提供存储服务。块存储分多钟类型，有单机块存储（磁盘）、网络存储（NAS、SAN等），分布式存储（云硬盘）。通常块存储的表现形式就是一块设备，用户看到的就是类似于sda、sdb这样的逻辑设备。      &lt;/p&gt;
    
    </summary>
    
    
    
      <category term="network/linux" scheme="http://blog.jonnydu.me/tags/network-linux/"/>
    
  </entry>
  
  <entry>
    <title>NVMe驱动之I/O请求</title>
    <link href="http://blog.jonnydu.me/2017/04/27/NVMe-IO-Request/"/>
    <id>http://blog.jonnydu.me/2017/04/27/NVMe-IO-Request/</id>
    <published>2017-04-27T12:31:41.000Z</published>
    <updated>2020-04-30T14:16:30.279Z</updated>
    
    <content type="html"><![CDATA[<p>NVMe SSD具有极高的I/O读写性能，不存在传统磁盘所具有的访问寻道、抖动问题。为了发挥NVMe SSD的性能，无论在软件还是在硬件上都需要采用多队列技术，通过多队列方式充分发挥NVMe SSD的性能。</p><a id="more"></a> <p>Linux的NVMe驱动采用一个Core独占一个Queue（包含一个Completion Queue和至少一个Submission Queue）的方式来充分发挥NVMe SSD的性能，这样可以避免一个队列被多个Core竞争访问（引起的锁竞争等问题），各CPU使用自己的Queue，互不影响。 </p><p><img src="http://i.imgur.com/sIdEXDh.gif" alt="">   </p><p>NVMe响应I/O的过程已经在<a href="http://blog.dujiong.net/2017/02/17/NVMe/" target="_blank" rel="noopener">NVMe技术浅析</a>一文中详细描述，这里将结合部分源码做进一步说明。    </p><h3 id="创建NVMe-Queue"><a href="#创建NVMe-Queue" class="headerlink" title="创建NVMe Queue"></a>创建NVMe Queue</h3><pre><code>/* * pci.c */static int nvme_setup_io_queues(struct nvme_dev* dev){    struct nvme_queue* adminq = dev-&gt;queues[0];    struct pci_dev* pdev = to_pci_dev(dev-&gt;dev);    int result, nr_io_queues, size;    nr_io_queues = num_online_cpus();    result = nvme_set_queue_count(&amp;dev-&gt;ctrl, &amp;nr_io_queues);    ...    result = queue_request_irq(adminq);    if(result){        admin-&gt;cq_vector = -1;        return result;    }    return nvme_create_io_queues(dev);}</code></pre><p>驱动程序在创建队列之前首先通过内核函数num_online_cpus获取当前系统中运行的CPU数量m，然后通过nvme_set_queue_count得到SSD的队列数量限制n，选择二者较小值作为所创建的队列数。<br>此外，关于Queue还有一个重要的概念，那就是队列深度，简单来说就是这个Queue能够放多少个成员（比如NVMe Command）。在NVMe中，这个队列深度是由NVMe SSD决定的，存储在NVMe设备的BAR空间里。另外，NVMe驱动没有区分Namespace，也就是一个设备上的多个Namespace共享这些队列资源。<br>因此，它们之间的关系是：队列用来存放NVMe Command，NVMe Command是Host与SSD Controller交流的基本单元，应用的I/O请求也要转化成NVMe Command。</p><h3 id="提交I-O请求"><a href="#提交I-O请求" class="headerlink" title="提交I/O请求"></a>提交I/O请求</h3><p>Block层下发的IO请求以BIO表示，我们需要通过DMA发送这些数据，Command使用dma_alloc_coherent分配DMA地址，但是BIO是存放在普通的内核线程空间的（线程的虚拟空间不能直接作为DMA地址），使用dma_alloc_coherent分配再将BIO中的数据拷贝显然不是有效的方法。这里linux提供了另一个函数dma_map_single，这个函数能够将虚拟空间地址（BIO数据存放地址）转换成DMA可用地址，并且多个IO请求的DMA地址可以通过scatterlist来表示。有了DMA地址就可以把BIO封装成NVMe Command发送出去。从BIO到NVMe Command有可能会经过拆分，放入等待队列等。我们知道，驱动会给每个CPU分配一个Queue，那么I/O请求到来时，该由哪个Queue来存放这个Command呢？你可能已经想到，应该由当前线程运行的CPU所属的Queue，这样才能保证Queue不被其他Core抢占，驱动使用get_cpu()获得当前处理I/O请求的CPU号来索引对应的Queue。 </p><pre><code>/* * pci.c */static void __nvme_submit_cmd(struct nvme_queue* nvmeq, struct nvme_command* cmd){    u16 tail = nvmeq-&gt;sq_tail;    if(nvmeq-&gt;sq_cmds_io){        memcpy_toio(&amp;nvmeq-&gt;sq_cmds_io[tail], cmd, sizeof(*cmd));    }else{        memcpy(&amp;nvmeq-&gt;sq_cmds[tail], cmd, sizeof(*cmd));    }    if(++tail == nvmeq-&gt;q_depth)        tail = 0;    writel(tail, nvmeq-&gt;q_db);    nvmeq-&gt;sq_tail = tail;}</code></pre><p>BIO封装成的Command会顺序存入Submission Queue中。对于Submission Queue来说，使用Tail表示最后操作的Command Index（nvmeq-&gt;sq_tail）。每存入一个Command，Host就会更新Queue对应的Doorbell寄存器中的Tail值。Doorbell定义在BAR空间，通过QID可以索引到。NVMe没有规定Command存入队列的执行顺序，Controller可以一次取出多个Command进行批量处理，所以一个队列中的Command执行顺序是不固定的（可能导致先提交的请求后处理）。</p><h3 id="获得处理结果"><a href="#获得处理结果" class="headerlink" title="获得处理结果"></a>获得处理结果</h3><p>SSD Controller根据Doorbell的值，获取NVMe Command和对应数据，待处理完成后将结果存入Completion Queue中。Controller通过中断的方式通知Host，驱动为每一个Queue分配一个MSI/MSI-X中断。<br>驱动使用request_irq将中断注册到kernel，并且绑定中断处理函数nvme_irq。此中断处理函数调用__nvme_process_cq，先将Completion Command从Completion Queue中取出，然后把队列的head增加1，并调用上层的callback函数，完成IO处理。由于NVMe Command可以批量处理，这里使用while循环取出所有新的Completion Command。     </p><pre><code>/* * pci.c */static void __nvme_process_cq(struct nvme_queue *nvmeq, unsigned int *tag){    u16 head, phase;    head = nvmeq-&gt;cq_head;    phase = nvmeq-&gt;cq_phase;    while(nvme_cqe_valid(nvmeq, head, phase)){        struct nvme_completion cqe = nvmeq-&gt;cqes[head];        struct request *req;        if(++head == nvmeq-&gt;q_depth){            head = 0;            phase = !phase;        }        ...        if (unlikely(nvmeq-&gt;qid == 0 &amp;&amp;            cqe.command_id &gt;= NVME_AQ_BLKMQ_DEPTH)) {                nvme_complete_async_event(&amp;nvmeq-&gt;dev-&gt;ctrl,                                        cqe.status, &amp;cqe.result);                continue;        }        req = blk_mq_tag_to_rq(*nvmeq-&gt;tags, cqe.command_id);        nvme_req(req)-&gt;result = cqe.result;        blk_mq_complete_request(req, le16_to_cpu(cqe.status) &gt;&gt; 1);        }    if (head == nvmeq-&gt;cq_head &amp;&amp; phase == nvmeq-&gt;cq_phase)        return;    if (likely(nvmeq-&gt;cq_vector &gt;= 0))        writel(head, nvmeq-&gt;q_db + nvmeq-&gt;dev-&gt;db_stride);    nvmeq-&gt;cq_head = head;    nvmeq-&gt;cq_phase = phase;    nvmeq-&gt;cqe_seen = 1;}</code></pre><p>处理完Command后，往Completion Queue的Doorbell写入Head值，通知NVMe Controller操作完成。中断处理结束。</p><h3 id="Head-Tail机制"><a href="#Head-Tail机制" class="headerlink" title="Head/Tail机制"></a>Head/Tail机制</h3><p>我们知道，SQ和CQ都是队列，队列的头尾很重要，头决定谁会最先被服务，尾决定新到来命令的位置，所以，我们需要记录SQ和CQ的头尾位置。  </p><p><img src="http://i.imgur.com/bsa3KFs.png" alt=""></p><p>对一个SQ来说，它的生产者是Host，因为它往SQ的Tail位置写入命令；消费者是SSD，它从SQ的Head取出指令执行。CQ则刚刚相反，生产者是SSD，消费者是Host。     </p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文简单介绍了NVMe I/O请求的处理过程，其核心是多队列，Submission Queue/Completion Queue、Doorbell寄存器和Head/Tail机制。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NVMe SSD具有极高的I/O读写性能，不存在传统磁盘所具有的访问寻道、抖动问题。为了发挥NVMe SSD的性能，无论在软件还是在硬件上都需要采用多队列技术，通过多队列方式充分发挥NVMe SSD的性能。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="NVMe over RDMA" scheme="http://blog.jonnydu.me/tags/NVMe-over-RDMA/"/>
    
  </entry>
  
  <entry>
    <title>理解TCP/IP网络栈</title>
    <link href="http://blog.jonnydu.me/2017/04/21/trace-linux-network-stack/"/>
    <id>http://blog.jonnydu.me/2017/04/21/trace-linux-network-stack/</id>
    <published>2017-04-21T08:45:24.000Z</published>
    <updated>2020-04-30T14:16:30.278Z</updated>
    
    <content type="html"><![CDATA[<p>看到一篇讲解TCP/IP协议栈原理及其处理数据包流程很好的文章，特记录下来。  </p><a id="more"></a><p>原文地址：<a href="http://yuedu.163.com/news_reader/#/~/source?id=50668df1-784d-42b8-8893-ab4063734506_1&cid=47928decaeae4753923ba2fc552f0669_1" target="_blank" rel="noopener">理解TCP/IP网络栈&amp;编写网络应用</a>     </p><h3 id="数据传输"><a href="#数据传输" class="headerlink" title="数据传输"></a>数据传输</h3><p>网络栈有很多层。图1表示了这些层的类型：</p><p><img src="http://i.imgur.com/KrEHsC4.png" alt=""><br>图1 发送数据时网络栈中各层对数据的操作</p><p>这些层可以被大致归类到三个区域中：（1）用户区；（2）内核区；（3）设备区。用户区和内核区的任务由CPU执行。用户区和内核区被叫做“主机（host）”以区别于设备区。在这里，设备是发送和接收数据包的网络接口卡（Network Interface Card,NIC）。它有一个更常用的术语：网卡。<br>我们来了解一下用户区。首先，应用程序创建要发送的数据（图1中的“User Data”）并且调用write()系统调用来发送数据（在这里假设socket已经被创建了，对应图1中的“fd”）。当系统调用被调用之后上下文切换到内核区。<br>像Linux或者Unix这类POSIX系列的操作系统通过文件描述符（file descriptor）把socket暴露给应用程序。在这类系统中，socket是文件的一种。文件系统执行简单的检查并调用socket结构中指向的socket函数。<br>内核中的socket包含两个缓冲区：一个用于缓冲要发送的数据；一个用于缓冲要接收的数据。<br>当write()系统调用被调用时，用户区的数据被拷贝到内核内存中，并插入到socket的发送缓冲区，这样来保证发送的数据有序。之后，TCP被调用了。<br>socket会关联一个叫做TCP控制块（TCP Control Block，TCB）的结构，TCB包含了处理TCP连接所需的数据，包括连接状态，接收窗口，阻塞窗口，序列号等。<br>如果当前的TCP状态允许数据传输，一个新的TCP分段（TCP segment）将被创建，当然，如果由于流量控制或其他原因不能传输数据，系统调用将会结束，之后返回用户态，即将控制权交回到应用程序代码。<br>TCP分段有两部分：TCP头和携带的数据。     </p><p><img src="http://i.imgur.com/nGYgq7F.png" alt=""><br>图2 TCP帧的结构    </p><p>TCP数据包的payload部分会包含在socket发送缓冲区里的没有应答的数据。携带数据的最大长度是接收窗口、阻塞窗口和最大分段长度（MSS）中的最大值。之后会计算TCP校验值。在计算时，头信息（ip地址、分段长度和端口号）会包含在内。根据TCP状态可发送一个或多个数据包。事实上，当前的网络栈使用了校验卸载（checksum offload），TCP校验和会由NIC计算，而不是内核。但是，为了解释方便我们还是假设内核计算校验和。<br>被创建的TCP分段继续走到下面的IP层。IP层向TCP分段中增加了IP头并且执行了IP路由。IP路由是寻找到达目的IP的下一条IP地址的过程。<br>在IP层计算并增加了IP头校验和之后，它把数据发送到链路层。链路层通过地址解析协议（Address Resolution Protocol，ARP）搜索下一跳IP地址对应的MAC地址。之后它会向数据包中增加链路头，在增加链路头之后主机要发送的数据包就是完整的了。<br>在执行IP路由时，会选择一个传输接口（NIC）。接口被用于把数据包传送至下一跳IP。于是，用于发送的NIC驱动程序被调用了。<br>在这个时候，如果正在执行数据包捕获程序（例如tcpdump或wireshark）的话，内核将把数据包拷贝到这些程序的内存缓冲区中。相同的方式，接收的数据包直接在驱动被捕获。<br>在接收到数据包传输请求之后，NIC把数据包从系统内存中拷贝到它自己的内存中，之后把数据包发送到网络上。在此时，由于要遵守以太网标准（Ethernet standard），NIC会向数据包中增加帧间隙（Inter-Frame Gap，IFG），同步码（preamble）和crc校验和。帧间隙和同步码用于区分数据包的开始（网络术语叫做帧，framing），crc用于保护数据（与TCP或者IP校验和的目的相同）。NIC会基于以太网的物理速度和流量控制决定数据包开始传输的时间。<br>当NIC发送了数据包，NIC会在主机的CPU上产生中断。所有的中断会有自己的中断号，操作系统会用这个中断号查找合适的程序去处理中断。驱动程序在启动时会注册一个处理中断的函数。操作系统调用中断处理程序，之后中断处理程序会把已发送的数据包返回给操作系统。   </p><h3 id="数据接收"><a href="#数据接收" class="headerlink" title="数据接收"></a>数据接收</h3><p>现在我们看一下数据是如何被接收的。数据接收是网络栈如何处理流入数据包的过程。     </p><p><img src="http://i.imgur.com/969v08v.png" alt=""><br>图3 接收数据时网络栈中各层对数据的操作     </p><p>首先，NIC把数据包写入它自身的内存。通过CRC校验检查数据包是否有效，之后把数据包发送到主机的内存缓冲区。这里说的缓冲区是驱动程序提前向内核申请好的一块内存区域，用于存放接收的数据包。在缓冲区被系统分配之后，驱动会把这部分内存的地址和大小告知NIC。如果主机没有为驱动程序分配缓冲区，那么当NIC接收到数据包时有可能会直接丢弃它。<br>在把数据包发送到主机缓冲区之后，NIC会向主机发出中断。之后，驱动程序会判断它是否能处理新的数据包。到目前为止使用的是由制造商定义的网卡驱动的通讯协议。<br>当驱动应该向上层发送数据包时，数据包必须被放进一个操作系统能够理解和使用的数结构。例如Linux的sk_buff，或者BSD系列内核的mbuf，或者windows的NET_BUF_LIST，驱动会把数据包包装在指定的数据结构，并发送到上一层。<br>链路层会检查数据包是否有效并且解析出上层的协议（网络协议）。此时它会判断链路头中的以太网类型。（IPv4的以太网类型是0×0800）。它会把链路头删掉并且把数据包发送到IP层。<br>IP层同样会检查数据包是否有效。或者说，会检查IP头校验和。它会执行IP路由判断，判断是由本机处理数据包还是把数据包发送到其它系统。如果数据包必须由本地系统处理，IP层会通过IP header中引用的原型值（proto value）解析上层协议（传输协议）。TCP原型值为6。系统会删除IP头，并且把数据包发送到TCP层。<br>就像之前的几层，TCP层检查数据包是否有效，同时会检查TCP校验和。就像之前提到的，如果当前的网络栈使用了校验卸载，那么TCP校验和会由NIC计算，而不是内核。<br>之后它会查找数据包对应的TCP控制块（TCB），这时会使用数据包中的&lt;源ip，源端口，目的IP，目的端口&gt;做标识。在查询到对应的连接之后，会执行协议中定义的操作去处理数据包。如果接收到的是新数据，数据会被增加到socket的接收缓冲区。根据tcp连接的状态，此时也可以发送新的TCP包（比如发送ACK包）。此时，TCP/IP接收数据包的处理完成。<br>socket接收缓冲区的大小就是TCP接收窗口。TCP吞吐量会随着接收窗口变大而增加。过去socket缓冲区大小是应用或操作系统的配置的定值。最新的网络栈使用了一个函数去自动决定接收缓冲区的大小。<br>当应用程序调用read系统调用时，程序会切换到内核区，并且会把socket接收缓冲区中的数据拷贝到用户区。拷贝后的数据会从socket缓冲区中移除。之后TCP会被调用，TCP会增加接收窗口的大小（因为缓冲区有了新空间）。并且会根据协议状态发送数据包。如果没有数据包传送，系统调用结束。</p><h3 id="如何处理中断和接收数据包"><a href="#如何处理中断和接收数据包" class="headerlink" title="如何处理中断和接收数据包"></a>如何处理中断和接收数据包</h3><p>这一部分内容已经在<a href="http://blog.dujiong.net/2017/04/13/trace-linux-networking-stack-receiving-data/" target="_blank" rel="noopener">Linux网络协议栈数据处理流程</a>一文中详细叙述过。<br>中断处理过程是复杂的，但是你需要了解数据包接收和处理流程中的和性能相关的问题。图4展示了一次中断的处理流程。  </p><p><img src="http://i.imgur.com/dmtkwf5.png" alt=""><br>图4 处理中断、软中断和接收数据    </p><p>假设CPU0正在执行应用程序，在此时，NIC接收到了一个数据包并且在CPU0上产生了中断。CPU0执行了内核中断处理程序（irq）。这个处理程序关联了一个中断号并且内核会调用驱动里对应的中断处理程序。驱动在释放已经传输的数据包之后调用napi_schedule()函数去处理接收到的数据包，这个函数会请求软中断。在执行了驱动的中断处理程序后，控制权被转移到内核中断处理程序。内核中的处理程序会执行软中断的处理程序。<br>在中断上下文被执行之后，软中断的上下文会被执行。中断上下文和软中断上下文会通过相同的线程执行，但是它们会使用不同的栈。并且中断上下文会屏蔽硬件中断；而软中断上下文不会屏蔽硬件中断。<br>处理接收到的数据包的软中断处理程序是net_rx_action()函数。该函数会调用驱动程序的poll()函数。而poll()函数会调用netif_receive_skb()函数，并把接收到的数据包一个接一个的发送到上层。在软中断处理后，应用程序会从停止的位置重新开始执行。<br>因此，接收中断请求的CPU会负责处理接收数据包从始至终的整个过程。在Linux、BSD和Windows中，处理过程基本是类似的。    </p><h3 id="相关数据结构"><a href="#相关数据结构" class="headerlink" title="相关数据结构"></a>相关数据结构</h3><h4 id="sk-buff"><a href="#sk-buff" class="headerlink" title="sk_buff"></a>sk_buff</h4><p>首先，sk_buff结构或skb结构代表一个数据包，图5展现了sk_buff中的一些结构。随着功能变得更强大，它们也变得更复杂了。     </p><p><img src="http://i.imgur.com/r1C515B.png" alt=""><br>图5 数据包结构    </p><p>这个结构直接包含或者通过指针引用了数据包。一些必要的信息比如头和内容长度被保存在元数据区。例如，在图5中，mac_header、network_header和transport_header都有相应的指针，指向链路头、IP头和TCP头的起始地址。这种方式让TCP协议处理过程变得简单。<br>数据包在网络栈的各层中上升或下降时会增加或删除数据头。为了更有效率的处理而使用了指针。例如，要删除链路头只需要修改head pointer的值。     </p><h4 id="TCP控制块"><a href="#TCP控制块" class="headerlink" title="TCP控制块"></a>TCP控制块</h4><p>其次，有一个表示TCP连接的数据结构，之前它被抽象的叫做TCP控制块。Linux使用了tcp_sock这个数据结构。在图6中，你可以看到文件、socket和tcp_socket的关系。    </p><p><img src="http://i.imgur.com/BJI48GO.png" alt=""><br>图6 TCP Connection结构</p><p>当系统调用发生后，它会找到应用程序在进行系统调用时使用的文件描述符对应的文件。对Unix系的操作系统来说，文件本身和通用文件系统存储的设备都被抽象成了文件。因此，文件结构包含了必要的信息。对于socket来说，使用独立的socket结构保存socket相关的信息，文件系统通过指针来引用socket。socket又引用了tcp_sock。tcp_sock可以分为sock,inet_sock等等,用来支持除了TCP之外的协议，可以认为这是一种多态。<br>所有TCP协议用到的状态信息都被存在tcp_sock里。例如顺序号、接收窗口、阻塞控制和重发送定时器都保存在tcp_sock中。<br>socket的发送缓冲区和接收缓冲区由sk_buff链表组成并被包含在tcp_sock中。为防止频繁查找路由，也会在tcp_sock中引用IP路由结果dst_entry。通过dst_entry可以简单的查找到目标的MAC地址之类的ARP的结果。dst_entry是路由表的一部分，而路由表是个很复杂的结构，在这篇文档里就不再讨论了。用来传送数据的网卡(NIC)可以通过dst_entry找到。网卡通过net_device描述。<br>因此，仅通过查找文件和指针就可以很简单的查找到处理TCP连接的所有的数据结构（从文件到驱动）。这个数据结构的大小就是每个TCP连接占用内存的大小。这个结构占用的内存只有几kb大小（排除了数据包中的数据）。但随着一些功能被加入，内存占用也在逐渐增加。<br>最后，我们来看一下TCP连接查找表（TCP connection lookup table）。这是一个用来查找接收到的数据包对应tcp连接的哈希表。系统会用数据包的&lt;来源ip，目标ip，来源端口，目标端口&gt;和Jenkins哈希算法去计算哈希值。选择这个哈希函数的原因是为了防止对哈希表的攻击。</p><h3 id="追踪代码：传输数据"><a href="#追踪代码：传输数据" class="headerlink" title="追踪代码：传输数据"></a>追踪代码：传输数据</h3><p>我们将会通过追踪实际的Linux内核源码去检查协议栈中执行的关键任务。这里以发送数据的执行路径为例。<br>首先是应用程序调用write系统调用。当应用调用了write系统调用时，内核将在文件层执行write()函数。首先，内核会取出文件描述符对应的文件结构体，之后会调用aio_write，这是一个函数指针。在文件结构体中，你可以看到file_perations结构体指针。这个结构被通称为函数表（function table），其中包含了一些函数的指针，比如aio_read或者aio_write。对于socket来说，实际的表是socket_file_ops，aio_write对应的函数是sock_aio_write。在这里函数表的作用类似于java中的interface，内核使用这种机制进行代码抽象或重构。</p><pre><code>static ssize_t sock_aio_write(struct kiocb *iocb, const struct iovec *iov, ..) {    /* ... */    struct socket *sock = file-&gt;private_data;    /* ... */    return sock-&gt;ops-&gt;sendmsg(iocb, sock, msg, size);    struct socket {        /* ... */        struct file *file;        struct sock *sk;        const struct proto_ops *ops;    };        struct proto_ops {    `    /* ... */        int (*connect) (struct socket *sock, ...)        int (*accept) (struct socket *sock, ...)        int (*listen) (struct socket *sock, int len);        int (*sendmsg) (struct kiocb *iocb, struct socket *sock, ...)        int (*recvmsg) (struct kiocb *iocb, struct socket *sock, ...)        /* ... */    };};</code></pre><p>sock_aio_write()函数会从文件结构体中取出socket结构体并调用sendmsg，这也是一个函数指针。socket结构体中包含了proto_ops函数表。IPv4的TCP实现中，proto_ops的具体实现是inet_stream_ops，sendmsg的实现是tcp_sendmsg。<br>tcp_sengmsg会从socket中取得tcp_sock（也就是TCP控制块，TCB），并把应用程序请求发送的数据拷贝到socket发送缓冲中(根据发送数据创建sk_buff链表),当把数据拷贝到sk_buff中时，每个sk_buff会包含多少字节数据？在代码创建数据包时，每个sk_buff中会包含MSS字节(通过tcp_send_mss函数获取)，在这里MSS表示每个TCP数据包能携带数据的最大值。通过使用TSO(TCP Segment Offload)和GSO(Generic Segmentation Offload)技术，一个sk_buff可以保存大于MSS的数据。在这篇文章里就不详细解释了。<br>sk_stream_alloc_skb函数会创建新的sk_buff，之后通过skb_entail把新创建的sk_buff放到send_socket_buffer的末尾。skb_add_data函数会把应用层的数据拷贝到sk_buff的buffer中。通过重复这个过程（创建sk_buff然后把它加入到socket发送缓冲区）完成所有数据的拷贝。因此，大小是MSS的多个sk_buff会在socket发送缓冲区中形成一个链表。最终调用tcp_push把待发送的数据做成数据包，并且发送出去。<br>tcp_push函数会在TCP允许的范围内顺序发送尽可能多的sk_buff数据。首先会调用tcp_send_head取得发送缓冲区中第一个sk_buff，然后调用tcp<br>_cwnd_test和tcp_send_wnd_test检查堵塞窗口和接收窗口，判断接收方是否可以接收新数据。之后调用tcp_transmit_skb函数来创建数据包。</p><pre><code>static int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb,int clone_it, gfp_t gfp_mask) {    const struct inet_connection_sock *icsk = inet_csk(sk);    struct inet_sock *inet;    struct tcp_sock *tp;    /* ... */    if (likely(clone_it)) {        if(unlikely(skb_cloned(skb)))            skb = pskb_copy(skb, gfp_mask);        else            skb = skb_clone(skb, gfp_mask);        if (unlikely(!skb))            return -ENOBUFS;    }    skb_push(skb, tcp_header_size);    skb_reset_transport_header(skb);    skb_set_owner_w(skb, sk);    /* Build TCP header and checksum it */    th = tcp_hdr(skb);    th-&gt;source = inet-&gt;inet_sport;    th-&gt;dest = inet-&gt;inet_dport;    th-&gt;seq = htonl(tcb-&gt;seq);    th-&gt;ack_seq = htonl(tp-&gt;rcv_nxt);    /* ... */    return net_xmit_eval(err);}</code></pre><p>tcp_transmit<br>_skb会创建指定sk_buff的拷贝（通过pskb_copy)，但它不会拷贝应用层发送的数据，而是拷贝一些元数据。之后会调用skb_push来确保和记录头部字段的值。send_check计算TCP校验和（如果使用校验和卸载checksum offload技术则不会做这一步计算）。最终调用queue_xmit把数据发送到IP层。IPv4中queue_xmit的实现函数是ip_queue_xmit。<br>ip_queue_xmit函数执行IP层的一些必要的任务。__sk_dst_check检查缓存的路由是否有效。如果没有被缓存的路由项，或者路由无效，它将会执行IP路由选择（IP routing)。之后调用skb_push来计算和记录IP头字段的值。之后，随着函数执行，ip_send_check计算IP头校验和并且调用netfilter功能。如果使用ip_finish_output函数会创建IP数据分片，但在使用TCP协议时不会创建分片，因此内核会直接调用ip_finish_output2来增加链路头，并完成数据包的创建。<br>最终的数据包会通过dev_queue_xmit函数完成传输。首先，数据包通过排队规则传递。如果使用了默认的排队规则并且队列是空的，那么会跳过队列而直接调用sch_direct_xmit把数据包发送到驱动。dev_hard_start_xmit会调用实际的驱动程序。在调用驱动之前，设备的发送被锁定，防止多个线程同时使用设备。由于内核锁定了设备的发送，驱动发送数据相关的代码就不需要额外的锁了。<br>ndo_start_xmit函数会调用驱动的代码。在这之前，你会看到ptype_all和dev_queue_\xmit_nit。ptype_all是个包含了一些模块的列表（比如数据包捕获）。如果捕获程序正在运行，数据包会被ptype_all拷贝到其它程序中。因此，tcpdump中显示的都是发送给驱动的数据包。当使用了校验和卸载(checksum offload)或TSO(TCP Segment Offload)这些技术时，网卡（NIC）会操作数据包，所以tcpdump得到的数据包和实际发送到网络的数据包有可能不一致。在结束了数据包传输以后，驱动中断处理程序会返回发送了的sk_buff。      </p><h3 id="驱动和网卡如何通信"><a href="#驱动和网卡如何通信" class="headerlink" title="驱动和网卡如何通信"></a>驱动和网卡如何通信</h3><p>驱动（driver）和网卡（NIC）之间的通讯处于协议栈的底层，大多数人并不关心。但是，为了解决性能问题，网卡会处理越来越多的任务。理解基础的处理方式会帮助你理解额外这些优化技术。<br>网卡和驱动之间使用异步通讯。首先，驱动请求数据传输时CPU不会等待结果而是会继续处理其它任务，之后网卡发送数据包并通知CPU，驱动程序返回通过事件接收的数据包（这些数据包可以看作是异步发送的返回值）。<br>和数据包传输类似，数据包的接收也是异步的。首先驱动请求接受接收数据包然后CPU去执行其它任务，之后网卡接收数据包并通知CPU，然后驱动处理接收到的数据包（返回数据）。<br>因此需要有一个空间来保存请求和响应（request and response）。大多数情况网卡会使用环形队列数据结构（ring structure）。环形队列类似于普通的队列，其中有固定数量的元素，每个元素会保存一个请求或一个相应数据。环形队列中元素是顺序的，“环形”的意思是队列虽然是定长的，但是其中的元素会按顺序重用。<br>图7展示了数据包的传输过程，会看到其中如何使用环形队列。    </p><p><img src="http://i.imgur.com/kVn0OjV.png" alt=""><br>图7 驱动和网卡之间的通讯：传输数据包  </p><p>驱动接收到上层发来的数据包并创建网卡能够识别的描述符。发送描述符（send descriptor）默认会包含数据包的大小和内存地址。这里网卡需要的是内存中的物理地址，驱动需要把数据包的虚拟地址转换成物理地址。之后，驱动会把发送描述符添加到发送环形队列（TX ring）（1），发送环形队列中包含的实际是发送描述符。<br>之后，驱动会把请求通知给网卡（2）。驱动直接把数据（通知）写入指定的网卡的寄存器地址中。<br>网卡被通知后从主机的发送队列中取得发送描述符（3）。这种设备直接访问内存而不需要调用CPU的内存访问方式叫做直接内存访问（Direct Memory Access，DMA）。<br>在取得发送描述符后，网卡会得到数据包的地址和大小并且从主机内存中取得实际的数据包（4）。如果有校验和卸载（checksum offload）的话，网卡会在拿到数据后计算数据包的校验和，因此开销不大。<br>网卡发送数据包（5）之后把发送数据包的数量写入主机内存（6）。之后它会触发一次中断，驱动程序会读取发送数据包的数量并根据数量返回已发送的数据包。     </p><p>图8展示了接收数据包的过程。</p><p><img src="http://i.imgur.com/zKf9nxU.png" alt=""><br>图8 驱动程序和网卡之间的通讯：接收数据包    </p><p>调优网络栈时，大部分人会说环形队列和中断的设置需要被调整。当发送环形队列很大时，很多次的发送请求可以一次完成；当接收环形队列很大，可以一次性接收多个数据包。更大的环形队列对于大流量数据包接收/发送是很有用的。由于CPU在处理中断时有大量开销，大量大多数情况下，网卡使用一个计时器来减少中断。为了避免对宿主机过多的中断，发送和接收数据包的时候中断会被收集起来并且定期调用（interrupt coalescing，中断聚合）。</p><h3 id="网络栈中的缓冲区和流量控制"><a href="#网络栈中的缓冲区和流量控制" class="headerlink" title="网络栈中的缓冲区和流量控制"></a>网络栈中的缓冲区和流量控制</h3><p>在网络栈中流量控制在几个阶段被执行。图9展示了传输数据时的一些缓冲区。首先，应用会创建数据并把数据加入到socket发送缓冲区。如果缓冲区中没有剩余空间的话，系统调用会失败或阻塞应用进程。因此，应用程序到内核的发送速率由socket缓冲区大小来限制。</p><p><img src="http://i.imgur.com/qu43rVe.png" alt=""><br>图9 数据发送相关的缓冲</p><p>TCP通过传输队列（qdisc）创建并把数据包发送给驱动程序。这是一个典型的先入先出队列，队列最大长度是txqueuelen，可以通过ifconfig命令来查看实际大小。通常来说，大约有几千个数据包。<br>驱动和网卡之间是传输环形队列（TX ring），它被认为是传输请求队列（transmission request queue）。如果队列中没有剩余空间的话就不会再继续创建传输请求，并且数据包会积累在传输队列中，如果数据包积累的太多，那么新的数据包会被丢弃。<br>网卡会把要发送的数据包保存在内部缓冲区中。这个队列中的数据包速度受网卡物理速度的影响（例如，1Gb/s的网卡不能承担10Gb/s的性能）。根据以太网流量控制，当网卡的接收缓冲区没有空间时，数据包传输会被停止。<br>当内核速度大于网卡时，数据包会堆积在网卡的缓冲区中。如果缓冲区中没有空间时会停止处理传输环形队列（TX ring）。越来越多的请求堆积在传输环形队列中，最终队列中空间被耗尽。驱动程序不能再继续创建传输请求数据包会堆积在传输队列（transmit queue）中。压力通过各种缓冲从底向上逐级反馈。<br>图10展示了接收数据包经过的缓冲区。数据包先被保存在网卡的接收缓冲区中。从流量控制的视角来看，驱动和网卡之间的接收环形缓冲区(RX ring)可以被看作是数据包的缓冲区。驱动程序从环形缓冲区取得数据包并把它们发送到上层。服务器系统的网卡驱动默认会使用NAPI，所以在驱动和上层之间没有缓冲区。因此，可以认为上层直接从接收环形缓冲区中取得数据，数据包的数据部分被保存在socket的接收缓冲区中。应用程序从socket接收缓冲区取得数据。 </p><p><img src="http://i.imgur.com/CB8RYpt.png" alt=""><br>图10 与接收数据包相关的缓冲   </p><p>不支持NAPI的驱动程序会把数据包保存在积压队列（backlog queue）中。之后，NAPI处理程序取得数据包，因此积压队列可以被认为是在驱动程序和上层之间的缓冲区。<br>如果内核处理数据包的速度低于网卡的速度，接收循环缓冲区队列(RX ring)会被写满，网卡的缓冲区空间(NIC internal buffer)也会被写满，当使用了以太流量控制（Ethernet flow control）时，接收方网卡会向发送方网卡发送请求来停止传输或丢弃数据包。<br>因为TCP支持端对端流量控制，所以不会出现由于socket接收队列空间不足而丢包的情况。但是，当使用UDP协议时，因为UDP协议不支持流量控制，如果应用程序处理速度不够的时候会出现socket接收缓冲区空间不足而丢包的情况。<br>在图9和图10中展示的传输环形队列（TX ring）和接收环形队列（RX ring）的大小可以用ethtool查看。在大多数看重吞吐量的负载情况下，增加环形队列的大小和socket缓冲区大小会有一些帮助。增加大小会减少高速收发数据包时由于缓冲区空间不足而造成的异常。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>原文写的很好，共勉！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;看到一篇讲解TCP/IP协议栈原理及其处理数据包流程很好的文章，特记录下来。  &lt;/p&gt;
    
    </summary>
    
    
    
      <category term="network/linux" scheme="http://blog.jonnydu.me/tags/network-linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux网络协议栈数据处理流程---收包</title>
    <link href="http://blog.jonnydu.me/2017/04/13/trace-linux-networking-stack-receiving-data/"/>
    <id>http://blog.jonnydu.me/2017/04/13/trace-linux-networking-stack-receiving-data/</id>
    <published>2017-04-13T06:15:59.000Z</published>
    <updated>2020-04-30T14:16:30.249Z</updated>
    
    <content type="html"><![CDATA[<p>本文详细记录了从网卡驱动加载和初始化开始，一直到网络数据包到达，最后驱动程序将其递交到网络协议栈的整个处理过程。</p><a id="more"></a><p>下文中出现的代码均基于Linux内核3.13.0版本，网卡驱动igb。</p><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>从整体上看，Linux网络协议栈对数据包的处理流程如下：    </p><ol><li>网卡驱动的加载和初始化   </li><li>数据包到达网卡   </li><li>网卡将数据包通过DMA的方式写入到指定的内存地址，该地址由网卡驱动分配并初始化    </li><li>网卡通过硬件中断的方式通知CPU有数据到来        </li><li>在中断处理程序中，驱动程序调用NAPI进入软中断处理                        </li><li>软中断进程ksoftirqd通过调用NAPI函数poll循环接收数据包，软中断进程运行在系统的每个CPU上。        </li><li>数据被传递到skb buffer中，随后递交网络协议栈       </li><li>如果RPS（Receive Packet Steering）被打开或者NIC有多个接收队列，接收到的数据包将分散在多个CPU中，这里涉及到多CPU的负载均衡     </li><li>数据帧从队列中递交到网络协议栈   </li><li>协议栈处理数据   </li><li>数据被添加到socket缓冲区   </li></ol><p>下面将详细描述每一步所经过的处理。</p><h3 id="网卡驱动的加载和初始化"><a href="#网卡驱动的加载和初始化" class="headerlink" title="网卡驱动的加载和初始化"></a>网卡驱动的加载和初始化</h3><p>网卡驱动程序向内核注册一个初始化函数，当驱动加载的时候，该函数被内核调用。Linux内核中通过module_init宏进行注册。<br>网卡需要有驱动才能工作，驱动是加载到内核中的模块，负责衔接网卡和内核。驱动在加载的时候将自己注册进网络模块，当相应的网卡收到数据包时，网络模块会调用相应的驱动程序处理数据。<br>Linux内核中igb初始化函数(igb_init_module)和驱动注册函数(module_init）<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L676-L697" target="_blank" rel="noopener">源码地址</a>，可以看出，igb_init_module函数的大部分工作是通过pci_register_driver来完成的。<br><img src="http://i.imgur.com/DBOCp1s.png" alt="">     </p><h4 id="PCI设备列表"><a href="#PCI设备列表" class="headerlink" title="PCI设备列表"></a>PCI设备列表</h4><p>我们知道，一个驱动程序可以支持一个或多个设备，而一个设备只会绑定给一个驱动程序。因此，驱动程序将其支持的所有设备保存在一个列表（PCI device IDs）中，内核使用这些表来决定加载驱动的类型。<br>Linux内核中igb驱动程序所支持的PCI设备列表<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L79-L117" target="_blank" rel="noopener">源码地址</a></p><pre><code>static DEFINE_PCI_DEVICE_TABLE(igb_pci_tbl) = {    { PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_BACKPLANE_1GBPS) },      { PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_SGMII) },      { PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_BACKPLANE_2_5GBPS) },      { PCI_VDEVICE(INTEL, E1000_DEV_ID_I211_COPPER), board_82575 },      { PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_COPPER), board_82575 },      { PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_FIBER), board_82575 },      { PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SERDES), board_82575 },      { PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SGMII), board_82575 },      { PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_COPPER_FLASHLESS), board_82575 },      { PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SERDES_FLASHLESS), board_82575 },    /* ... */};MODULE_DEVICE_TABLE(pci, igb_pci_tbl);</code></pre><p>如前文所述，igb初始化函数igb_init_module的大部分工作由pci_register_driver完成。后者通过一系列函数指针的赋值以及PCI设备列表的注册完成了PCI网卡设备的初始化。</p><pre><code>static struct pci_driver igb_driver = {    .name     = igb_driver_name,    .id_table = igb_pci_tbl,    .probe    = igb_probe,    .remove   = igb_remove,    /* ... */        };</code></pre><h4 id="PCI-probe"><a href="#PCI-probe" class="headerlink" title="PCI probe"></a>PCI probe</h4><p>一旦PCI设备通过其PCI ID被内核识别，内核就可以选择合适的驱动来控制该设备。这个过程是通过PCI驱动程序向内核PCI子系统注册的探测函数（probe function）完成的。其处理流程包括：       </p><ol><li>打开PCI设备       </li><li>申请内存和I/O端口     </li><li>设置DMA参数    </li><li>注册驱动支持的ethtool调用函数              </li><li>struct net_device_ops结构的创建、初始化和注册。该结构体包含了指向打开设备、发送数据、设置MAC地址等操作函数的函数指针。   </li><li>struct net_device结构体的创建、初始化和注册。sturct net_device代表一个网络设备。 </li></ol><p>下面是ibg驱动程序中igb_probe函数的部分代码。<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2016-L2429" target="_blank" rel="noopener">源码地址</a>   </p><pre><code>err = pci_enable_device_mem(pdev)/* ... */err = dma_set_mask_and_coherent(&amp;pdev-&gt;dev, DMA_BIT_MASK(64));  /* ... */err = pci_request_selected_regions(pdev, pci_select_bars(pdev, IORESOURCE_MEM), igb_driver_name)；pci_enable_pcie_error_report(pdev);pci_set_master(pdev);pci_save_state(pdev);/* ... */netdev-&gt;netdev_ops = &amp;igb_netdev_ops;</code></pre><p>以及net_device_ops结构体中函数指针的初始化。<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L1905-L1913" target="_blank" rel="noopener">源码地址</a></p><pre><code>struct const struct net_device_ops igb_netdev_ops = {    .ndo_open = igb_open,    .ndo_stop = igb_close,    .ndo_start_xmit = igb_xmit_frame,    .ndo_get_stats64 = igb_get_stats64,    .ndo_set_mac_address = igb_set_mac,    .ndo_change_mtu = igb_change_mtu,    .ndo_do_ioctl = igb_ioctl,    /* ... */</code></pre><p>下面说明驱动中注册ethtool调用函数的过程。<br>ethtool是一个命令行程序，可以使用它来获取并配置网卡硬件和驱动。在ubuntu系统下，可以使用<code>apt-get install ethtool</code>来安装ethtool。<br>ethtool最常见的用法是从网络设备中获取详细的统计数据。<br>ethtool通过使用ioctl系统调用来和网卡驱动打交道。网卡驱动通过注册一系列函数来运行ethtool的操作指令。当ethtool发起了一个ioctl系统调用，内核将会在合适的驱动中寻找注册的ethtool结构并执行注册的函数。<br>igb驱动程序仍然在igb_probe完成ethtool操作的注册。    </p><pre><code>static int igb_probe(stauct pci_dev* pdev, const struct pci_device_id *ent){    /* ... */    igb_set_ethtool_ops(netdev);}</code></pre><p>igb驱动中的ethtool操作的函数指针结构体及设置在 <a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_ethtool.c#L2970-L3015" target="_blank" rel="noopener">源码地址</a></p><pre><code>static const struct ethtool_ops igb_ethtool_ops = {    .get_settings = igb_get_settings,    .set_settings = igb_set_settings,    .get_drvinfo = igb_get_drvinfo,    .get_regs_len = igb_get_regs_len,    .get_regs = igb_get_regs,    /* ... */}static int igb_probe(struct pci_dev *pdev, const struct pci_devie_id *ent){    /* ... */    igb_set_ethtool_ops(netdev);}</code></pre><p>此外，需要说明的是，ethtool所实现的函数由驱动自身决定，因此，并不是所有的驱动都实现了ethtool的所有函数。</p><h4 id="更多关于Linux-PCI-PCIe驱动的信息"><a href="#更多关于Linux-PCI-PCIe驱动的信息" class="headerlink" title="更多关于Linux PCI/PCIe驱动的信息"></a>更多关于Linux PCI/PCIe驱动的信息</h4><p>PCI、PCIe设备驱动程序的细节可以参考<a href="http://wiki.osdev.org/PCI" target="_blank" rel="noopener">wiki</a>、<a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/PCI/pci.txt" target="_blank" rel="noopener">text file in the linux kernel</a>等相关资料，这里不做更深层次讲解。      </p><h3 id="中断和NAPI"><a href="#中断和NAPI" class="headerlink" title="中断和NAPI"></a>中断和NAPI</h3><p>当数据通过DMA的方式被传送到RAM之后，NIC需要一种通知机制来通告数据的到来。传统的方式是NIC产生一个硬件中断给CPU。但是，这种方法的缺陷在于，如果有大量的数据包到达，将会产生很多次的硬件中断，极大的降低了CPU处理的效率。<br>在这种情况下，NAPI（New Api）被提出，用于减少（不是消除）数据包到达时硬中断的产生次数，下面将详细介绍NAPI机制。     </p><h4 id="NAPI机制"><a href="#NAPI机制" class="headerlink" title="NAPI机制"></a>NAPI机制</h4><p>NAPI的核心概念是不采用中断的方式读取数据，而是首先采用中断唤醒数据接收的服务程序，然后使用poll方法来轮询数据。这样可以防止高频率的中断影响系统的整体运行效率。<br>当然，NAPI也存在一些缺陷，比如，对于上层的应用程序而言，系统不能在每个数据包接收到的时候都可以及时地区处理它，而且随着传输速度增加，累计的数据包将会耗费大量的内存；对大的数据包处理比较困难，原因是大的数据包传送到网络层上的时候耗费的时间比短数据包长很多。<br>NAPI机制的大致处理流程如下：<br><img src="http://i.imgur.com/sPbd3RI.jpg" alt="">          </p><ol><li>NAPI被驱动使能，但是初始时处于OFF状态    </li><li>数据包到来并通过DMA方式传送到内存    </li><li>NIC产生硬中断，触发中断处理函数      </li><li>中断处理函数中，驱动通过软中断唤醒NAPI子系统，通过在一个单独执行线程上调用注册的poll函数接收数据包    </li><li>网卡驱动程序关掉硬中断，从而允许NAPI子系统在处理数据包期间不会被中断。    </li><li>一旦处理完成，NAPI子系统被关闭，网卡硬件中断打开。        </li></ol><h4 id="igb驱动程序实现"><a href="#igb驱动程序实现" class="headerlink" title="igb驱动程序实现"></a>igb驱动程序实现</h4><p>下面结合igb驱动程序详细说明。<br>1.注册poll函数<br>设备驱动程序实现了poll函数，并通过调用netif_napi_add函数将其注册到NAPI子系统。该过程发生在驱动初始化的过程中。igb驱动中注册的相关代码<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L1145-L1271" target="_blank" rel="noopener">源码地址</a>   </p><pre><code>static int igb_alloc_q_vector(struct igb_adapter* adapter, int v_count, int v_idx, int txr_count, int trx_idx, int rxr_count, int rxr_idx){    q_vector = kzalloc(size, GFP_KERNEL);        if(!q_vector)        return -ENOMEM;    netif_napi_add(adapter-&gt;netdev, &amp;q_vector-&gt;napi, igb_poll, 64);/* ... */}   </code></pre><p>2.打开网络设备<br>当网络设备被使能（比如，ifconfig eth0 up），net_device_ops中的ndo_open所指向的函数(igb驱动中的igb_open)将会被调用。完成以下处理：<br>（1）分配RX和TX队列的内存空间；<br>（2）打开NAPI<br>（3）注册中断处理函数<br>（4）打开硬中断<br>3.网卡环形缓冲区和多队列<br>现在，大多数网卡都采用基于环形缓冲区的队列来进行DMA的传送。因此，驱动程序需要同操作系统配合为NIC预留一块可以使用的内存区域。一旦该内存区域被分配，硬件将会被通知其地址并且到达的数据将会被写入到RAM。<br>但是，如果数据包接收速率很高以致于一个CPU不能处理呢？ 这种情况下，就会导致大量丢包的发生。所以，RSS(Receive Side Scaling)和网卡多队列技术被提出。<br>RSS是一种能够在多处理器系统下使接收报文在多个CPU之间高效分发的网卡驱动技术。主要思想是基于Hash来实现动态的负载均衡，关于RSS，后面还会详细介绍。<br>而网卡多队列是指有些网卡能够同时将数据包写入到不同的区域，每一个区域都是一个单独的队列。这种情况下，操作系统可以使用多CPU在硬件层面上并行的处理到来的数据。<br>Inter I350 NIC支持多队列，可以在igb驱动程序中找到对应函数(<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2801-L2804" target="_blank" rel="noopener">源码地址</a>)   </p><pre><code>err = igb_setup_all_rx_resources(adapter);    if(err)         goto err_setup_rx;</code></pre><p>igb_setup_all_rx_resources函数调用igb_setup_rx_resources（<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L3098-L3122" target="_blank" rel="noopener">源码地址</a>）来为每一个RX队列DMA memory来存放网卡收到的数据。    </p><pre><code>static int igb_setup_rx_resources(struct igb_adapter* adapter){    struct pci_dev* pdev = adapter-&gt;pdev;    int i, err = 0;    for(i=0;i&lt;adapter-&gt;num_rx_queues;i++){        err = igb_setup_rx_resources(adapter-&gt;rx_ring[i]);        /* ... */    }    return err;}</code></pre><p>4.Enable NAPI<br>前面我们已经看到驱动程序如何将poll函数注册到NAPI子系统，但是，NAPI通常会等到设备被打开之后才会开始工作。<br>使能NAPI比较简单。在igb驱动中，调用napi_enable实现。  </p><pre><code>for(i=0;i&lt;adapter-&gt;num_q_vectors;i++){    napi_enable(&amp;(adapter-&gt;q_vector[i]-&gt;napi));}</code></pre><p>5.注册中断处理函数<br>使能NAPI后，接下来需要注册中断处理函数。通常情况下，设备可以采用不同的方式来通告中断: MSI-X, MSI和传统的中断方法。其中，MSI-X中断是较好的方法，特别是对于支持多RX队列的NIC，每个RX队列都有其特定分配的硬中断，可以被特定的CPU处理。关于这三种具体的中断机制，这里不再深究。<br>驱动程序必须采用device支持的中断方法注册合适的处理函数，以便当中断发生时，可以正常调用。<br>在igb驱动中，函数igb_msix_ring，igb_intr_msi，igb_intr分别是对应于MSI-X,MSI和legacy模式的中断处理函数。驱动程序将按照MSI-X-&gt;MSI和legacy的顺序尝试注册中断处理函数。<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L1360-L1413" target="_blank" rel="noopener">源码地址</a></p><pre><code>static int igb_request_irq(struct igb_adapter* adapter){    struct net_device* netdev = adapter-&gt;netdev;    struct pci_dev* pdev = adapter-&gt;pdev;    int err = 0;    if(adapter-&gt;msix_entries){        err = igb_request_msix(adapter);        if(!err)            goto request_done;        /* fall back to MSI */    }    /* ... */    if(adapter-&gt;flags &amp; IGB_FLAG_HAS_MSI){        err = request_irq(pdev-&gt;irq, igb_intr_msi, 0, netdev-&gt;name, adapter);        if(!err)            goto request_done;        /* fall back to MSI */    }    /* ... */    err = request_irq(pdev-&gt;irq, igb_intr, IRQF_SHARED, netdev-&gt;name, adapter);    /* ... */}</code></pre><p>以上便是igb驱动注册函数的过程，当NIC产生一个硬件中断信号表明数据包到来等待被处理时，这些函数将会执行。<br>6.Enable Interrupts<br>到这里为止，几乎所有的设置已经完成，除了打开NIC中断，等待数据包的到来。打开中断是一个硬件操作，igb驱动通过函数igb_irq_enable写寄存器实现。<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L1462-L1488" target="_blank" rel="noopener">源码地址</a></p><pre><code>static void igb_irq_enable(struct igb_adapter* adapter){    /* ... */    wr32(E1000_IMS, IMS_ENABLE_MASK | E1000_IMS_DRSTA);    wr32(E1000_IAM, IMS_ENABLE_MASK | E1000_IMS_DRSTA);    /* ... */}</code></pre><h3 id="接收数据包处理"><a href="#接收数据包处理" class="headerlink" title="接收数据包处理"></a>接收数据包处理</h3><h4 id="softirq"><a href="#softirq" class="headerlink" title="softirq"></a>softirq</h4><p>先来看下Linux内核中的软中断。<br>Linux内核中的软中断是一种在驱动层面上实现的在进程上下文之外执行中断处理函数的机制。该机制十分重要，因为硬件中断在中断处理程序执行期间可能会被关闭。关闭的时间越长，事件被错过处理的机会就越大。因此，在中断处理程序之外推迟长时间的事件处理十分重要，这样可以尽快完成中断处理程序并且重新打开设备中断。<br>软中断可以被想象成一系列的内核线程（每个CPU一个） ，这些内核线程执行针对不同软中断事件注册的事件处理函数。比如，使用top命令，在内核线程列表中看到的ksoftirqd/0，就是运行在CPU 0上的。<br>内核通过执行open_softirq函数来注册软中断处理函数。    </p><h4 id="ksoftirq"><a href="#ksoftirq" class="headerlink" title="ksoftirq"></a>ksoftirq</h4><p>软中断对于延迟设备驱动的事件处理非常重要，因此ksoftirq进程随着内核启动很早就会运行。<a href="https://github.com/torvalds/linux/blob/v3.13/kernel/softirq.c#L743-L758" target="_blank" rel="noopener">相关源码</a>     </p><pre><code>static struct smp_hotplug_thread_softirq_threads = {    .store             = &amp;ksoftirq,    .thread_should_run = ksoftirq_should_run,    .thread_fn         = run_ksoftirqd,    .thread_comm       = &quot;ksoftirqd/%u&quot;. };static __init int spawn_ksoftirqd(void) {    register_cpu_notifier(&amp;cpu_nfb);    BUG_ON(smpboot_register_percpu_thread(&amp;softirq_threads));    return 0;}  early_initcall(spawn_ksoftirqd);</code></pre><h4 id="Linux网络子系统"><a href="#Linux网络子系统" class="headerlink" title="Linux网络子系统"></a>Linux网络子系统</h4><p>到现在为止，我们已经分析了网络驱动和软中断的工作流程，接下来开始分析Linux netdevice子系统的初始化和网络数据包到达之后的处理流程。<br>netdevice子系统通过net_dev_init函数进行初始化。该函数将会对每个CPU创建struct softnet_data结构体，该结构体包含了指向（1）注册到该CPU的NAPI结构体列表；（2）数据处理的backlog值；（3）RPS（Receive packet steering）值等重要数据的指针。<br>此外，net_dev_init注册了两个软中断处理函数，分别用于处理到来和发送的数据包。</p><pre><code>static int __init net_dev_init(void){    /* ... */    open_softirq(NET_TX_SOFTIRQ, net_tc_action);    open_softirq(NET_RX_SOFTIRQ, net_rx_action);    /* ... */        }</code></pre><p>后文中，我们很快就会看到驱动处理函数怎么“触发”注册在NET_RX_SOFTIQ（NET_TX_SOFTIQ）上的net_rx_action(net_tx_action)函数。   </p><h4 id="数据包到来"><a href="#数据包到来" class="headerlink" title="数据包到来"></a>数据包到来</h4><p>终于，数据包到来了。<br>数据包到达RX队列之后，将通过DMA传输到RAM，并触发硬件中断。如前文所述，中断处理函数将尽量推迟更多的处理发生在中断上下文之外。因为在处理中断的过程中，其他中断可能会被阻塞。<br>下面是igb驱动程序中MSI-X中断处理函数的<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L5148-L5158" target="_blank" rel="noopener">源代码</a>，从中我们可以看出，中断处理程序的代码逻辑非常简单，调用igb_write_itr和napi_schedule两个函数完成快速处理。前者用于更新特定于硬件的寄存器。后者唤醒NAPI处理循环，用于在软中断中处理数据包。       </p><pre><code>static irqreturn_t igb_msix_ring(int irq, void* data){    struct igb_q_vector* q_vector = data;    /* Write the ITR value calculated from the previous interrupt */    igb_write_itr(q_vector);    napi_schedule(&amp;q_vector-&gt;napi);    return IRQ_HANDLED;}</code></pre><p>napi_schedule是__napi_schedule函数的封装。后者主要完成了两个处理，一个是struct napi_struct添加到当前CPU所关联的结构体softnet_data中的poll_list链表中，另一个则是调用__raise_softirq_irqoff来触发NET_RX_SOFTIRQ软中断，从而执行在netdevice子系统初始化期间注册的net_rx_action函数。后文我们会看到，软中断处理函数nx_rx_action将会调用NAPI的poll函数来接收数据包。     </p><pre><code>void __napi_schedule(struct napi_struct *n){    unsigned long flags;    local_irq_save(flags);    __napi_schedule(&amp;__get_cpu_var(softnet_data), n);    local_irq_restore(flags);}EXPORT_SYMBOL(__napi_schedule);static inline void __napi_schedule(struct softnet_data *sd, struct napi_struct* napi){    list_add_tail(&amp;napi-&gt;poll_list, &amp;sd-&gt;poll_list);    __raise_softirq_irqoff(NET_RX_SOFTIRQ);}    </code></pre><p>最后，需要说明的是，中断处理程序做快速操作和将数据包接收处理推迟到软中断都是在同一CPU上进行的。这也是通常将中断处理绑定到固定CPU的原因。  </p><h4 id="多队列"><a href="#多队列" class="headerlink" title="多队列"></a>多队列</h4><p>如果NIC支持RSS、multi queue，或者想更好的利用好数据的局部性原理，我们可以设置使用一些特定的CPU来处理NIC中断。<br>在决定调整IRQ处理的参数之前，应该首先检查是否已经在后台执行irqbalance，该进程将会尝试自动平衡IRQ和CPU之间的关系，并且会覆盖用户的设置。然后在/proc/interrupts中获取NIC中每个RX队列的IRQ numbers之后，就可以通过修改/proc/irq/IRQ_NUMBER/smp_affinity来调整CPU和IRQ的对应关系。</p><h4 id="数据包处理"><a href="#数据包处理" class="headerlink" title="数据包处理"></a>数据包处理</h4><p>一旦软中断softirq开始被处理并调用net_rx_action，数据包处理流程就正式开始了。<br>net_rx_action遍历当前CPU队列中的NAPI列表，取出队列中的NAPI结构，并依次对其进行操作。正如前面提到的，net_rx_action将会调用NAPI的poll函数来接收数据包。<br>igb驱动中net_rx_action的部分实现<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c" target="_blank" rel="noopener">代码</a>如下：</p><pre><code>/* initialize NAPI */netif_napi_add(adapter-&gt;netdev, &amp;q_vector-&gt;napi, igb_poll, 64);        //64: weight/* net_rx_action */wight = n-&gt;weight;work = 0; if(test_bit(NAPI_STATE_SCHED, &amp;n-&gt;state)){    work = n-&gt;poll(n, weight);    trace_napi_poll(n);}WARN_ON_ONCE(work-&gt;weight);budget-=work;</code></pre><p>其中,weight代表RX队列的处理权重，budget表示一种惩罚措施，用于多CPU多队列之间的公平性调度。<br>接收数据包的igb_poll函数的部分实现<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L5987-L6018" target="_blank" rel="noopener">代码</a></p><pre><code>static int igb_poll(struct napi_struct* napi, int budget){    struct igb_q_vector* q_vector = container_of(napi,struct igb_q_vector,napi);    bool clean_complete = true;    #ifdef CONFIG_IGB_DCA    if (q_vector-&gt;adapter-&gt;flags &amp; IGB_FLAG_DCA_ENABLED)            igb_update_dca(q_vector);    #endif    /* ... */    if (q_vector-&gt;rx.ring)            clean_complete &amp;= igb_clean_rx_irq(q_vector, budget);     /* If all work not completed, return budget and keep polling */    if (!clean_complete)            return budget;    /* If not enough Rx work done, exit the polling mode */    napi_complete(napi);    igb_ring_irq_enable(q_vector);    return 0;}</code></pre><p>其流程主要包括：<br>1.如果内核支持DCA（Direct Cache Access），CPU缓存将会被很好的命中；<br>2.调用igb_clean_rx_irq循环处理数据包，直到处理完毕或者达到budget，即一次调度时间完成。该循环中将会完成以下处理：<br>（1）当可用缓冲buffer的时候，为到来的数据包分配buffer；<br>（2）从RX队列中取buffer数据并存储在skb结构中；<br>（3）检查取出的buffer数据是否是“End of Packet”，如果是，递交下一步处理；<br>（4）验证数据头部等信息是否正确；<br>（5）构建好的skb通过napi_gro_receive函数调用被递交到网络协议栈；<br>3.检查clean_complete标志位判定是否所有的工作已经完成，如果是，返回budget值，否则，调用napi_complete关闭NAPI，并通过igb_ring_irq_enable重新使能中断，以保证下次中断到来会重新使能NAPI。</p><h4 id="Generic-Receive-Offloading-GRO"><a href="#Generic-Receive-Offloading-GRO" class="headerlink" title="Generic Receive Offloading(GRO)"></a>Generic Receive Offloading(GRO)</h4><p>GRO是硬件优化方法LRO（Large Receive Offloading）的软件实现。<br>两种方法的主要思想都是通过合并“类似”的数据包来减少传递给网络协议栈的数据包数量，达到降低CPU利用率的目的。<br>这类优化方法可能带来的问题是信息的丢失。如果一个数据包含有一些重要的选项或标志位，这些选项或标志位可能在它和其他数据包合并的时候丢失。这也是通常不推荐使用LRO的原因，而GRO针对数据包合并的规则比较宽松。<br>GRO作为LRO的软件实现，在数据包合并的规则上显得更加严格。<br>顺便说一句，如果在使用tcpdump抓包时，看到一些很大的数据包，很有可能是系统启用了GRO。<br>比如，TCP协议需要决定是否或者什么时候将ACK合并到已存在的数据包中。<br>一旦dev_gro_receive执行完成，napi_skb_finish被调用，该函数或者释放不再需要的数据结构，因为数据包已经被合并；或者调用netif_receive_skb将数据传输到网络协议栈。<br>napi_gro_receive将通过GRO完成网络数据的处理，并将数据传送到网络协议栈。其中大部分的处理逻辑通过调用函数dev_gro_receive来完成。<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3826-L3916" target="_blank" rel="noopener">相关源码</a><br>当dev_gro_receive执行完成后，napi_skb_finish函数被调用，完成多余数据结构的清理（因为数据包已经被合并），或者调用netif_receive_skb将数据传送给网络协议栈。 </p><h4 id="RSS和RPS"><a href="#RSS和RPS" class="headerlink" title="RSS和RPS"></a>RSS和RPS</h4><p>在讲解netif_receive_skb之前，我们先来了解下Receive Packet Steering（RPS）和Receive Slide Steering（RSS）这两种用于多CPU负载均衡的处理机制。    </p><p>RSS用于多队列网卡，能够将网络流量分载到多个CPU上，降低单个CPU的占有率。默认情况下，每个CPU核对应一个RSS队列，驱动程序将收到的数据包的源、目的IP和端口号等，交由网卡硬件计算出一个hash值，再根据这个hash值来决定将数据包分配到哪个队列中。<br>可以看出，RSS是和硬件相关联的，必须要有网卡的硬件进行支持。<br>RPS可以认为是RPS的软件实现。RPS主要是把软中断负载均衡到各个CPU。简单地说，是网卡驱动对每个流生成一个hash标识，然后由中断处理程序根据这个hash表示将流分配到相应的CPU上，这样就可以比较充分地发挥多核的能力。<br>可以看出，RPS是在软件层面模拟实现硬件的多队列网卡功能，主要针对单队列网卡多CPU环境，如果网卡本身支持多队列的话RPS就不会有任何的功能。</p><p>因此，netif_receive_skb将根据是否设置RPS对数据包进行不同的操作：<br>（1）no RPS<br>如果没有配置RPS，netif_receive_skb将会调用__netif_receive_skb，后者在做一些信息的记录之后，调用__netif_receive_skb_core将数据移动到协议栈。<br>（2）RPS<br>如果RPS被打开，netif_receive_skb将会调用get_rps_cpu来计算hash并决定使用哪个CPU的积压队列。<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3699-L3705" target="_blank" rel="noopener">源码地址</a>，具体的入队操作则由get_rps_cpu调用enqueue_to_backlog完成。   </p><pre><code>cpu = get\_rps\_cpu(skb-&gt;dev, &amp;rflows);if(cpu&gt;=0){    ret = enqueue_to_backlog(skb, cpu, &amp;rflow-&gt;last_qtail);    rcu_read_unlock();    return ret;    }</code></pre><p>enqueue_to_backlog首先得到指向CPU的softnet_data结构体的指针，该结构体中含有指向input_pkt_queue队列结构的指针。因此，enqueue_to_backlog函数首先检查input_pkt_queue队列长度<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3199-L3200" target="_blank" rel="noopener">源码</a>,只有当队列长度同时不超过netdev_max_backlog和flow limit的值，数据包才会入队，否则将会被丢弃。    </p><pre><code>qlen = skb_queue_len(&amp;sd-&gt;input_pkt_queue);if(qlen &lt;= netdev_max_backlog &amp;&amp; !skb_flow_limit(skb, qlen)){    if(skb_queue_len(&amp;sd-&gt;input_pkt_queue)){enqueue:        __skb_queue_tail(&amp;sd-&gt;input_pkt_queue, skb);        input_queue_tail_incr_save(sd, qtail);        rps_unlock(sd);        local_irq_restore(flags);        return NET_RX_SUCCESS;    }}</code></pre><p>同没有配置RPS时一样，backlog队列中的数据将通过__netif_receive_skb（实际处理发生在__netif_receive_skb_core）函数出队并传递给网络协议栈。__netif_receive_skb_core首先得到接收数据包的协议类型字段，然后遍历为该协议类型注册的传递函数列表进行数据传送。<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3548-L3554" target="_blank" rel="noopener">部分源码</a>    </p><pre><code>type = skb-&gt;protocol;list_for_each_entry_rcu(ptype, &amp;ptype_all, list) {    if (!ptype-&gt;dev || ptype-&gt;dev == skb-&gt;dev) {        if (pt_prev)            ret = deliver_skb(skb, pt_prev, orig_dev);        pt_prev = ptype;    }}</code></pre><h4 id="协议栈处理"><a href="#协议栈处理" class="headerlink" title="协议栈处理"></a>协议栈处理</h4><p>接下来，数据包将依次经过Linux内核中的TCP/IP协议栈，在处理完成之后添加到socket缓冲区（或者被转发、丢弃等）、等待应用层程序的读取。该过程将会在后续的文章中详细分析。   </p><h3 id="附"><a href="#附" class="headerlink" title="附"></a>附</h3><p><a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/" target="_blank" rel="noopener">Monitoring and Tuning the Linux Networking Stack: Receiving Data</a>     </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文详细记录了从网卡驱动加载和初始化开始，一直到网络数据包到达，最后驱动程序将其递交到网络协议栈的整个处理过程。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="network/linux" scheme="http://blog.jonnydu.me/tags/network-linux/"/>
    
  </entry>
  
  <entry>
    <title>NVMe over RDMA浅析</title>
    <link href="http://blog.jonnydu.me/2017/04/05/NVMe-over-RDMA/"/>
    <id>http://blog.jonnydu.me/2017/04/05/NVMe-over-RDMA/</id>
    <published>2017-04-05T12:49:59.000Z</published>
    <updated>2020-04-30T14:16:30.260Z</updated>
    
    <content type="html"><![CDATA[<p>NVMe是一种Host与SSD之间的通信协议，为了把NVMe扩展到端到端的跨网络传输，NVMe的开发者提出了NVMe over Fabrics，用于解决将NVMe置于各种传输环境下所遇到的问题。  </p><a id="more"></a> <h3 id="NVMe-over-Fabrics整体架构"><a href="#NVMe-over-Fabrics整体架构" class="headerlink" title="NVMe over Fabrics整体架构"></a>NVMe over Fabrics整体架构</h3><p>NVMe over Fabrics的整体架构如下图所示。<br><img src="http://i.imgur.com/vvD1rC7.png" alt=""><br>可以看出，NVMe Host(Controller)-side Transport Abstraction这两层便是NVMe over Fabrics协议的实现层。该协议只是一个用于NVMe Transport的抽象层而已，它并不实现真正的命令和数据传输功能，它只是为命令和数据传输定义了统一的规范，因此该协议只是“指导方针”。它是构建在”Fabrics”之上的，即它并不关心实际的Fabrics到底是什么，它只是提供了Fabrics通用的对接NVMe的接口，完成了对NVMe接口和命令在各种Fabrics而非只是PCIe上（NVMe Base协议只涉及PCIe这一种Fabric）的拓展。因此，为了使NVMe可以架构于不同的Fabric之上，各Fabric还需开发专用的功能实现层，真正实现基于此Fabric的数据传输功能，并完成和Transport Abstraction抽象层（即NVMe over Fabrics协议的实现层）的对接以使得传输抽象层可以调用到这些函数。         </p><h3 id="NVMe-over-RDMA"><a href="#NVMe-over-RDMA" class="headerlink" title="NVMe over RDMA"></a>NVMe over RDMA</h3><h4 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h4><p><a href="http://blog.dujiong.net/2017/02/27/RDMA/" target="_blank" rel="noopener">RDMA技术浅析</a>一文中介绍了RDMA这种”Fabric”以及它的几种实现方式。鉴于后续研究需要，这里选择RoCE（如无特别说明，均指的是v2版本）这种”Fabric”来展开说明。<br>NVMe over RoCE的架构如下图所示。<br><img src="http://i.imgur.com/Ukm4No4.png" alt=""><br>NVMe Host和NVMe Subsystem Controller是NVMe Base协议扩展到NVMe over Fabrics的部分；NVMe Host(Controller)-side Transport Abstraction则是NVMe over Fabrics传输抽象层的实现。RoCE层则是支持RoCE技术的网卡及相关驱动和RDMA协议栈，而不论InfiniBand、RoCE或者iWarp何种具体的RDMA实现形式，都约定提供统一的操作接口，RDMA Verbs便是RDMA技术向上层提供的接口。NVMe RDMA则是实现将RDMA的接口Verbs和NVMe对接的关键粘合层，简言之，其作用是将NVMe Transport Abstraction传输抽象层提供的传输接口可以调用到下层RDMA提供的传输接口（即verbs）。  </p><h4 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h4><p>此架构的具体操作原理将结合Linux 4.8版本内核的实现来解读。在4.8版本的Linux内核中加入了符合上图逻辑的NVMe over RDMA的代码。因此对于我们来说，只要编译最新内核并添加相关模块完成配置后，内核的代码完全不需要添加或修改。对于用户空间的应用程序来说，使用NVMe over RDMA的操作与传统的文件读写操作并无差异。这得益于Linux内核的软件分层架构，虚拟文件系统层、文件系统层和通用块层的架构可以屏蔽底层的硬件细节，且对上层提供统一的接口，这样上层用户空间的应用程序自然不会（也不应该）关注到底层硬件及使用协议的差别。Host端的Linux kernel的架构如下图所示。<br><img src="http://i.imgur.com/a9Nzq2g.png" alt=""><br>如果用户使用NVMe特有的命令，即当用户下发NVMe commands时，这需要通过Linux内核的IOCTL接口，该接口的目的本就是实现用户和底层驱动打交道。可以看到，NVMe commands通过IOCTL机制便到达NVMe Common代码层，该层代码的任务就是解析并执行各种NVMe command。在NVMe Base标准的定义中，命令是被封装在capsule这一数据结构中传输，应用将capsule压入内存中的NVMe Submission Queue，接着controller取出capsule并解析应用的command，处理之后将回复capsule压入NVMe Completion Queue，应用取出回复消息完成一次通信。具体过程参考前文<a href="http://blog.dujiong.net/2017/02/17/NVMe/" target="_blank" rel="noopener">NVMe技术浅析</a><br>而在NVMe over RDMA的环境下，传输过程如下图所示。<br><img src="http://i.imgur.com/zqCUGcT.png" alt=""><br>首先，host的command被封装进capsule后压入Host NVMe Submission Queue；接着该capsule会被放入Host RDMA Send Queue变成RDMA_SEND消息的消息负载；接着消息被host的RDMA网卡发包，当被target网卡接收后，capsule被放到target网卡的RDMA ReceIve Queue；接下来该capsule被放置到target端的内存中并由target处理该command；待处理完后，target生成回复信息（Respond Command）并封装进capsule然后将该回复capsule压入target端的NVMe Controller的Submission Queue，并经由target的网卡发送给host。这样，一次NVMe over RDMA的通信就完成了。</p><h4 id="NVMe-over-RDMA读写文件原理"><a href="#NVMe-over-RDMA读写文件原理" class="headerlink" title="NVMe over RDMA读写文件原理"></a>NVMe over RDMA读写文件原理</h4><p>下面以host读文件为例讨论数据流的传输。<br>事实上，在NVMe over Fabrics协议中，提出了两种数据传输方式，一种是将data附到capsule中，只要在传输command时负载需要传输的data即可。另一种则是直接的内存传输方式。这种方式原理上是将要传输的数据的地址和长度等元信息负载到capsule中。如用户发出read请求后，最终的capsule消息负载中会包含数据的存储地址、要读取的数据的长度以及要读到的内存地址。利用RDMA技术，这种传输的特性将表现的更加淋漓尽致。<br>由Linux内核知识可知，虚拟文件系统（VFS）向上层提供统一的文件操作接口，当用户空间进程读取文件时便调用read API；接着VFS调用到真正的文件系统（例如Ext4等）真正的read函数，真正文件系统的作用是确定数据的位置，简言之就是根据用户要读取的文件、长度、偏移量等确定文件的逻辑块号（在真实的文件存储中，一个文件是被分割成若干块存储的）；接下来内核利用通用块层（Block Layer）启动IO操作来传送所请求的数据，通用块层为所有的块设备提供了一个抽象视图，隐藏了硬件块设备间的差异，而每次IO操作由一个“块I/O”结构（struct bio）的对象来描述。至此，所有的读文件操作都是这套同样的流程。由于NVMe或AHCI都是更底层的接口标准，因此差异从通用块层之下才开始。<br>此后，进入到NVMe Transport Abstraction（即NVMe over Fabrics协议的实现层）或者NVMe over PCIe，这时便会由此生成NVMe（base或over Fabrics）标准的Read Command，由NVMe Base标准对Read Command的定义，Read Command中包含了数据应该读到的内存区域的地址（Read Command的Data Pointer字段）、以及存储数据的逻辑块号的起始地址（Starting LBA字段）等其他在read操作中必需的字段（显然地，这些信息从上层通用块层传递的bio对象中获取），并将该Read Command封装进NVMe的通信单元capsule中。接着便进入到下一层的RDMA stack。RDMA Stack之下便是支持RDMA技术的网卡驱动，最底层便是不同技术（IB/RoCE/iWarp）实现的RDMA网卡。内核的NVMe RDMA层代码完成调用RDMA的接口verbs将该capsule封装为RDMA_SEND消息的负载。<br>Target的controller处理完read command后，数据流传输便开始了。数据流的传输则完全借助于RoCE技术。由于此前在target端注册了host的内存，接下来从target的SSD中读出的数据便直接封装为RDMA_WRITE消息的负载（注意host的Read Command触发的却是target的RDMA_WRITE操作），然后将这部分数据直接从target端写入到host的内存中，而写入的地址在Read Command中便已指明。该过程的实现得益于RDMA的数据零复制技术。      </p><p>说了这么多，通过一幅图来直观展示上述NVMe over RDMA读文件的过程。<br><img src="http://i.imgur.com/Mi5Okr5.png" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文分析了将NVMe扩展到端到端的跨网络传输—NVMe over RDMA的架构和实现原理。以读文件为例，详细分析了数据流的传输过程。后续将进入实战阶段，在服务器上搭建小型NVMe存储系统，并通过RDMA进行跨网络传输。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NVMe是一种Host与SSD之间的通信协议，为了把NVMe扩展到端到端的跨网络传输，NVMe的开发者提出了NVMe over Fabrics，用于解决将NVMe置于各种传输环境下所遇到的问题。  &lt;/p&gt;
    
    </summary>
    
    
    
      <category term="NVMe over RDMA" scheme="http://blog.jonnydu.me/tags/NVMe-over-RDMA/"/>
    
  </entry>
  
  <entry>
    <title>LevelDB原理剖析</title>
    <link href="http://blog.jonnydu.me/2017/03/27/leveldb/"/>
    <id>http://blog.jonnydu.me/2017/03/27/leveldb/</id>
    <published>2017-03-27T08:49:41.000Z</published>
    <updated>2020-04-30T14:16:30.256Z</updated>
    
    <content type="html"><![CDATA[<p>LevelDB是能够处理十亿级别规模Key-Value型数据持久性存储的C++程序库，是谷歌两位大牛Jeff Dean和Sanjay Ghemawat发起的开源项目。自己已经将拜读LevelDB纳入了项目完成后的学习计划。最近偶然看到一篇关于LevelDB很好的博客，特记录下来，希望对后面的学习有帮助。</p><a id="more"></a> <p>转自：<a href="http://www.ezlippi.com/blog/2014/11/leveldb.html" target="_blank" rel="noopener">leveldb原理剖析</a></p><h3 id="LevelDB剖析之一：介绍"><a href="#LevelDB剖析之一：介绍" class="headerlink" title="LevelDB剖析之一：介绍"></a>LevelDB剖析之一：介绍</h3><p>LevelDB有如下一些特点：<br>首先，LevelDB是一个持久化存储的KV系统，和Redis这种内存型的KV系统不同，LevelDB不会像Redis一样狂吃内存，而是将大部分数据存储到磁盘上。<br>其次，LevleDB在存储数据时，是根据记录的key值有序存储的，就是说相邻的key值在存储文件中是依次顺序存储的，而应用可以自定义key大小比较函数，LevleDB会按照用户定义的比较函数依序存储这些记录。<br>再次，像大多数KV系统一样，LevelDB的操作接口很简单，基本操作包括写记录，读记录以及删除记录。也支持针对多条操作的原子批量操作。<br>另外，LevelDB支持数据快照（snapshot）功能，使得读取操作不受写操作影响，可以在读操作过程中始终看到一致的数据。<br>除此外，LevelDB还支持数据压缩等操作，这对于减小存储空间以及增快IO效率都有直接的帮助。<br>LevelDB性能非常突出，官方网站报道其随机写性能达到40万条记录每秒，而随机读性能达到6万条记录每秒。总体来说，LevelDB的写操作要大大快于读操作，而顺序读写操作则大大快于随机读写操作。      </p><h3 id="LevelDB剖析之二：整体架构"><a href="#LevelDB剖析之二：整体架构" class="headerlink" title="LevelDB剖析之二：整体架构"></a>LevelDB剖析之二：整体架构</h3><p>LevelDB本质上是一套存储系统以及在这套存储系统上提供的一些操作接口。为了便于理解整个系统及其处理流程，我们可以从两个不同的角度来看待LevleDB：静态角度和动态角度。从静态角度，可以假想整个系统正在运行过程中（不断插入删除读取数据），此时我们给LevelDB照相，从照片可以看到之前系统的数据在内存和磁盘中是如何分布的，处于什么状态等；从动态的角度，主要是了解系统是如何写入一条记录，读出一条记录，删除一条记录的，同时也包括除了这些接口操作外的内部操作比如compaction，系统运行时崩溃后如何恢复系统等等方面。<br>本节所讲的整体架构主要从静态角度来描述，之后接下来的几节内容会详述静态结构涉及到的文件或者内存数据结构，LevelDB剖析后半部分主要介绍动态视角下的LevelDB，就是说整个系统是怎么运转起来的。<br>LevelDB为存储系统，数据记录的存储介质包括内存以及磁盘文件，如果像上面说的，当LevelDB运行了一段时间，此时我们给LevelDB进行透视拍照，那么您会看到如下一番景象：<br><img src="http://i.imgur.com/CiivZ3q.png" alt=""><br>图1.1：LevelDB结构</p><p>从图中可以看出，构成LevelDB静态结构的包括六个主要部分：内存中的MemTable和Immutable MemTable以及磁盘上的几种主要文件：Current文件，Manifest文件，log文件以及SSTable文件。当然，LevelDB除了这六个主要部分还有一些辅助的文件，但是以上六个文件和数据结构是LevelDB的主体构成元素。<br>LevelDB的Log文件和Memtable与Bigtable论文中介绍的是一致的，当应用写入一条Key:Value记录的时候，LevelDB会先往log文件里写入，成功后将记录插进Memtable中，这样基本就算完成了写入操作，因为一次写入操作只涉及一次磁盘顺序写和一次内存写入，所以这是为何说LevelDB写入速度极快的主要原因。<br>Log文件在系统中的作用主要是用于系统崩溃恢复而不丢失数据，假如没有Log文件，因为写入的记录刚开始是保存在内存中的，此时如果系统崩溃，内存中的数据还没有来得及Dump到磁盘，所以会丢失数据（Redis就存在这个问题）。为了避免这种情况，LevelDB在写入内存前先将操作记录到Log文件中，然后再记入内存中，这样即使系统崩溃，也可以从Log文件中恢复内存中的Memtable，不会造成数据的丢失。<br>当Memtable插入的数据占用内存到了一个界限后，需要将内存的记录导出到外存文件中，LevleDB会生成新的Log文件和Memtable，原先的Memtable就成为Immutable Memtable，顾名思义，就是说这个Memtable的内容是不可更改的，只能读不能写入或者删除。新到来的数据被记入新的Log文件和Memtable，LevelDB后台调度会将Immutable Memtable的数据导出到磁盘，形成一个新的SSTable文件。SSTable就是由内存中的数据不断导出并进行Compaction操作后形成的，而且SSTable的所有文件是一种层级结构，第一层为Level 0，第二层为Level 1，依次类推，层级逐渐增高，这也是为何称之为LevelDB的原因。<br>SSTable中的文件是Key有序的，就是说在文件中小key记录排在大Key记录之前，各个Level的SSTable都是如此，但是这里需要注意的一点是：Level 0的SSTable文件（后缀为.sst）和其它Level的文件相比有特殊性：这个层级内的.sst文件，两个文件可能存在key重叠，比如有两个level 0的sst文件，文件A和文件B，文件A的key范围是：{bar, car}，文件B的Key范围是{blue,samecity}，那么很可能两个文件都存在key=”blood”的记录。对于其它Level的SSTable文件来说，则不会出现同一层级内.sst文件的key重叠现象，就是说Level L中任意两个.sst文件，那么可以保证它们的key值是不会重叠的。这点需要特别注意，后面您会看到很多操作的差异都是由于这个原因造成的。<br>SSTable中的某个文件属于特定层级，而且其存储的记录是key有序的，那么必然有文件中的最小key和最大key，这是非常重要的信息，LevelDB应该记下这些信息。Manifest就是干这个的，它记载了SSTable各个文件的管理信息，比如属于哪个Level，文件名称叫啥，最小key和最大key各自是多少。下图是Manifest所存储内容的示意：<br><img src="http://i.imgur.com/5JI6jhp.png" alt=""><br>图2.1：Manifest存储示意图</p><p>图中只显示了两个文件（manifest会记载所有SSTable文件的这些信息），即Level 0的test.sst1和test.sst2文件，同时记载了这些文件各自对应的key范围，比如test.sstt1的key范围是“an”到 “banana”，而文件test.sst2的key范围是“baby”到“samecity”，可以看出两者的key范围是有重叠的。<br>Current文件是干什么的呢？这个文件的内容只有一个信息，就是记载当前的manifest文件名。因为在LevleDB的运行过程中，随着Compaction的进行，SSTable文件会发生变化，会有新的文件产生，老的文件被废弃，Manifest也会跟着反映这种变化，此时往往会新生成Manifest文件来记载这种变化，而Current则用来指出哪个Manifest文件才是我们关心的那个Manifest文件。<br>以上介绍的内容就构成了LevelDB的整体静态结构，在LevelDB剖析接下来的内容中，我们会首先介绍重要文件或者内存数据的具体数据布局与结构。</p><h3 id="LevelDB剖析之三：log文件"><a href="#LevelDB剖析之三：log文件" class="headerlink" title="LevelDB剖析之三：log文件"></a>LevelDB剖析之三：log文件</h3><p>上节内容讲到log文件在LevelDB中的主要作用是系统故障恢复时，能够保证不会丢失数据。因为在将记录写入内存的Memtable之前，会先写入Log文件，这样即使系统发生故障，Memtable中的数据没有来得及Dump到磁盘的SSTable文件，LevelDB也可以根据log文件恢复内存的Memtable数据结构内容，不会造成系统丢失数据，在这点上LevelDB和Bigtable是一致的。<br>下面我们带大家看看log文件的具体物理和逻辑布局是怎样的，LevelDB对于一个log文件，会把它切割成以32K为单位的物理Block，每次读取的单位以一个Block作为基本读取单位，下图展示的log文件由3个Block构成，所以从物理布局来讲，一个log文件就是由连续的32K大小Block构成的。<br><img src="http://i.imgur.com/lYMLMrA.png" alt=""><br>图3.1 log文件布局  </p><p>在应用的视野里是看不到这些Block的，应用看到的是一系列的Key:Value对，在LevelDB内部，会将一个Key:Value对看做一条记录的数据，另外在这个数据前增加一个记录头，用来记载一些管理信息，以方便内部处理。<br>记录头包含三个字段，ChechSum是对“类型”和“数据”字段的校验码，为了避免处理不完整或者是被破坏的数据，当LevelDB读取记录数据时候会对数据进行校验，如果发现和存储的CheckSum相同，说明数据完整无破坏，可以继续后续流程。“记录长度”记载了数据的大小，“数据”则是上面讲的Key:Value数值对，“类型”字段则指出了每条记录的逻辑结构和log文件物理分块结构之间的关系，具体而言，主要有以下四种类型：FULL/FIRST/MIDDLE/LAST。<br>如果记录类型是FULL，代表了当前记录内容完整地存储在一个物理Block里，没有被不同的物理Block切割开；如果记录被相邻的物理Block切割开，则类型会是其他三种类型中的一种。我们以图3.1所示的例子来具体说明。<br>假设目前存在三条记录，Record A，Record B和Record C，其中Record A大小为10K，Record B 大小为80K，Record C大小为12K，那么其在log文件中的逻辑布局会如图3.1所示。Record A是图中蓝色区域所示，因为大小为10K&lt;32K，能够放在一个物理Block中，所以其类型为FULL；Record B 大小为80K，而Block 1因为放入了Record A，所以还剩下22K，不足以放下Record B，所以在Block 1的剩余部分放入Record B的开头一部分，类型标识为FIRST，代表了是一个记录的起始部分；Record B还有58K没有存储，这些只能依次放在后续的物理Block里面，因为Block 2大小只有32K，仍然放不下Record B的剩余部分，所以Block 2全部用来放Record B，且标识类型为MIDDLE，意思是这是Record B中间一段数据；Record B剩下的部分可以完全放在Block 3中，类型标识为LAST，代表了这是Record B的末尾数据；图中黄色的Record C因为大小为12K，Block 3剩下的空间足以全部放下它，所以其类型标识为FULL。<br>从这个小例子可以看出逻辑记录和物理Block之间的关系，LevelDB一次物理读取为一个Block，然后根据类型情况拼接出逻辑记录，供后续流程处理。</p><h3 id="LevelDB剖析之四：SSTable文件"><a href="#LevelDB剖析之四：SSTable文件" class="headerlink" title="LevelDB剖析之四：SSTable文件"></a>LevelDB剖析之四：SSTable文件</h3><p>SSTable是Bigtable中至关重要的一块，对于LevelDB来说也是如此，对LevelDB的SSTable实现细节的了解也有助于了解Bigtable中一些实现细节。<br>本节内容主要讲述SSTable的静态布局结构，我们曾在“LevelDB剖析之二：整体架构”中说过，SSTable文件形成了不同Level的层级结构，至于这个层级结构是如何形成的我们放在后面Compaction一节细说。本节主要介绍SSTable某个文件的物理布局和逻辑布局结构，这对了解LevelDB的运行过程很有帮助。<br>LevelDB同层级有很多SSTable文件（以后缀.sst为特征），所有.sst文件内部布局都是一样的。上节介绍Log文件是物理分块的，SSTable也一样会将文件划分为固定大小的物理存储块，但是两者逻辑布局大不相同，根本原因是：Log文件中的记录是Key无序的，即先后记录的key大小没有明确大小关系，而.sst文件内部则是根据记录的Key由小到大排列的，从下面介绍的SSTable布局可以体会到Key有序是为何如此设计.sst文件结构的关键。<br><img src="http://i.imgur.com/zM7RpqK.png" alt=""><br>图4.1 .sst文件的分块结构  </p><p>图4.1展示了一个.sst文件的物理划分结构，同Log文件一样，也是划分为固定大小的存储块，每个Block分为三个部分，红色部分是数据存储区， 蓝色的Type区用于标识数据存储区是否采用了数据压缩算法（Snappy压缩或者无压缩两种），CRC部分则是数据校验码，用于判别数据是否在生成和传输中出错。<br>以上是.sst的物理布局，下面介绍.sst文件的逻辑布局，所谓逻辑布局，就是说尽管大家都是物理块，但是每一块存储什么内容，内部又有什么结构等。图4.2展示了.sst文件的内部逻辑解释。<br><img src="http://i.imgur.com/IOpRF3E.png" alt=""><br>图4.2 逻辑布局   </p><p>从图4.2可以看出，从大的方面，可以将.sst文件划分为数据存储区和数据管理区，数据存储区存放实际的Key:Value数据，数据管理区则提供一些索引指针等管理数据，目的是更快速便捷的查找相应的记录。两个区域都是在上述的分块基础上的，就是说文件的前面若干块实际存储KV数据，后面数据管理区存储管理数据。管理数据又分为四种不同类型：紫色的Meta Block，红色的MetaBlock 索引和蓝色的数据索引块以及一个文件尾部块。<br>LevelDB 1.2版对于Meta Block尚无实际使用，只是保留了一个接口，估计会在后续版本中加入内容，下面我们看看数据索引区和文件尾部Footer的内部结构。<br><img src="http://i.imgur.com/I141rvW.png" alt=""><br>图4.3 数据索引   </p><p>图4.3是数据索引的内部结构示意图。再次强调一下，Data Block内的KV记录是按照Key由小到大排列的，数据索引区的每条记录是对某个Data Block建立的索引信息，每条索引信息包含三个内容，以图4.3所示的数据块i的索引Index i来说：红色部分的第一个字段记载大于等于数据块i中最大的Key值的那个Key，第二个字段指出数据块i在.sst文件中的起始位置，第三个字段指出Data Block i的大小（有时候是有数据压缩的）。后面两个字段好理解，是用于定位数据块在文件中的位置的，第一个字段需要详细解释一下，在索引里保存的这个Key值未必一定是某条记录的Key,以图4.3的例子来说，假设数据块i 的最小Key=“samecity”，最大Key=“the best”;数据块i+1的最小Key=“the fox”,最大Key=“zoo”,那么对于数据块i的索引Index i来说，其第一个字段记载大于等于数据块i的最大Key(“the best”)同时要小于数据块i+1的最小Key(“the fox”)，所以例子中Index i的第一个字段是：“the c”，这个是满足要求的；而Index i+1的第一个字段则是“zoo”，即数据块i+1的最大Key。<br>metaindex_handle指出了metaindex block的起始位置和大小；index、_handle指出了index Block的起始地址和大小；这两个字段可以理解为索引的索引，是为了正确读出索引值而设立的，后面跟着一个填充区和魔数。<br>上面主要介绍的是数据管理区的内部结构，下面我们看看数据区的一个Block的数据部分内部是如何布局的（图4.1中的红色部分），图4.4是其内部布局示意图。<br><img src="http://i.imgur.com/UB90CaD.png" alt=""><br>图4.4 数据Block内部结构    </p><p>从图中可以看出，其内部也分为两个部分，前面是一个个KV记录，其顺序是根据Key值由小到大排列的，在Block尾部则是一些“重启点”（Restart Point）,其实是一些指针，指出Block内容中的一些记录位置。<br>“重启点”是干什么的呢？我们一再强调，Block内容里的KV记录是按照Key大小有序的，这样的话，相邻的两条记录很可能Key部分存在重叠，比如key i=“the Car”，Key i+1=“the color”,那么两者存在重叠部分“the c”，为了减少Key的存储量，Key i+1可以只存储和上一条Key不同的部分“olor”，两者的共同部分从Key i中可以获得。记录的Key在Block内容部分就是这么存储的，主要目的是减少存储开销。“重启点”的意思是：在这条记录开始，不再采取只记载不同的Key部分，而是重新记录所有的Key值，假设Key i+1是一个重启点，那么Key里面会完整存储“the color”，而不是采用简略的“olor”方式。Block尾部就是指出哪些记录是这些重启点的。<br><img src="http://i.imgur.com/fApkG8i.png" alt=""><br>图4.5 记录格式   </p><p>在Block内容区，每个KV记录的内部结构是怎样的？图4.5给出了其详细结构，每个记录包含5个字段：key共享长度，比如上面的“olor”记录， 其key和上一条记录共享的Key部分长度是“the c”的长度，即5；key非共享长度，对于“olor”来说，是4；value长度指出Key:Value中Value的长度，在后面的Value内容字段中存储实际的Value值；而key非共享内容则实际存储“olor”这个Key字符串。<br>上面讲的这些就是.sst文件的全部内部奥秘。</p><h3 id="LevelDB剖析之五：MemTable详解"><a href="#LevelDB剖析之五：MemTable详解" class="headerlink" title="LevelDB剖析之五：MemTable详解"></a>LevelDB剖析之五：MemTable详解</h3><p>LevelDB剖析前述小节大致讲述了磁盘文件相关的重要静态结构，本小节讲述内存中的数据结构Memtable，Memtable在整个体系中的重要地位也不言而喻。总体而言，所有KV数据都是存储在Memtable，Immutable Memtable和SSTable中的，Immutable Memtable从结构上讲和Memtable是完全一样的，区别仅仅在于其是只读的，不允许写入操作，而Memtable则是允许写入和读取的。当Memtable写入的数据占用内存到达指定数量，则自动转换为Immutable Memtable，等待Dump到磁盘中，系统会自动生成新的Memtable供写操作写入新数据，理解了Memtable，那么Immutable Memtable自然不在话下。<br>LevelDB的MemTable提供了将KV数据写入，删除以及读取KV记录的操作接口，但是事实上Memtable并不存在真正的删除操作,删除某个Key的Value在Memtable内是作为插入一条记录实施的，但是会打上一个Key的删除标记，真正的删除操作是Lazy的，会在以后的Compaction过程中去掉这个KV。<br>需要注意的是，LevelDB的Memtable中KV对是根据Key大小有序存储的，在系统插入新的KV时，LevelDB要把这个KV插到合适的位置上以保持这种Key有序性。其实，LevelDB的Memtable类只是一个接口类，真正的操作是通过背后的SkipList来做的，包括插入操作和读取操作等，所以Memtable的核心数据结构是一个SkipList。<br>SkipList是平衡树的一种替代数据结构，但是和红黑树不相同的是，SkipList对于树的平衡的实现是基于一种随机化的算法的，这样也就是说SkipList的插入和删除的工作是比较简单的。LevelDB的SkipList基本上是一个具体实现，并无特殊之处。<br>SkipList不仅是维护有序数据的一个简单实现，而且相比较平衡树来说，在插入数据的时候可以避免频繁的树节点调整操作，所以写入效率是很高的，LevelDB整体而言是个高写入系统，SkipList在其中应该也起到了很重要的作用。Redis为了加快插入操作，也使用了SkipList来作为内部实现数据结构。 </p><h3 id="LevelDB剖析之六：写入与删除记录"><a href="#LevelDB剖析之六：写入与删除记录" class="headerlink" title="LevelDB剖析之六：写入与删除记录"></a>LevelDB剖析之六：写入与删除记录</h3><p>在之前的五节LevelDB剖析中，我们介绍了LevelDB的一些静态文件及其详细布局，从本节开始，我们看看LevelDB的一些动态操作，比如读写记录，Compaction，错误恢复等操作。<br>本节介绍levelDB的记录更新操作，即插入一条KV记录或者删除一条KV记录。levelDB的更新操作速度是非常快的，源于其内部机制决定了这种更新操作的简单性。<br><img src="http://i.imgur.com/buYgI4P.png" alt=""><br>图6.1 LevelDB写入记录 </p><p>图6.1是levelDB如何更新KV数据的示意图，从图中可以看出，对于一个插入操作Put(Key,Value)来说，完成插入操作包含两个具体步骤：首先是将这条KV记录以顺序写的方式追加到之前介绍过的log文件末尾，因为尽管这是一个磁盘读写操作，但是文件的顺序追加写入效率是很高的，所以并不会导致写入速度的降低；第二个步骤是:如果写入log文件成功，那么将这条KV记录插入内存中的Memtable中，前面介绍过，Memtable只是一层封装，其内部其实是一个Key有序的SkipList列表，插入一条新记录的过程也很简单，即先查找合适的插入位置，然后修改相应的链接指针将新记录插入即可。完成这一步，写入记录就算完成了，所以一个插入记录操作涉及一次磁盘文件追加写和内存SkipList插入操作，这是为何levelDB写入速度如此高效的根本原因。<br>从上面的介绍过程中也可以看出：log文件内是key无序的，而Memtable中是key有序的。那么如果是删除一条KV记录呢？对于levelDB来说，并不存在立即删除的操作，而是与插入操作相同的，区别是，插入操作插入的是Key:Value 值，而删除操作插入的是“Key:删除标记”，并不真正去删除记录，而是后台Compaction的时候才去做真正的删除操作。<br>levelDB的写入操作就是如此简单。真正的麻烦在后面将要介绍的读取操作中。 </p><h3 id="LevelDB剖析之七：读取记录"><a href="#LevelDB剖析之七：读取记录" class="headerlink" title="LevelDB剖析之七：读取记录"></a>LevelDB剖析之七：读取记录</h3><p>LevelDB是针对大规模Key/Value数据的单机存储库，从应用的角度来看，LevelDB就是一个存储工具。而作为称职的存储工具，常见的调用接口无非是新增KV，删除KV，读取KV，更新Key对应的Value值这么几种操作。LevelDB的接口没有直接支持更新操作的接口，如果需要更新某个Key的Value,你可以选择直接生猛地插入新的KV，保持Key相同，这样系统内的key对应的value就会被更新；或者你可以先删除旧的KV， 之后再插入新的KV，这样比较委婉地完成KV的更新操作。<br>假设应用提交一个Key值，下面我们看看LevelDB是如何从存储的数据中读出其对应的Value值的。图7-1是LevelDB读取过程的整体示意图。<br><img src="http://i.imgur.com/FF4f9KB.png" alt=""><br>图7-1  LevelDB读取记录流程  </p><p>LevelDB首先会去查看内存中的Memtable，如果Memtable中包含key及其对应的value，则返回value值即可；如果在Memtable没有读到key，则接下来到同样处于内存中的Immutable Memtable中去读取，类似地，如果读到就返回，若是没有读到,那么只能万般无奈下从磁盘中的大量SSTable文件中查找。因为SSTable数量较多，而且分成多个Level，所以在SSTable中读数据是相当蜿蜒曲折的一段旅程。总的读取原则是这样的：首先从属于level 0的文件中查找，如果找到则返回对应的value值，如果没有找到那么到level 1中的文件中去找，如此循环往复，直到在某层SSTable文件中找到这个key对应的value为止（或者查到最高level，查找失败，说明整个系统中不存在这个Key)。<br>那么为什么是从Memtable到Immutable Memtable，再从Immutable Memtable到文件，而文件中为何是从低level到高level这么一个查询路径呢？道理何在？之所以选择这么个查询路径，是因为从信息的更新时间来说，很明显Memtable存储的是最新鲜的KV对；Immutable Memtable中存储的KV数据对的新鲜程度次之；而所有SSTable文件中的KV数据新鲜程度一定不如内存中的Memtable和Immutable Memtable的。对于SSTable文件来说，如果同时在level L和Level L+1找到同一个key，level L的信息一定比level L+1的要新。也就是说，上面列出的查找路径就是按照数据新鲜程度排列出来的，越新鲜的越先查找。<br>为啥要优先查找新鲜的数据呢？这个道理不言而喻，举个例子。比如我们先往levelDB里面插入一条数据 {key=”<a href="http://www.samecity.com&quot;" target="_blank" rel="noopener">www.samecity.com&quot;</a>  value=”我们”},过了几天，samecity网站改名为：69同城，此时我们插入数据{key=”<a href="http://www.samecity.com&quot;" target="_blank" rel="noopener">www.samecity.com&quot;</a>  value=”69同城”}，同样的key,不同的value；逻辑上理解好像levelDB中只有一个存储记录，即第二个记录，但是在levelDB中很可能存在两条记录，即上面的两个记录都在levelDB中存储了，此时如果用户查询key=”<a href="http://www.samecity.com&quot;,我们当然希望找到最新的更新记录，也就是第二个记录返回，这就是为何要优先查找新鲜数据的原因。" target="_blank" rel="noopener">www.samecity.com&quot;,我们当然希望找到最新的更新记录，也就是第二个记录返回，这就是为何要优先查找新鲜数据的原因。</a><br>前文有讲：对于SSTable文件来说，如果同时在level L和Level L+1找到同一个key，level L的信息一定比level L+1的要新。这是一个结论，理论上需要一个证明过程，否则会招致如下的问题：为神马呢？从道理上讲呢，很明白：因为Level L+1的数据不是从石头缝里蹦出来的，也不是做梦梦到的，那它是从哪里来的？Level L+1的数据是从Level L 经过Compaction后得到的（如果您不知道什么是Compaction，那么……..也许以后会知道的），也就是说，您看到的现在的Level L+1层的SSTable数据是从原来的Level L中来的，现在的Level L比原来的Level L数据要新鲜，所以可证，现在的Level L比现在的Level L+1的数据要新鲜。<br>SSTable文件很多，如何快速地找到key对应的value值？在LevelDB中，level 0一直都爱搞特殊化，在level 0和其它level中查找某个key的过程是不一样的。因为level 0下的不同文件可能key的范围有重叠，某个要查询的key有可能多个文件都包含，这样的话LevelDB的策略是先找出level 0中哪些文件包含这个key（manifest文件中记载了level和对应的文件及文件里key的范围信息，LevelDB在内存中保留这种映射表）， 之后按照文件的新鲜程度排序，新的文件排在前面，之后依次查找，读出key对应的value。而如果是非level 0的话，因为这个level的文件之间key是不重叠的，所以只从一个文件就可以找到key对应的value。<br>最后一个问题,如果给定一个要查询的key和某个key range包含这个key的SSTable文件，那么levelDB是如何进行具体查找过程的呢？levelDB一般会先在内存中的Cache中查找是否包含这个文件的缓存记录，如果包含，则从缓存中读取；如果不包含，则打开SSTable文件，同时将这个文件的索引部分加载到内存中并放入Cache中。 这样Cache里面就有了这个SSTable的缓存项，但是只有索引部分在内存中，之后levelDB根据索引可以定位到哪个内容Block会包含这条key，从文件中读出这个Block的内容，在根据记录一一比较，如果找到则返回结果，如果没有找到，那么说明这个level的SSTable文件并不包含这个key，所以到下一级别的SSTable中去查找。<br>从之前介绍的LevelDB的写操作和这里介绍的读操作可以看出，相对写操作，读操作处理起来要复杂很多，所以写的速度必然要远远高于读数据的速度，也就是说，LevelDB比较适合写操作多于读操作的应用场合。而如果应用是很多读操作类型的，那么顺序读取效率会比较高，因为这样大部分内容都会在缓存中找到，尽可能避免大量的随机读取操作。     </p><h3 id="LevelDB剖析之八：Compaction操作"><a href="#LevelDB剖析之八：Compaction操作" class="headerlink" title="LevelDB剖析之八：Compaction操作"></a>LevelDB剖析之八：Compaction操作</h3><p>前文有述，对于LevelDB来说，写入记录操作很简单，删除记录仅仅写入一个删除标记就算完事，但是读取记录比较复杂，需要在内存以及各个层级文件中依照新鲜程度依次查找，代价很高。为了加快读取速度，levelDB采取了compaction的方式来对已有的记录进行整理压缩，通过这种方式，来删除掉一些不再有效的KV数据，减小数据规模，减少文件数量等。<br>levelDB的compaction机制和过程与Bigtable所讲述的是基本一致的，Bigtable中讲到三种类型的compaction: minor ，major和full。所谓minor Compaction，就是把memtable中的数据导出到SSTable文件中；major compaction就是合并不同层级的SSTable文件，而full compaction就是将所有SSTable进行合并。<br>LevelDB包含其中两种，minor和major。先来看看minor Compaction的过程。minor compaction 的目的是当内存中的memtable大小到了一定值时，将内容保存到磁盘文件中，图8.1是其机理示意图。<br><img src="http://i.imgur.com/9nVKQV2.png" alt=""><br>图8.1 minor compaction    </p><p>从8.1可以看出，当memtable数量到了一定程度会转换为immutable memtable，此时不能往其中写入记录，只能从中读取KV内容。之前介绍过，immutable memtable其实是一个多层级队列SkipList，其中的记录是根据key有序排列的。所以这个minor compaction实现起来也很简单，就是按照immutable memtable中记录由小到大遍历，并依次写入一个level 0 的新建SSTable文件中，写完后建立文件的index 数据，这样就完成了一次minor compaction。从图中也可以看出，对于被删除的记录，在minor compaction过程中并不真正删除这个记录，原因也很简单，这里只知道要删掉key记录，但是这个KV数据在哪里?那需要复杂的查找，所以在minor compaction的时候并不做删除，只是将这个key作为一个记录写入文件中，至于真正的删除操作，在以后更高层级的compaction中会去做。<br>当某个level下的SSTable文件数目超过一定设置值后，levelDB会从这个level的SSTable中选择一个文件（level&gt;0），将其和高一层级的level+1的SSTable文件合并，这就是major compaction。<br>我们知道在大于0的层级中，每个SSTable文件内的Key都是由小到大有序存储的，而且不同文件之间的key范围（文件内最小key和最大key之间）不会有任何重叠。Level 0的SSTable文件有些特殊，尽管每个文件也是根据Key由小到大排列，但是因为level 0的文件是通过minor compaction直接生成的，所以任意两个level 0下的两个sstable文件可能再key范围上有重叠。所以在做major compaction的时候，对于大于level 0的层级，选择其中一个文件就行，但是对于level 0来说，指定某个文件后，本level中很可能有其他SSTable文件的key范围和这个文件有重叠，这种情况下，要找出所有有重叠的文件和level 1的文件进行合并，即level 0在进行文件选择的时候，可能会有多个文件参与major compaction。<br>LevelDB在选定某个level进行compaction后，还要选择是具体哪个文件要进行compaction，LevelDB在这里有个小技巧， 就是说轮流来，比如这次是文件A进行compaction，那么下次就是在key range上紧挨着文件A的文件B进行compaction，这样每个文件都会有机会轮流和高层的level 文件进行合并。<br>如果选好了level L的文件A和level L+1层的文件进行合并，那么问题又来了，应该选择level L+1哪些文件进行合并？LevelDB选择L+1层中和文件A在key range上有重叠的所有文件来和文件A进行合并。<br>也就是说，选定了level L的文件A,之后在level L+1中找到了所有需要合并的文件B,C,D…等等。剩下的问题就是具体是如何进行major合并的？就是说给定了一系列文件，每个文件内部是key有序的，如何对这些文件进行合并，使得新生成的文件仍然Key有序，同时抛掉哪些不再有价值的KV数据。<br>图8.2说明了这一过程。<br><img src="http://i.imgur.com/abldSaG.png" alt=""><br>图8.2 SSTable Compaction </p><p>Major compaction的过程如下：对多个文件采用多路归并排序的方式，依次找出其中最小的Key记录，也就是对多个文件中的所有记录重新进行排序。之后采取一定的标准判断这个Key是否还需要保存，如果判断没有保存价值，那么直接抛掉，如果觉得还需要继续保存，那么就将其写入level L+1层中新生成的一个SSTable文件中。就这样对KV数据一一处理，形成了一系列新的L+1层数据文件，之前的L层文件和L+1层参与compaction 的文件数据此时已经没有意义了，所以全部删除。这样就完成了L层和L+1层文件记录的合并过程。<br>那么在major compaction过程中，判断一个KV记录是否抛弃的标准是什么呢？其中一个标准是:对于某个key来说，如果在小于L层中存在这个Key，那么这个KV在major compaction过程中可以抛掉。因为我们前面分析过，对于层级低于L的文件中如果存在同一Key的记录，那么说明对于Key来说，有更新鲜的Value存在，那么过去的Value就等于没有意义了，所以可以删除。<br>##LevelDB剖析之九：LevelDB中的Cache<br>书接前文，前面讲过对于LevelDB来说，读取操作如果没有在内存的memtable中找到记录，要多次进行磁盘访问操作。假设最优情况，即第一次就在level 0中最新的文件中找到了这个key，那么也需要读取2次磁盘，一次是将SSTable的文件中的index部分读入内存，这样根据这个index可以确定key是在哪个block中存储；第二次是读入这个block的内容，然后在内存中查找key对应的value。<br>LevelDB中引入了两个不同的Cache:Table Cache和Block Cache。其中Block Cache是配置可选的，即在配置文件中指定是否打开这个功能。<br><img src="http://i.imgur.com/8ZUqBxO.png" alt=""><br>图9.1 table cache </p><p>图9.1是table cache的结构。在Cache中，key值是SSTable的文件名称，Value部分包含两部分，一个是指向磁盘打开的SSTable文件的文件指针，这是为了方便读取内容；另外一个是指向内存中这个SSTable文件对应的Table结构指针，table结构在内存中，保存了SSTable的index内容以及用来指示block cache用的cache_id ,当然除此外还有其它一些内容。<br>比如在get(key)读取操作中，如果LevelDB确定了key在某个level下某个文件A的key range范围内，那么需要判断是不是文件A真的包含这个KV。此时，LevelDB会首先查找Table Cache，看这个文件是否在缓存里，如果找到了，那么根据index部分就可以查找是哪个block包含这个key。如果没有在缓存中找到文件，那么打开SSTable文件，将其index部分读入内存，然后插入Cache里面，去index里面定位哪个block包含这个Key 。如果确定了文件哪个block包含这个key，那么需要读入block内容，这是第二次读取。<br>Block Cache是为了加快这个过程的。其中的key是文件的cache_id加上这个block在文件中的起始位置block_offset。而value则是这个Block的内容。<br>如果LevelDB发现这个block在block cache中，那么可以避免读取数据，直接在cache里的block内容里面查找key的value就行，如果没找到呢？那么读入block内容并把它插入block cache中。LevelDB就是这样通过两个cache来加快读取速度的。从这里可以看出，如果读取的数据局部性比较好，也就是说要读的数据大部分在cache里面都能读到，那么读取效率应该还是很高的，而如果是对key进行顺序读取效率也应该不错，因为一次读入后可以多次被复用。但是如果是随机读取，您可以推断下其效率如何。     </p><h3 id="LevelDB剖析之十：Version、VersionEdit、VersionSet"><a href="#LevelDB剖析之十：Version、VersionEdit、VersionSet" class="headerlink" title="LevelDB剖析之十：Version、VersionEdit、VersionSet"></a>LevelDB剖析之十：Version、VersionEdit、VersionSet</h3><p>Version保存了当前磁盘以及内存中所有的文件信息，一般只有一个Version叫做”current” version（当前版本）。LevelDB还保存了一系列的历史版本，这些历史版本有什么作用呢？<br>当一个Iterator创建后，Iterator就引用到了current version(当前版本)，只要这个Iterator不被delete那么被Iterator引用的版本就会一直存活。这就意味着当你用完一个Iterator后，需要及时删除它。<br>当一次Compaction结束后（会生成新的文件，合并前的文件需要删除），LevelDB会创建一个新的版本作为当前版本，原先的当前版本就会变为历史版本。<br>VersionSet是所有Version的集合，管理着所有存活的Version。<br>VersionEdit 表示Version之间的变化，相当于delta 增量，表示有增加了多少文件，删除了文件。他们之间的关系为：Version0 +VersionEdit–&gt;Version1。VersionEdit会保存到MANIFEST文件中，当做数据恢复时就会从MANIFEST文件中读出来重建数据。<br>LevelDB的这种版本的控制，让我想到了双buffer切换，双buffer切换来自于图形学中，用于解决屏幕绘制时的闪屏问题，在服务器编程中也有用处。比如我们的服务器上有一个字典库，每天我们需要更新这个字典库，我们可以新开一个buffer，将新的字典库加载到这个新buffer中，等到加载完毕，将字典的指针指向新的字典库。<br>LevelDB的version管理和双buffer切换类似，但是如果原version被某个iterator引用，那么这个version会一直保持，直到没有被任何一个iterator引用，此时就可以删除这个version。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;LevelDB是能够处理十亿级别规模Key-Value型数据持久性存储的C++程序库，是谷歌两位大牛Jeff Dean和Sanjay Ghemawat发起的开源项目。自己已经将拜读LevelDB纳入了项目完成后的学习计划。最近偶然看到一篇关于LevelDB很好的博客，特记录下来，希望对后面的学习有帮助。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="LevelDB" scheme="http://blog.jonnydu.me/tags/LevelDB/"/>
    
  </entry>
  
  <entry>
    <title>CentOS配置双网卡（Mellanox和传统以太网卡）</title>
    <link href="http://blog.jonnydu.me/2017/03/20/config-two-network-interfaces/"/>
    <id>http://blog.jonnydu.me/2017/03/20/config-two-network-interfaces/</id>
    <published>2017-03-20T09:57:25.000Z</published>
    <updated>2020-04-30T14:16:30.261Z</updated>
    
    <content type="html"><![CDATA[<p>刚给服务器装上了操作系统，期待已久的Mellanox网卡终于到了，关于该网卡这里就不做过多的介绍了，可以参考<a href="http://www.mellanox.com/" target="_blank" rel="noopener">Mellanox官网</a>，下面记录在服务器上Mellanox网卡和传统以太网的配置。</p><a id="more"></a><h3 id="安插Mellanox网卡并安装相应驱动"><a href="#安插Mellanox网卡并安装相应驱动" class="headerlink" title="安插Mellanox网卡并安装相应驱动"></a>安插Mellanox网卡并安装相应驱动</h3><p>首先，在服务器背板安插Mellanox网卡，然后执行<code>ifconfig</code>查看是否多了一张新的网卡。初始状态下<code>ifconfig</code>看到的Mellanox网卡的名字是ib0，这说明该网卡的默认链路层协议时InfiniBand协议，而不是Ethernet协议。由于我们后续需要使用NVMe over RoCEv2，所以需要将Mellanox的链路层协议改为Ethernet。因此，需要先下载并安装相应的驱动。<br>在官网上确定好操作系统对应的驱动版本号（比如CentOS7.2对应的版本是MLNX_OFED-3.4-2.0.0.0-rhel7.2-x86_64）后，下载并安装驱动。</p><pre><code>wget http://www.mellanox.com/downloads/ofed/MLNX_OFED-3.4-2.0.0.0/MLNX_OFED_LINUX-3.4-2.0.0.0-rhel7.2-x86_64.tgztar -xvf MLNX_OFED_LINUX-3.4-2.0.0.0-rhel7.2-x86_64.tgzcd MLNX_OFED_LINUX-3.4-2.0.0.0-rhel7.2-x86_64/./mlnxofedinstall  (--add-kernel-support)       ###kernel 4.8.15可能需要添加特定的选项/etc/init.d/openibd restart</code></pre><h3 id="修改Mellanox网卡配置"><a href="#修改Mellanox网卡配置" class="headerlink" title="修改Mellanox网卡配置"></a>修改Mellanox网卡配置</h3><p>如前文所述，Mellanox网卡的默认链路层协议是Infiniband协议，不是Ethernet协议。所以，接下来修改网卡的链路层协议。<br>在修改之前，可以先通过<code>lspci | grep Mellanox</code>查看网卡信息，确定是否为InfiniBand协议。</p><pre><code>mst startibv_devinfo | grep vendor_part_id    ##得到vendor_part_id(我这里是4115)mlxconfig -d /dev/mst/mt4115_pciconf0 q  ##再次查询网卡信息mlxconfig -d /dev/mst/mt4115_pciconf0 set LINK_TYPE_P1=2  ##修改为Ethernet协议</code></pre><p>修改并重新启动后，可以看到网卡的名称发生了变化（这里是enp133s0）。最后，使用<code>lspci | grep Mellanox</code>确认修改。</p><h3 id="CentOS双网卡配置"><a href="#CentOS双网卡配置" class="headerlink" title="CentOS双网卡配置"></a>CentOS双网卡配置</h3><p>现在服务器的网卡连接情况是一张Mellanox网卡通过专用网线连接到EdgeCore 100G白牌交换机上，另外一张传统以太网卡连接到以太网交换机（192.168.1.1）。接下来双网卡配置双IP，一个192.168.1.x，可通过以太网交换机连接外网，另一个192.168.5.x，连接到100G白牌交换机。   </p><p>以太网卡在服务器上对应的名称为eno1。修改其配置文件。</p><pre><code>vim /etc/sysconfig/network-scripts/ifcfg-eth0</code></pre><p>修改和添加以下内容：</p><pre><code>BOOTPROTO=staticHWADDR=6c:92:bf:42:27:2e    #mac addressIPADDR=192.168.1.152NETMASK=255.255.255.0GATEWAY=192.168.1.1ONBOOT=yes</code></pre><p>修改后的配置文件如下图所示。<br><img src="http://i.imgur.com/WqcbomA.png" alt=""><br>通过<code>service network restart</code>重启网络后，就成功的将该以太网卡设置IP设置成了静态IP（192.168.1.152）。</p><p>Mellanox网卡的配置和以太网卡一样。所以，这里只列举出修改后的配置文件（/etc/sysconfig/network-scripts/ifcfg-enp133s0）<br><img src="http://i.imgur.com/jcLC6h9.png" alt=""></p><p>配置好后的服务器网络接口情况如下图所示。<br><img src="http://i.imgur.com/rNo8Xe3.png" alt=""></p><h3 id="配置路由表"><a href="#配置路由表" class="headerlink" title="配置路由表"></a>配置路由表</h3><p>目前系统默认网关是192.168.1.1，所以需要增加两个路由表，实现双网关正常访问。<br>    vim /etc/iproute2/rt_tables<br>增加两行内容：<br>    252 net2<br>    251 net3<br>在/etc/rc.local添加静态路由规则。</p><pre><code>ip route flush table net2ip route add default via 192.168.1.1 dev eno1 src 192.168.1.152 table net2ip rule add from 192.168.1.152 table net2ip route flush table net3ip route add default via 192.168.5.1 dev enp133s0 src 192.168.5.86 table net3ip rule add from 192.168.5.86 table net3</code></pre><p>这时，双网卡双IP应该就配置好了，<code>service network restart</code>重启网络就可以实现不同网卡对应不同网络访问。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;刚给服务器装上了操作系统，期待已久的Mellanox网卡终于到了，关于该网卡这里就不做过多的介绍了，可以参考&lt;a href=&quot;http://www.mellanox.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Mellanox官网&lt;/a&gt;，下面记录在服务器上Mellanox网卡和传统以太网的配置。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="network/linux" scheme="http://blog.jonnydu.me/tags/network-linux/"/>
    
      <category term="NVMe over RDMA" scheme="http://blog.jonnydu.me/tags/NVMe-over-RDMA/"/>
    
  </entry>
  
  <entry>
    <title>RDMA技术浅析</title>
    <link href="http://blog.jonnydu.me/2017/02/27/RDMA/"/>
    <id>http://blog.jonnydu.me/2017/02/27/RDMA/</id>
    <published>2017-02-27T02:22:39.000Z</published>
    <updated>2020-04-30T14:16:30.270Z</updated>
    
    <content type="html"><![CDATA[<p>RDMA，即Remote DMA，最直观的解释就是将发生在本机的直接内存访问扩展到主机与主机之间。</p><a id="more"></a><h3 id="DMA"><a href="#DMA" class="headerlink" title="DMA"></a>DMA</h3><p>首先，对DMA技术做简单的复习和总结。<br>在最初的PC体系结构中，CPU是系统中唯一的总线主控器，也就是说，为了提取和存储RAM存储单元的值，CPU是唯一可以驱动地址/数据总线的硬件设备。而随着更多诸如PCI的现代总线体系结构的出现，如果提供合适的电路，每一个外围设备都可以充当总线主控器。因此，现在所有的PC都包含一个辅助的DMA电路，它可以用来控制在RAM和IO设备之间数据的传送。DMA一旦被CPU激活，就可以自行传送数据；在数据传输完成之后，DMA发出一个中断请求，再由CPU接管。当CPU和DMA同时访问同一内存单元时，所产生的的冲突由一个名为内存仲裁器的硬件电路解决。<br>由于DMA的设置时间比较长，所以使用DMA最多的是磁盘驱动器和其他需要一次传送大量字节的设备，而在传送数量很少的数据时直接使用CPU效率更高。   </p><h3 id="RDMA"><a href="#RDMA" class="headerlink" title="RDMA"></a>RDMA</h3><p>传统的TCP/IP技术在数据包处理过程中，要经过操作系统及其他软件层，需要占用大量的服务器资源和内存总线带宽，数据在系统内存、处理器缓存和网络控制器缓存之间来回进行复制移动，给服务器的CPU和内存造成了沉重负担。尤其是网络带宽、处理器速度与内存带宽三者的严重”不匹配性”，更加剧了网络延迟效应。<br>RDMA是一种新的内存访问技术，RDMA让计算机可以直接存取其他计算机的内存，而不需要经过处理器耗时的处理。RDMA将数据从一个系统快速移动到远程系统存储器中，而不对操作系统造成任何影响。<br>RDMA技术的原理及其与TCP/IP架构的对比如下图所示。<br><img src="http://i.imgur.com/G5E7u7t.png" alt="">  </p><p>因此，RDMA可以简单理解为利用相关的硬件和网络技术，服务器1的网卡可以直接读写服务器2的内存，最终达到高带宽、低延迟和低资源利用率的效果。如下图所示，应用程序不需要参与数据传输过程，只需要指定内存读写地址，开启传输并等待传输完成即可。<br><img src="http://i.imgur.com/cFfY963.jpg" alt=""></p><p>在实现上，RDMA实际上是一种智能网卡与软件架构充分优化的远端内存直接高速访问技术，通过在网卡上将RDMA协议固化于硬件，以及支持零复制网络技术和内核内存旁路技术这两种途径来达到其高性能的远程直接数据存取的目标。<br>（1）零复制：零复制网络技术使网卡可以直接与应用内存相互传输数据，从而消除了在应用内存与内核之间复制数据的需要。因此，传输延迟会显著减小。<br>（2）内核旁路：内核协议栈旁路技术使应用程序无需执行内核内存调用就可向网卡发送命令。在不需要任何内核内存参与的条件下，RDMA请求从用户空间发送到本地网卡并通过网络发送给远程网卡，这就减少了在处理网络传输流时内核内存空间与用户空间之间环境切换的次数。</p><p>在具体的远程内存读写中，RDMA操作用于读写操作的远程虚拟内存地址包含在RDMA消息中传送，远程应用程序要做的只是在其本地网卡中注册相应的内存缓冲区。远程节点的CPU除在连接建立、注册调用等之外，在整个RDMA数据传输过程中并不提供服务，因此没有带来任何负载。</p><h3 id="RDMA的不同实现"><a href="#RDMA的不同实现" class="headerlink" title="RDMA的不同实现"></a>RDMA的不同实现</h3><p>如下图所示，RDMA的实现方式主要分为InfiniBand和Ethernet两种传输网络。而在以太网上，又可以根据与以太网融合的协议栈的差异分为iWARP和RoCE（包括RoCEv1和RoCEv2）。<br><img src="http://i.imgur.com/EgvDMTD.png" alt=""><br>其中，InfiniBand是最早实现RDMA的网络协议，被广泛应用到高性能计算中。但是InfiniBand和传统TCP/IP网络的差别非常大，需要专用的硬件设备，承担昂贵的价格。鉴于此，这里不对InfiniBand做过多的讨论。<br>在基于以太网的版本中，下面重点选择RoCEv2来讨论。<br>可以看出，RoCEv2的协议栈包括IB传输层、TCP/UDP、IP和Ethernet，其中，后面三层都使用了TCP/IP中相应层次的封包格式。RoCEv2的封包格式如下图所示。<br><img src="http://i.imgur.com/0KMeGlN.png" alt=""><br>其中，UDP包头中，目的端口号为4791即代表是RoCEv2帧。IB BTH即InfiniBand Base Transport Header，定义了IB传输层的相应头部字段。IB Payload即为消息负载。ICRC和FCS分别对应冗余检测和帧校验。<br>IB BTH格式和字段定义如下图。其中，Opcode用于表明该包的type或IB payload中更高层的协议类型。S是Solicited Event的缩写，表明回应者产生应该产生一个事件。M是MigReq的缩写，一般用于迁移状态。Pad表明有多少额外字节被填充到IB payload中。TVer即Transport Header Version，表明该包的版本号。Partition Key用来表征与本packet关联的逻辑内存分区。rsvd是reserved的缩写，该字段是保留的。Destination QP表明目的端Queue Pair序号。A是Acknowledge Request，表示该packet的应答可由响应者调度。PSN是Packet Sequence Number，用来检测丢失或重复的数据包。<br><img src="http://i.imgur.com/uPjVonk.png" alt=""><br>最后，顺带说下RDMA网卡的出包。如前文所述，RDMA是一种智能网卡与软件架构充分优化的远端内存直接高速访问技术，通过将RDMA技术固化于网卡上实现，即，在RoCEv2协议栈中，IB BTH、UDP、IP以及Ethernet Layer全是固化在网卡上的。用户空间的Application通过OFA Stack（亦或其他组织编写的RDMA stack）提供的verbs编程接口（比如WRITE、READ、SEND等）形成IB payload，接下来便直接进入硬件，由RDMA网卡实现负载的层层封装。</p><h3 id="附"><a href="#附" class="headerlink" title="附"></a>附</h3><p><img src="http://i.imgur.com/vwXu2Zm.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;RDMA，即Remote DMA，最直观的解释就是将发生在本机的直接内存访问扩展到主机与主机之间。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="NVMe over RDMA" scheme="http://blog.jonnydu.me/tags/NVMe-over-RDMA/"/>
    
  </entry>
  
  <entry>
    <title>NVMe技术浅析</title>
    <link href="http://blog.jonnydu.me/2017/02/17/NVMe/"/>
    <id>http://blog.jonnydu.me/2017/02/17/NVMe/</id>
    <published>2017-02-17T12:09:40.000Z</published>
    <updated>2020-04-30T14:16:30.271Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="http://blog.dujiong.net/2017/02/13/storage/" target="_blank" rel="noopener">存储的那些词儿</a>一文中已经提到了NVMe是使用PCIe通道的一种逻辑设备接口标准（是接口标准，不是接口！），本文将进一步地分析NVMe的设计及其带来的性能提升。</p><a id="more"></a><h3 id="NVMe设计"><a href="#NVMe设计" class="headerlink" title="NVMe设计"></a>NVMe设计</h3><p>NVMe指定了Host与SSD之间通信的命令，以及命令执行的流程。NVMe由两种命令组成，一种是Admin Command，用于Host管理和控制SSD；另外一种是I/O Command，用于Host和SSD之间数据的传输。<br>同ATA中定义的命令相比，NVMe的命令个数少了很多，完全就是为SSD量身定制的。这里放一张NVMe1.2支持的IO Command列表。<br><img src="http://i.imgur.com/INLzJpX.png" alt=""><br>除了指定Host与SSD之间的通信命令以外，NVMe还定义了命令的执行流程。NVMe中定义了三个关键组件用于命令和数据的处理：Submission Queue（SQ）、Completion Queue（CQ）和Doorbell Register（DB）。SQ和CQ位于Host的内存中，DB位于SSD的控制器内部。在详细说明NVMe命令的处理流程之前，先上一张图宏观感受下PCIe系统和NVMe的结合。图中的NVMe Subsystem一般就是SSD，作为一个PCIe Endpoint通过PCIe连接着Root Complex（RC），然后RC连接着CPU和内存。<br><img src="http://i.imgur.com/vl7O3dS.png" alt=""><br>下面通过SQ、CQ和DB这三个关键组件来总体认识NVMe是如何处理命令的。SQ位于内存中，Host要发送命令时，先把准备好的命令放在SQ中（每个命令条目大小是64字节），然后通知SSD来取；在通知中发挥作用的就是DB寄存器，Host通过写SSD端的DB寄存器来告知SSD执行命令；此外，CQ也是位于Host内存中，一个命令执行完成，成功或失败，SSD总会往CQ中写入命令完成状态（每个条目是16字节）。<br>下图展示了NVMe一次完整处理指令的流程，共八步：<br>（1）Host将指令写到SQ；<br>（2）Host写DB，通知SSD取指令；<br>（3）SSD收到通知，从SQ中取走指令；<br>（4）SSD执行指令；<br>（5）指令执行完成，SSD往CQ写入指令执行结果；<br>（6）SSD通知Host指令执行完成；<br>（7）Host收到通知，处理CQ，查看指令完成状态；<br>（8）Host处理完CQ中的指令执行结果，通过DB回复SSD。<br><img src="http://i.imgur.com/Z25eGGp.png" alt="">   </p><h4 id="SQ-CQ-DB详细分析"><a href="#SQ-CQ-DB详细分析" class="headerlink" title="SQ/CQ/DB详细分析"></a>SQ/CQ/DB详细分析</h4><p>如前文所述，SQ/CQ/DB是NVMe处理命令的关键。Host往SQ中写入命令，SSD往CQ中写入命令完成结果。NVMe中有两种SQ和CQ，一种是Admin SQ，用于放Admin命令；一种是I/O SQ，放I/O命令。如下图所示，系统中只有一对Admin SQ/CQ，它们是一一对应的关系；I/O SQ/CQ却可以有很多，最大值65535（64K-1）。此外，Host端每个Core可以有一个或多个SQ，但只有一个CQ，即SQ与CQ可以是一对一或多对一的关系。这样设计的原因有二：一是性能需求，一个Core中有多线程，可以做到一个线程独享一个SQ；二是QoS需求，可以对多个SQ设置不同的优先级。<br><img src="http://i.imgur.com/bIIfhg0.png" alt=""><br>此外，作为队列，每个SQ和CQ都有一定的深度，对Admin SQ/CQ来说，其深度可以是2-4096（4K），对I/O SQ/CQ，队列深度可以是2-65536（64K）。<br>综上，NVMe中SQ/CQ的个数可以配置，每个SQ/CQ的深度也可以配置，即NVMe的性能是可以通过配置队列个数和队列深度来灵活调节的。这一点和AHCI相比是巨大的提升，因为我们知道，AHCI只有一个命令队列，且队列深度是固定的32。<br>说了这么多SQ和CQ，DB呢？我们知道，SQ和CQ都是队列（且是环形队列），队列的头尾很重要，头决定谁会最先被服务，尾决定新到来命令的位置。所以，我们需要记录SQ和CQ的头尾位置，这就是DB的作用之一。DB是在SSD端的寄存器，记录SQ和CQ的头尾位置。每个SQ或CQ，都有两个对应的DB：Head DB和Tail DB。<br><img src="http://i.imgur.com/8IzJSna.png" alt=""><br>从上图可以看出，这是一个生产者/消费者模型。对一个SQ来说，它的生产者是Host，因为它往SQ的Tail位置写入命令，消费者是SSD，它从SQ的Head取出指令执行。CQ则刚好相反，生产者是SSD，消费者是Host。<br>DB的另一个作用是通知，Host更新SQ Tail DB的同时，也是在告知SSD有新的命令需要处理；Host更新CQ Head DB的同时，也是在告知SSD返回的命令完成状态信息已经被处理。<br>下面以一个实例来说明SQ/CQ/DB三者配合的详细过程。<br>（1）开始时假设SQ1和CQ1是空的，Head=Tail=0；<br><img src="http://i.imgur.com/L8WPeKE.png" alt=""><br>（2）Host往SQ1中写入三个命令，SQ1的Tail变为3。Host在往SQ1写入三个命令后，同时去更新SSD Controller的SQ1 Tail DB寄存器，值为3。Host更新这个寄存器的同时，也是在告诉SSD Controller，有新命令了。<br><img src="http://i.imgur.com/sRJyKnc.png" alt=""><br>（3）SSD Controller收到通知后，于是派人去SQ1把3个命令都取回来执行。SSD把SQ1的三个命令都消费了，SQ1的Head从而也调整为3，SSD Controller会把这个Head值写入到本地的SQ1 Head DB寄存器。<br><img src="http://i.imgur.com/riHdErF.png" alt=""><br>（4）SSD执行完了两个命令，于是往CQ1中写入两个命令完成信息，同时更新CQ1对应的Tail DB寄存器，值为2。同时，SSD发消息给Host告知有命令完成。<br><img src="http://i.imgur.com/UmkUyR6.png" alt=""><br>（5）Host收到SSD的通知后，从CQ1中取出那两条完成信息处理。待处理完毕，Host往CQ1 Head DB寄存器中写入CQ1的head，值为2。<br><img src="http://i.imgur.com/vd75vaC.png" alt=""><br>这样，就完成了一次完整的命令处理。    </p><h3 id="NVMe总结"><a href="#NVMe总结" class="headerlink" title="NVMe总结"></a>NVMe总结</h3><p>NVMe所带来的重大改进主要包含以下几方面：一是低延迟，低延时和良好的并行性可以让SSD的随机性能大幅提升；其次是支持多队列和更高的队列深度，多队列让CPU的性能得到更好的释放，而队列深度从32提升到最大64K，则大幅提升了SSD的IOPS能力；然后是NVMe的低功耗，其加入了自动功耗状态切换和动态能耗管理功能；最后是其驱动的适用性广，解决了不同PCIe SSD之间的驱动适用性问题。支持NVMe标准的PCIe SSD可适用于多个不同平台，也不需要厂商独立提供驱动支持。目前Windows、Linux、Solaris、Unix、VMware、UEFI等都加入了对NVMe SSD的支持。</p><h3 id="附"><a href="#附" class="headerlink" title="附"></a>附</h3><p>特别鸣谢：<br><a href="http://www.ssdfans.com/" target="_blank" rel="noopener">ssdfans</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;http://blog.dujiong.net/2017/02/13/storage/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;存储的那些词儿&lt;/a&gt;一文中已经提到了NVMe是使用PCIe通道的一种逻辑设备接口标准（是接口标准，不是接口！），本文将进一步地分析NVMe的设计及其带来的性能提升。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="NVMe over RDMA" scheme="http://blog.jonnydu.me/tags/NVMe-over-RDMA/"/>
    
  </entry>
  
  <entry>
    <title>存储的那些词儿</title>
    <link href="http://blog.jonnydu.me/2017/02/13/storage/"/>
    <id>http://blog.jonnydu.me/2017/02/13/storage/</id>
    <published>2017-02-13T07:06:23.000Z</published>
    <updated>2020-04-30T14:16:30.247Z</updated>
    
    <content type="html"><![CDATA[<p>海量数据的迸发和传统存储技术所面临的性能瓶颈，促进了新型存储技术和系统架构的高速发展。</p><a id="more"></a><h3 id="存储的那些词儿"><a href="#存储的那些词儿" class="headerlink" title="存储的那些词儿"></a>存储的那些词儿</h3><p>本文简单介绍下在存储领域出现频率最高的几个词语:SATA、PCIe、AHCI和NVMe（Non-Volatile Memory Express，非易失性存储器标准）。</p><h4 id="SATA和PCIe"><a href="#SATA和PCIe" class="headerlink" title="SATA和PCIe"></a>SATA和PCIe</h4><p>SATA和PCIe大家应该比较熟悉，这两个都是总线标准。<br>SATA由IDE/ATA标准发展而来，主要用途是把存储设备连接到主板。SATA的发展主要经历了以下版本：<br>SATA revision 1.0 (1.5 Gbit/s, 150 MB/s)<br>SATA revision 2.0 (3 Gbit/s, 300 MB/s)<br>SATA revision 3.0 (6 Gbit/s, 600 MB/s)<br>SATA revision 3.1<br>SATA revision 3.2 (16 Gbit/s, 1969 MB/s)<br>SATA在发展过程中，考虑了向下兼容的问题，比如主板上SATA-3的接口，可以连接SATA-2的硬盘。但同时，向下兼容也造成了其发展缓慢。<br>出于向下兼容的考虑，SATA可以工作在两种模式：传统模式和AHCI模式。传统模式是为了兼容以前的 IDE/ATA。AHCI模式则比较新，支持SATA独有的功能，如热插拔、原生命令队列（NCQ）等。      </p><p>PCIe是另一种总线标准，由AGP、PCI、PCI-x发展而来，而这些总线的发展，主要的动力是显卡的发展。AGP就是Accelerated Graphics Port（加速图像端口）的缩写。由于显卡需要很大的带宽和速度，PCI总线标准就不断升级来满足要求。当然，除了显卡外，PCI总线还用于其他的扩展卡，如网卡（包括有线网卡、无线网卡、3G/4G卡等）。  </p><h4 id="AHCI和NVMe"><a href="#AHCI和NVMe" class="headerlink" title="AHCI和NVMe"></a>AHCI和NVMe</h4><p>AHCI和NVMe是逻辑（或者说软件、驱动程序）上的标准。<br>从上面 SATA的不同版本可以看到，提速是一个主要任务（当然也有其他的改进）。但进入SSD时代后，SATA的改版速度（由于要考虑向下兼容），已经跟不上传输速度的要求了。这时候，业界就考虑采用PCIe来连接存贮设备。但在驱动程序层面，仍然采用AHCI。这是因为AHCI已经非常成熟，广泛被各种操作系统（如Windows、Linux）所采用。<br>AHCI是为了发挥SATA的潜能而设计的，当时算是“高大上”了。但当时仍然是机械硬盘统治市场，因此AHCI的设计是基于机械硬盘的特性（旋转式磁性盘片）。虽然AHCI也可以用于SSD，但却不能发挥极致。因为SSD更像内存，而不像“盘片”。譬如说，机械硬盘，如果磁头错过了一个扇区，那就得等盘片转一圈回来才能访问。SSD就不存在这个问题。因此，业界重新设计一个新的NVMe协议，希望发挥 SSD的潜能。下面是AHCI和NVMe的对比：<br><img src="http://i.imgur.com/4uOqsG8.png" alt="">   </p><h4 id="物理接口"><a href="#物理接口" class="headerlink" title="物理接口"></a>物理接口</h4><p>说完了总线和协议，下面说说物理接口。无论采用什么总线和协议，主板总得连接到存储设备上。这里所说的物理接口，指是是物理尺寸和形状，电气特征不作讨论。接口分为主机端和设备端，种类繁多，这里挑几个常见的。<br>1.SATA接口。采用这种接口的，只能使用SATA总线，不能使用PCIe总线。大部分2.5”SSD就是这种接口。<br>2.M.2接口。采用这种接口的SSD，可以使用SATA或者PCIe总线（取决于主板和SSD）。如果采用PCIe总线，又分为AHCI和NVMe两种协议。<br>3.SATA Express接口。SATA Express使用的是PCIe总线，向下兼容SATA总线。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>不同存储总线标准/协议之间的组合如下图所示。<br><img src="http://i.imgur.com/J22ZZLp.png" alt=""><br>可以看出，AHCI和NVMe是驱动程序层面的。NVMe只适用于SSD（SSD和主板也要支持NVMe才行）。AHCI则适用于机械硬盘和SSD。<br>在主板芯片层面，有AHCI控制器和PCIe控制器。有趣的是AHCI驱动程序“居然”可以使用PCIe控制器（中间那条橙色的线）。这个其实是个过渡方案，目的是在利用PCIe高带宽的同时，保持对上层软件的兼容性。<br>绿色那个框是主板上的物理接口。注意即使是同样的物理接口，也可以选择不同的总线和协议（如果主机和设备支持的话）。<br>右下角的PCIe SSD设备则可以有两种不同的控制器（最下面的两个框）：AHCI和NVMe。因此，同样是PCIe的SSD，也可以有不同的传输效能。<br>说了这么多，相信大家已经晕了。下面按传输效率做个排序。<br>1.PCIe NVMe<br>这个是最高大上的。在笔记本市场，根据效能，可以再细分为两个等级：<br>（1）M.2尺寸的NVMe（如三星 950 PRO）。可以有四条PCIe通道，速度最快。但由于电路板面积限制，容量和发热都是个问题。<br>（2）2.5″尺寸的NVMe（如东芝XG3），采用SATA Express接口，可以有两条PCIe通道，传输速率较低。此外，由于2.5″体积较大，容量和发热比 M.2 要好。<br>2.PCIe AHCI<br>效能比1稍低，是由于AHCI协议的滞后性决定的。笔记本上只有M.2外形，没有2.5″外形。<br>3.SATA AHCI<br>效能最低，但兼容性最好，根据外形可分为两类。这两类的传输效能是一样的，无分高低。<br>（1）M.2外形的设备，如三星850EVO的M.2盘。<br>（2）2.5″外形的设备，如目前广泛使用的机械硬盘，固态硬盘等。</p><h3 id="附"><a href="#附" class="headerlink" title="附"></a>附</h3><p>接下来将详细分析NVMe的设计及其带来的性能提升。   </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;海量数据的迸发和传统存储技术所面临的性能瓶颈，促进了新型存储技术和系统架构的高速发展。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="NVMe over RDMA" scheme="http://blog.jonnydu.me/tags/NVMe-over-RDMA/"/>
    
  </entry>
  
  <entry>
    <title>分配排序（桶排序和基数排序）总结</title>
    <link href="http://blog.jonnydu.me/2017/01/20/Distribute-Sort/"/>
    <id>http://blog.jonnydu.me/2017/01/20/Distribute-Sort/</id>
    <published>2017-01-20T11:51:47.000Z</published>
    <updated>2020-04-30T14:16:30.271Z</updated>
    
    <content type="html"><![CDATA[<p>分配排序，是指不需要进行两两之间的比较，而根据记录自己的关键码的分配来进行排序的一类方法，因此，在进行分配排序时，我们通常需要知道记录序列的一些具体情况，比如关键码的分布。分配排序主要包括桶排序和基数排序。   </p><a id="more"></a><h3 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h3><p>首先来介绍桶排序。如果我们知道序列中的记录都位于一个比较小的区间范围之内，那我们可以把相同值的记录分配到同一个桶里面，然后依次收集这些桶就能得到一个有序的序列。<br>例如，下图中，我们知道待排序数组的记录值在0~9的范围之内，因此，我们可以设定10个桶，然后就把这些记录值分配到各个桶里面。<br><img src="http://i.imgur.com/SVkuDWp.png" alt=""><br>此外，我们还维护了前若干桶的一个累加值，并且，在进行排序的时候，我们从数组记录值的最右边开始遍历，这样来保持排序的稳定性。<br><img src="http://i.imgur.com/yGcdDMb.png" alt=""></p><h4 id="桶排序的实现"><a href="#桶排序的实现" class="headerlink" title="桶排序的实现"></a>桶排序的实现</h4><p>在弄清楚了原理之后，桶排序的实现就很简单了。</p><pre><code> void BucketSort(vector&lt;int&gt;&amp; nums, int max){     int n=nums.size();     vector&lt;int&gt; temp(begin(nums), end(nums));     vector&lt;int&gt; count(max);     for(int i=0;i&lt;max;i++){         count[i]=0;     }     for(int i=0;i&lt;n;i++){         count[nums[i]]++;     }     for(int i=0;i&lt;max;i++){         count[i]=count[i-1]+count[i];     }     for(int i=n-1;i&gt;=0;i--){             nums[--count[temp[i]]]=temp[i];     }}</code></pre><h4 id="桶排序性能分析"><a href="#桶排序性能分析" class="headerlink" title="桶排序性能分析"></a>桶排序性能分析</h4><p>由前面的分析可知，桶排序使用的场景是：待排序的数组长度为n，所有数组记录值位于[0,m)上，且m相对于n很小。<br>因为排序过程需要遍历原数组和count数组，所以总的时间复杂度为O（n+m）。空间代价为O（n+m），包括临时数组和计数器。桶排序是稳定的。   </p><h3 id="静态基数排序"><a href="#静态基数排序" class="headerlink" title="静态基数排序"></a>静态基数排序</h3><p>我们已经知道，桶排序只适合m很小的情况，那如果出现m很大，甚至大于n的情况，应该怎么办呢？不难想象，我们可以把这些记录的关键码认为地拆成几个部分，然后再运用几次桶排序，从而完成整个排序过程，这就是基数排序的思想，基数排序又可以分为静态基数排序和链式基数排序。<br>比如，我们需要对0到9999之间的整数进行排序，这时直接使用桶排序显然是代价较高的。我们可以将四位数看做是由四个排序码决定，个位为最低排序码。基数等于10。然后，按照千，百，十，个位数字依次进行4次桶排序。<br>下面以一个记录值都是两位数的数组的排序过程来说明静态基数排序两趟桶排的具体过程，这里和后面的代码都采用的是低位优先法。<br><img src="http://i.imgur.com/gPF11Hv.png" alt=""><br><img src="http://i.imgur.com/hr8v9TV.png" alt=""></p><h4 id="静态基数排序的实现"><a href="#静态基数排序的实现" class="headerlink" title="静态基数排序的实现"></a>静态基数排序的实现</h4><p>同样，给出静态基数排序的实现。</p><pre><code>void RadixSort(vector&lt;int&gt;&amp; nums, int d, int r){    int n=nums.size();    vector&lt;int&gt; temp(n,0);    vector&lt;int&gt; count(r,0);        //r:radix,10    int radix=1;    int i,j,k;    for(i=1;i&lt;=d;i++){        //d:排序码的个数        for(j=0;j&lt;r;j++){            count[j]=0;        }        for(j=0;j&lt;n;j++){            k=(nums[j]/radix)%r;            count[k]++;        }        for(j=1;j&lt;r;j++){            count[j]=count[j]+count[j-1];        }        for(j=n-1;j&gt;=0;j--){            k=(nums[j]/radix)%r;            count[k]--;            temp[count[k]]=nums[j];        }        for(j=0;j&lt;n;j++){            nums[j]=temp[j];        }        radix*=r;    }}</code></pre><h4 id="静态基数排序性能分析"><a href="#静态基数排序性能分析" class="headerlink" title="静态基数排序性能分析"></a>静态基数排序性能分析</h4><p>同样的，静态基数排序也需要一个临时数组和r个计数器，所以总的空间代价为O（n+r）。时间代价则为O（d*(n+r)），因为它相当于进行了d次桶式排序。此外，静态基数排序也是稳定的。</p><h3 id="链式基数排序"><a href="#链式基数排序" class="headerlink" title="链式基数排序"></a>链式基数排序</h3><p>基于静态链的基数排序相对于静态基数排序的差别在于将分配出来的子序列存放在r个静态链组织的队列中。<br>下面，同样以一个例子来说明链式基数排序的过程。<br>假设有待排序数组[97, 53, 88, 59, 26, 41, 88, 31, 22]，首先按照个位进行第一趟分配，分配完元素后的队列如下图所示。<br><img src="http://i.imgur.com/D5ZCnQk.png" alt=""><br>然后进行第一趟收集，即个位有序：[41, 31, 22, 53, 26, 97, 88, 88]。<br>接下来，再按照十位进行第二趟分配。<br><img src="http://i.imgur.com/BUeHB5r.png" alt=""><br>最后进行第二趟收集，即完成最终的排序。</p><h4 id="链式基数排序的实现"><a href="#链式基数排序的实现" class="headerlink" title="链式基数排序的实现"></a>链式基数排序的实现</h4><p>下面给出链式基数排序的实现。</p><pre><code>typedef struct Node{    int key;    int next;    //下一个节点在数组中的下标}Node;typedef struct StaticQueue{    int head;    int tail;}StaticQueue;void Distribute(vector&lt;Node&gt;&amp; node, int first, int i, int r, StaticQueue* queue);void Collect(vector&lt;Node&gt;&amp; node, int&amp; first, int r, StaticQueue* queue);void RadixSort(vector&lt;int&gt;&amp; nums, int d, int r){    int i, first=0;    int n=nums.size();    vector&lt;Node&gt; node(n);    StaticQueue* queue = new StaticQueue[r];    for(i=0;i&lt;n-1;i++){        node[i].key=nums[i];        node[i].next=i+1;    }    node[n-1].key=nums[n-1];    node[n-1].next=-1;    for(i=0;i&lt;d;i++){            //d趟的分配和收集        Distribute(node, first, i, r, queue);            //分配到不同队列        Collect(node, first, r, queue);                    //聚合    }    for(i=0;i&lt;r;i++){        if(queue[i].head==-1) continue;        else break;    }    int j=queue[i].head;    while(node[j].next!=-1){        //排序结果        cout &lt;&lt; node[j].key &lt;&lt; endl;        j = node[j].next;    }    cout &lt;&lt; node[j].key &lt;&lt; endl;    delete[] queue;}void Distribute(vector&lt;Node&gt;&amp; node, int first, int i, int r, StaticQueue* queue){    int current=first;    for(int j=0;j&lt;r;j++){        queue[j].head=-1;    }    while(current!=-1){        int k = node[current].key;        for(int a=0;a&lt;i;a++){            k=k/r;        }        k=k%r;        if(queue[k].head==-1){            queue[k].head=current;        }else{            node[queue[k].tail].next=current;        }        queue[k].tail=current;        current=node[current].next;    }}void Collect(vector&lt;Node&gt;&amp; node, int&amp; first, int r, StaticQueue* queue){    int last,k=0;    while(queue[k].head==-1) k++;    first=queue[k].head;    last=queue[k].tail;    while(k&lt;r-1){        k++;        while(k&lt;r-1 &amp;&amp; queue[k].head==-1){            k++;        }        if(queue[k].head!=-1){            node[last].next=queue[k].head;            last=queue[k].tail;        }    }    node[last].next=-1;}</code></pre><h4 id="链式基数排序性能分析"><a href="#链式基数排序性能分析" class="headerlink" title="链式基数排序性能分析"></a>链式基数排序性能分析</h4><p>由前面的分析和代码可知，链式基数排序在分配和收集的过程中，不需要移动记录本身，只是在做记录next指针的修改。因此，它的时间代价和空间代价与静态基数排序一致，分别为O（d*(n+r)）和O（n+r）。并且，根据队列的先入先出的特点，链式基数排序也是稳定的。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文总结了分配排序中最常见的桶排序和基数排序，并给出了具体的实现和相关分析。可以看出，分配排序不会对记录值进行两两比较，因此不受O（nlgn）时间复杂度的限制。从另外一个角度看，分配排序也可以看做是以空间换时间的典型方法。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;分配排序，是指不需要进行两两之间的比较，而根据记录自己的关键码的分配来进行排序的一类方法，因此，在进行分配排序时，我们通常需要知道记录序列的一些具体情况，比如关键码的分布。分配排序主要包括桶排序和基数排序。   &lt;/p&gt;
    
    </summary>
    
    
    
      <category term="数据结构与算法" scheme="http://blog.jonnydu.me/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>搭建Redis集群</title>
    <link href="http://blog.jonnydu.me/2017/01/15/Redis-Cluster/"/>
    <id>http://blog.jonnydu.me/2017/01/15/Redis-Cluster/</id>
    <published>2017-01-15T07:08:05.000Z</published>
    <updated>2020-04-30T14:16:30.279Z</updated>
    
    <content type="html"><![CDATA[<p>Redis集群是Redis提供的分布式数据库方案，集群通过分片来进行数据共享，并提供复制和故障转移功能。    </p><a id="more"></a><h3 id="Redis集群概述"><a href="#Redis集群概述" class="headerlink" title="Redis集群概述"></a>Redis集群概述</h3><p>Redis集群使用数据分片而非一致性哈希来实现，一个Redis集群包含16384个哈希槽（slot），数据库中的每个键都属于这16384个哈希槽中的其中一个，集群中的每个节点可以处理0个或最多16384个槽，当数据库中的16384个槽都有节点在处理时，集群处于上线状态；而如果数据库中有任何一个槽没有得到处理，那么集群处于下线状态。<br>集群中的每个节点负责处理一部分哈希槽，比如，现在有三个独立的节点127.0.0.1:7000,127.0.0.1:7001,127.0.0.1:7002，其中各节点处理的哈希槽关系：节点7000负责处理0到5000号哈希槽，节点7001负责处理5001到10000号哈希槽，节点7002负责处理10001到16384号哈希槽。<br>从而，当向集群中添加或删除节点时，集群只需在对应节点的哈希槽做移动即可。不会造成节点阻塞、集群下线。<br>当然，为了使得集群在一部分节点下线的情况下仍然可以正常运作，Redis集群对节点提供了主从复制功能，集群中的每个节点都有1到N个复制节点，形成主-从模型。</p><h3 id="Redis集群架构图"><a href="#Redis集群架构图" class="headerlink" title="Redis集群架构图"></a>Redis集群架构图</h3><p>Redis集群架构如下图所示：<br><img src="http://i.imgur.com/O4QfdDF.jpg" alt=""><br>Redis集群架构的主要特点有:<br>（1）所有节点批次互联（PING-PONG机制），没有中心控制协调节点，内部使用二进制协议优化传输速度和带宽；<br>（2）节点的失效通过集群中超过半数的节点“投票”监测；<br>（3）客户端与Redis节点直连，不需要中间proxy层，客户端连接集群中任何一个可用节点即可；    </p><h3 id="Redis集群实践"><a href="#Redis集群实践" class="headerlink" title="Redis集群实践"></a>Redis集群实践</h3><p>下面搭建一个简单的Redis集群环境，集群共包含6个节点，其中3个主节点，3个从节点。节点都部署在本机（ubuntu 14.04 x86-64），以端口号区分，分别为127.0.0.1:7000~127.0.0.1:7005。<br>1.首先安装3.0版本之后的Redis（因为Redis集群是在3.0版本提出的功能），这里采用源码(3.2.8版本)安装。</p><pre><code>wget http://download.redis.io/releases/redis-3.2.8.tar.gz</code></pre><p>2.然后解压，安装</p><pre><code>tar xf redis-3.2.8.tar.gz    cd redis-3.2.8       make &amp;&amp; make install</code></pre><p>3.安装完成后查看Redis版本信息，验证安装是否成功。<br><img src="http://i.imgur.com/6QuQXPQ.png" alt=""></p><p>4.创建存放多个节点实例的目录</p><pre><code>mkdir redis-data/cluster -pcd redis-data/clustermkdir 7000 7001 7002 7003 7004 7005</code></pre><p>5.复制并修改配置文件redis.conf</p><pre><code>cp /etc/redis/redis.conf redis-data/cluster/7000</code></pre><p>修改配置文件中下面选项</p><pre><code>port 7000daemonize yescluster-enabled yescluster-config-file nodes.confcluster-node-timeout 5000appendonly yes</code></pre><p>然后把修改完成的redis.conf复制到7001-7005目录下，端口修改成和文件夹对应。 </p><p>6.分别启动6个Redis实例</p><pre><code>cd redis-data/cluster/7000redis-server redis.conf...cd redis-data/cluster/7001redis-server redis.conf</code></pre><p>7.查看进程是否运行<br><img src="http://i.imgur.com/nWUeGum.png" alt=""></p><p>8.接下来创建集群，首先安装依赖包</p><pre><code>apt-get install ruby</code></pre><p>然后安装gem-redis，下载地址<a href="https://rubygems.org/gems/redis/versions/3.3.0" target="_blank" rel="noopener">https://rubygems.org/gems/redis/versions/3.3.0</a></p><pre><code>gem install redis-3.3.0.gem</code></pre><p>接下来复制集群管理程序到/usr/local/bin</p><pre><code>cp redis-3.2.8/src/redis-trib.rb /usr/local/bin/redis-trib</code></pre><p>就可以使用redis-trib创建集群了。</p><pre><code>redis-trib create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005</code></pre><p>其中，命令create表示创建一个新的集群，选项–replicas 1表示未集群中的每个主节点创建一个从节点。即，上述命令运行后，redis-trib将创建一个包含三个主节点和三个从节点的集群。<br><img src="http://i.imgur.com/6jhul4S.png" alt=""><br>键入“yes”后一个三主三从的集群架构就创建完毕。<br><img src="http://i.imgur.com/WTTZPh8.png" alt=""></p><h3 id="一些简单的测试"><a href="#一些简单的测试" class="headerlink" title="一些简单的测试"></a>一些简单的测试</h3><p>创建完毕后，可以随时通过redis-cli -p 7000(node number) cluster nodes来查看集群中各节点的信息。包括唯一的节点ID，主从关系，每个主节点分配的slots范围。<br><img src="http://i.imgur.com/jJcOqwB.png" alt=""></p><h4 id="集群查询"><a href="#集群查询" class="headerlink" title="集群查询"></a>集群查询</h4><p>做一个简单的测试，    在7000节点上存储K-V数据，然后分别在其从节点和其他主节点上获取该数据。<br><img src="http://i.imgur.com/X3GWKHJ.png" alt=""><br><img src="http://i.imgur.com/a0P55nA.png" alt=""><br>要找的数据没有在当前节点上时，cluster发送MOVED指令指示到对应的节点上穿，可以通过-c参数指定查询时接收到MOVED指令自动跳转。</p><h4 id="删除节点"><a href="#删除节点" class="headerlink" title="删除节点"></a>删除节点</h4><p>Slave节点的作用是提供冗余和高可用，大部分情况下用于分担读的压力。移除Slave节点无须多余的动作，直接删除即可。<br><img src="http://i.imgur.com/Hr2xMz6.png" alt=""><br>而如果是Master节点，则需要将其负责的slots范围迁移到其他节点上。保证数据不丢失。<br>所以，先使用redis-trib reshard …来将待删除的节点上的数据迁移到其他节点，然后删除该节点。<br><img src="http://i.imgur.com/xLG6ctH.png" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Redis Cluster是3.0版本之后提供的新功能，采用了P2P的去中心化架构，而没有采用像Codis之类的Proxy解决方案中的中心协调节点设计。本文只是简单搭建了一个Redis集群环境，后续还将在此基础上，进一步深入研究高可用、可扩展的Redis集群方案。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Redis集群是Redis提供的分布式数据库方案，集群通过分片来进行数据共享，并提供复制和故障转移功能。    &lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Redis" scheme="http://blog.jonnydu.me/tags/Redis/"/>
    
  </entry>
  
</feed>

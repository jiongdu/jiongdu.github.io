<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一期一会</title>
  <icon>https://www.gravatar.com/avatar/82f6d8814fca7e2e77b918b4c5e2f486</icon>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.jonnydu.me/"/>
  <updated>2018-05-01T14:04:25.284Z</updated>
  <id>http://blog.jonnydu.me/</id>
  
  <author>
    <name>jonnydu</name>
    <email>jonnydu.uestc@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Linux块IO浅析</title>
    <link href="http://blog.jonnydu.me/2017/05/05/Linux-Block-IO/"/>
    <id>http://blog.jonnydu.me/2017/05/05/Linux-Block-IO/</id>
    <published>2017-05-05T12:08:28.000Z</published>
    <updated>2018-05-01T14:04:25.284Z</updated>
    
    <content type="html"><![CDATA[<p>块存储，简单来说就是使用块设备为系统提供存储服务。块存储分多钟类型，有单机块存储（磁盘）、网络存储（NAS、SAN等），分布式存储（云硬盘）。通常块存储的表现形式就是一块设备，用户看到的就是类似于sda、sdb这样的逻辑设备。<br><a id="more"></a><br>可以说，生活中最常见的块存储设备就是硬盘了，接下来将以硬盘为例简要说明Linux块IO。   </p><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><p>类似于网络协议栈的分层结构，Linux内核块设备I/O总体结构图如下所示。从图中可以看出，以对磁盘的读请求为例，读操作会依次经过虚拟文件系统层（VFS），Cache层（Page Cache）、具体的文件系统层（比如xfs），通用块层（Generic Block Layer）、I/O调度层、块设备驱动层，最后到达物理块设备层。当然，这其中还包括绕开Page Cache的直接（Direct）I/O模式。下面对每一层做简要介绍。</p><p><img src="http://i.imgur.com/2ELr6pl.png" alt=""></p><h4 id="虚拟文件系统层"><a href="#虚拟文件系统层" class="headerlink" title="虚拟文件系统层"></a>虚拟文件系统层</h4><p>VFS虚拟文件系统是一种软件机制，扮演着文件系统管理者的角色，与它相关的数据结构只存在于物理内存当中。它的作用是屏蔽下层具体文件系统操作的差异，为上层的操作提供一个统一的接口。也正是由于VFS的存在，Linux中允许众多不同的文件系统共存并且对文件的操作可以跨文件系统而执行。<br>Linux中VFS主要依靠四个数据结构来描述相关信息,分别是超级块、索引节点、目录项和文件对象。<br>（1）超级块(Super Block)<br>超级块对象表示一个文件系统。它存储一个已安装的文件系统的控制信息，包括文件系统名称（比如Ext2）、文件系统的大小和状态、块设备的引用和元数据信息（比如空闲列表等等）。超级块一般存储在磁盘的特定扇区中，但是对于那些基于内存的文件系统（如proc、sysfs），超级块是在使用时创建在内存中。<br>（2）索引节点（Inode）<br>索引节点是VFS中的核心概念，它包含内核在操作文件或目录时需要的全部信息。<br>一个索引节点代表文件系统的一个文件（这里的文件不仅是指我们平时所认为的普通的文件，还包括目录、特殊设备文件等）。<br>索引节点和超级块一样是实际存储在磁盘上的，当被应用程序访问时才会在内存中创建。<br>（3）目录项（Dentry）<br>引入目录项对象的概念主要是出于方便查找文件的目的。不同于前面的两个对象，目录项对象没有对应的磁盘数据结构，只存在于内存中，一个路径的各个组成部分，不管是目录还是普通的文件，都是一个目录项对象。在路径/home/source/test.cpp中，目录/, home, source和文件test.cpp都对应一个目录项对象。VFS在查找的时候，根据一层层的目录项找到对应的目录项Inode，就可以找到最终的文件。<br>（4）文件对象（File）<br>文件对象描述的是进程已经打开的文件。因为一个文件可以被多个进程打开，所以一个文件可以存在多个文件对象。一个文件对应的文件对象可能不是惟一的，但是其对应的索引节点和目录项对象肯定是惟一的。 </p><h4 id="Page-Cache层"><a href="#Page-Cache层" class="headerlink" title="Page Cache层"></a>Page Cache层</h4><p>引入Page Cache层的目的是为了提高Linux操作系统对磁盘访问的性能。Cache层在内存中缓存了磁盘上的部分数据。当数据的请求到达时，如果在Cache中存在该数据且是最新的，则直接将数据传递给用户程序，免除了对底层磁盘的操作，提高了性能。<br>在Linux的实现中，文件Cache分为两个层面，一是Page Cache，另一个是Buffer Cache，前者主要用来作为文件系统上的文件数据的缓存使用，尤其是针对当进程对文件有read/write操作的时候。Buffer Cache则主要设计用来在系统对块设备进行读写的时候，对块进行数据缓存的系统来使用。关于二者的详细对比说明，请参考<a href="http://blog.dujiong.net" target="_blank" rel="noopener">一期一会</a><br>磁盘Cache有两大功能：预读和回写。预读其实就是利用了局部性原理，具体过程是：对于每个文件的第一个读请求，系统读入所请求的页面并读入紧随其后的少数几个页面（通常是三个页面），这时的预读称为同步预读。对于第二次读请求，如果所读页面不在Cache中，即不在前次预读的页中，则表明文件访问不是顺序访问，系统继续采用同步预读；如果所读页面在Cache中，则表明前次预读命中，操作系统把预读页的大小扩大一倍，此时预读过程是异步的，应用程序可以不等预读完成即可返回，只要后台慢慢读页面即可，这时的预读称为异步预读。任何接下来的读请求都会处于两种情况之一：第一种情况是所请求的页面处于预读的页面中，这时继续进行异步预读；第二种情况是所请求的页面处于预读页面之外，这时系统就要进行同步预读。<br>回写是通过暂时将数据存在Cache里，然后统一异步写到磁盘中。通过这种异步的数据I/O模式解决了程序中的计算速度和存储速度不匹配的鸿沟，减少了访问底层存储介质的次数，使存储系统的性能大大提高。</p><h4 id="文件系统层"><a href="#文件系统层" class="headerlink" title="文件系统层"></a>文件系统层</h4><p>VFS的下一层是具体的文件系统。比如Ext2、Ext3、xfs等。<br>限于个人知识水平，暂时不对具体的文件系统做介绍。待后面研究过后，再做分析。    </p><h4 id="通用块层"><a href="#通用块层" class="headerlink" title="通用块层"></a>通用块层</h4><p>通用块层的主要工作是：接收上层发出的磁盘请求，并最终发出I/O请求。该层隐藏了底层硬件块设备的特性，为块设备提供了一个通用的抽象视图。<br>对于VFS和具体的文件系统来说，块（Block）是基本的数据传输单元，当内核访问文件的数据时，它首先从磁盘读取一个块。但是对于磁盘来说，扇区是最小的可寻址单元，块设备无法对比它还小的单元进行寻址和操作。由于扇区是磁盘的最小可寻址单元，所以块不能比扇区还小，只能整数倍于扇区大小，即一个块对应磁盘上的一个或多个扇区。一般来说，块大小是2的整数倍，而且由于Page Cache层的最小单元是页（Page），所以块大小不能超过一页的长度。<br>大多数情况下，数据的传输通过DMA方式。旧的磁盘控制器，仅仅支持简单的DMA操作：，只能传输磁盘上相邻的扇区，即数据在内存中也是连续的。这是因为如果传输非连续的扇区，会导致磁盘花费更多的时间在寻址操作上。而现在的磁盘控制器支持“分散/聚合”DMA操作，这种模式下，数据传输可以在多个非连续的内存区域中进行。为了利用“分散/聚合”DMA操作，块设备驱动必须能处理被称为段（segments）的数据单元。一个段就是一个内存页面或一个页面的部分，它包含磁盘上相邻扇区的数据。<br>通用块层是粘合所有上层和底层的部分，一个页的磁盘数据布局如下所示：      </p><p><img src="http://i.imgur.com/ISbyMca.png" alt=""></p><h4 id="I-O调度层"><a href="#I-O调度层" class="headerlink" title="I/O调度层"></a>I/O调度层</h4><p>I/O调度层的功能是管理块设备的请求队列。即接收通用块层发出的I/O请求，缓存请求并试图合并相邻的请求，并根据设置好的调度算法，回调驱动层提供的请求处理函数，以处理具体的I/O请求。<br>如果简单地以内核产生请求的次序直接将请求发给块设备的话，那么块设备性能肯定让人难以接受，因为磁盘寻址是整个计算机中最慢的操作之一。为了优化寻址操作，内核不会一旦接收到I/O请求后，就按照请求的次序发起块I/O请求。为此Linux实现了几种I/O调度算法，算法基本思想就是通过合并和排序I/O请求队列中的请求，以此大大降低所需的磁盘寻道时间，从而提高整体I/O性能。<br>常见的I/O调度算法包括Noop调度算法（No Operation）、CFQ（完全公正排队I/O调度算法）、DeadLine（截止时间调度算法）、AS预测调度算法等。     </p><h4 id="块设备驱动层"><a href="#块设备驱动层" class="headerlink" title="块设备驱动层"></a>块设备驱动层</h4><p>驱动层中的驱动程序对应具体的物理块设备。它从上层中取出I/O请求，并根据该I/O请求中指定的信息，通过向具体块设备的设备控制器发送命令的方式，来操纵设备传输数据。</p><h3 id="磁盘I-O优化技巧"><a href="#磁盘I-O优化技巧" class="headerlink" title="磁盘I/O优化技巧"></a>磁盘I/O优化技巧</h3><p>前文已经叙述了Linux通过Cache以及排序合并I/O请求来提高系统的性能，其本质原因是磁盘随机读写慢，顺序读写快。本节介绍一些现有开源系统常见的针对磁盘I/O特性的优化技巧。       </p><h4 id="追加写"><a href="#追加写" class="headerlink" title="追加写"></a>追加写</h4><p>在进行系统设计时，良好的读性能和写性能往往不可兼得。在许多常见的开源系统中都是在优先保证写性能的前提下来优化读性能的。使系统拥有良好的写性能的一个常见方法就是采用追加写，每次将数据添加到文件。由于完全是顺序的，所以可以具有非常好的写操作性能。但是这种方式也存在一些缺点：从文件中读一些数据时将会需要更多的时间：需要倒序扫描，直到找到所需要的内容。<br>LevelDB所采用的LSM（日志结构合并树）是一种较好的追加写设计方案，LSM的思想是将整个磁盘看做一个日志，在日志中存放永久性数据及其索引，每次都添加到日志的末尾。并且通过将很多小文件的存取转换为连续的大批量传输，使得对于文件系统的大多数存取都是顺序的，从而提高磁盘I/O。其具体设计可参考<a href="http://blog.dujiong.net/2017/03/27/leveldb/" target="_blank" rel="noopener">LevelDB原理剖析</a></p><h4 id="文件合并和元数据优化"><a href="#文件合并和元数据优化" class="headerlink" title="文件合并和元数据优化"></a>文件合并和元数据优化</h4><p>目前的大多数文件系统，如XFS/Ext4、HDFS，在元数据管理、缓存管理等实现策略上都侧重大文件。这些文件系统在面临海量小文件时在性能和存储效率方面都大幅降低，根本原因是磁盘最适合顺序的大文件I/O读写模式，而非常不适合随机的小文件I/O读写模式。主要原因体现在元数据管理低效和数据布局低效：<br>（1）元数据管理低效：由于小文件数据内容较少，因此元数据的访问性能对小文件访问性能影响巨大。Ext2文件系统中Inode和Data Block分别保存在不同的物理位置上，一次读操作需要至少经过两次的独立访问。在海量小文件应用下，Inode的频繁访问，使得原本的并发访问转变为了海量的随机访问，大大降低了性能。另外，大量的小文件会快速耗尽Inode资源，导致磁盘尽管有大量Data Block剩余也无法存储文件，会浪费磁盘空间。<br>（2）数据布局低效：Ext2在Inode中使用多级指针来索引数据块。对于大文件，数据块的分配会尽量连续，这样会具有比较好的空间局部性。但是对于小文件，数据块可能零散分布在磁盘上的不同位置，并且会造成大量的磁盘碎片，不仅造成访问性能下降，还大量浪费了磁盘空间。数据块一般为1KB、2KB或4KB，对于小于4KB的小文件，Inode与数据的分开存储破坏了空间局部性，同时也造成了大量的随机I/O。<br>对于海量小文件应用，常见的I/O流程复杂也是造成磁盘性能不佳的原因。对于小文件，磁盘的读写所占用的时间较少，而用于文件的open()操作占用了绝大部分系统时间，导致磁盘有效服务时间非常低，磁盘性能低下。针对于问题的根源，优化的思路大体上分为：<br>（1）针对数据布局低效，采用小文件合并策略，将小文件合并为大文件；<br>（2）针对元数据管理低效，优化元数据的存储和管理。   </p><h5 id="小文件合并"><a href="#小文件合并" class="headerlink" title="小文件合并"></a>小文件合并</h5><p>小文件合并为大文件后，首先减少了大量元数据，提高了元数据的检索和查询效率，降低了文件读写的I/O操作延时。其次将可能连续访问的小文件一同合并存储，增加了文件之间的局部性，将原本小文件间的随机访问变为了顺序访问，大大提高了性能。同时，合并存储能够有效的减少小文件存储时所产生的磁盘碎片问题，提高了磁盘的利用率。最后，合并之后小文件的访问流程也有了很大的变化，由原来许多的open操作转变为了seek操作，定位到大文件具体的位置即可。如何寻址这个大文件中的小文件呢？其实就是利用一个旁路数据库来记录每个小文件在这个大文件中的偏移量和长度等信息。其实小文件合并的策略本质上就是通过分层的思想来存储元数据。中控节点存储一级元数据，也就是大文件与底层块的对应关系；数据节点存放二级元数据，也就是最终的用户文件在这些一级大块中的存储位置对应关系，经过两级寻址来读写数据。<br>TFS是采用小文件合并存储策略的例子。TFS默认Block大小为64M，每个块中会存储许多不同的小文件，但是这个块只占用一个Inode。假设一个Block为64M，数量级为1PB。那么NameServer上会有16.7M个Block。假设每个Block的元数据大小为0.1K，则占用内存不到2G。在TFS中，文件名中包含了Block ID和File ID，通过Block ID定位到具体的DataServer上，然后DataServer会根据本地记录的信息来得到File ID所在Block的偏移量，从而读取到正确的文件内容。    </p><h5 id="元数据管理优化"><a href="#元数据管理优化" class="headerlink" title="元数据管理优化"></a>元数据管理优化</h5><p>一般来说元数据信息包括名称、文件大小、设备标识符、用户标识符、用户组标识符等等，在小文件系统中可以对元数据信息进行精简，仅保存足够的信息即可。元数据精简可以减少元数据通信延时，同时相同容量的Cache能存储更多的元数据，从而提高元数据使用效率。另外可以在文件名中就包含元数据信息，从而减少一个元数据的查询操作。最后针对特别小的一些文件，可以采取元数据和数据并存的策略，将数据直接存储在元数据之中，通过减少一次寻址操作从而大大提高性能。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;块存储，简单来说就是使用块设备为系统提供存储服务。块存储分多钟类型，有单机块存储（磁盘）、网络存储（NAS、SAN等），分布式存储（云硬盘）。通常块存储的表现形式就是一块设备，用户看到的就是类似于sda、sdb这样的逻辑设备。&lt;br&gt;
    
    </summary>
    
    
      <category term="network/linux" scheme="http://blog.jonnydu.me/tags/network-linux/"/>
    
  </entry>
  
  <entry>
    <title>NVMe驱动之I/O请求</title>
    <link href="http://blog.jonnydu.me/2017/04/27/NVMe-IO-Request/"/>
    <id>http://blog.jonnydu.me/2017/04/27/NVMe-IO-Request/</id>
    <published>2017-04-27T12:31:41.000Z</published>
    <updated>2017-05-23T14:19:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>NVMe SSD具有极高的I/O读写性能，不存在传统磁盘所具有的访问寻道、抖动问题。为了发挥NVMe SSD的性能，无论在软件还是在硬件上都需要采用多队列技术，通过多队列方式充分发挥NVMe SSD的性能。<br><a id="more"></a><br>Linux的NVMe驱动采用一个Core独占一个Queue（包含一个Completion Queue和至少一个Submission Queue）的方式来充分发挥NVMe SSD的性能，这样可以避免一个队列被多个Core竞争访问（引起的锁竞争等问题），各CPU使用自己的Queue，互不影响。 </p><p><img src="http://i.imgur.com/sIdEXDh.gif" alt="">   </p><p>NVMe响应I/O的过程已经在<a href="http://blog.dujiong.net/2017/02/17/NVMe/" target="_blank" rel="noopener">NVMe技术浅析</a>一文中详细描述，这里将结合部分源码做进一步说明。    </p><h3 id="创建NVMe-Queue"><a href="#创建NVMe-Queue" class="headerlink" title="创建NVMe Queue"></a>创建NVMe Queue</h3><pre><code>/* * pci.c */static int nvme_setup_io_queues(struct nvme_dev* dev){    struct nvme_queue* adminq = dev-&gt;queues[0];    struct pci_dev* pdev = to_pci_dev(dev-&gt;dev);    int result, nr_io_queues, size;    nr_io_queues = num_online_cpus();    result = nvme_set_queue_count(&amp;dev-&gt;ctrl, &amp;nr_io_queues);    ...    result = queue_request_irq(adminq);    if(result){        admin-&gt;cq_vector = -1;        return result;    }    return nvme_create_io_queues(dev);}</code></pre><p>驱动程序在创建队列之前首先通过内核函数num_online_cpus获取当前系统中运行的CPU数量m，然后通过nvme_set_queue_count得到SSD的队列数量限制n，选择二者较小值作为所创建的队列数。<br>此外，关于Queue还有一个重要的概念，那就是队列深度，简单来说就是这个Queue能够放多少个成员（比如NVMe Command）。在NVMe中，这个队列深度是由NVMe SSD决定的，存储在NVMe设备的BAR空间里。另外，NVMe驱动没有区分Namespace，也就是一个设备上的多个Namespace共享这些队列资源。<br>因此，它们之间的关系是：队列用来存放NVMe Command，NVMe Command是Host与SSD Controller交流的基本单元，应用的I/O请求也要转化成NVMe Command。</p><h3 id="提交I-O请求"><a href="#提交I-O请求" class="headerlink" title="提交I/O请求"></a>提交I/O请求</h3><p>Block层下发的IO请求以BIO表示，我们需要通过DMA发送这些数据，Command使用dma_alloc_coherent分配DMA地址，但是BIO是存放在普通的内核线程空间的（线程的虚拟空间不能直接作为DMA地址），使用dma_alloc_coherent分配再将BIO中的数据拷贝显然不是有效的方法。这里linux提供了另一个函数dma_map_single，这个函数能够将虚拟空间地址（BIO数据存放地址）转换成DMA可用地址，并且多个IO请求的DMA地址可以通过scatterlist来表示。有了DMA地址就可以把BIO封装成NVMe Command发送出去。从BIO到NVMe Command有可能会经过拆分，放入等待队列等。我们知道，驱动会给每个CPU分配一个Queue，那么I/O请求到来时，该由哪个Queue来存放这个Command呢？你可能已经想到，应该由当前线程运行的CPU所属的Queue，这样才能保证Queue不被其他Core抢占，驱动使用get_cpu()获得当前处理I/O请求的CPU号来索引对应的Queue。 </p><pre><code>/* * pci.c */static void __nvme_submit_cmd(struct nvme_queue* nvmeq, struct nvme_command* cmd){    u16 tail = nvmeq-&gt;sq_tail;    if(nvmeq-&gt;sq_cmds_io){        memcpy_toio(&amp;nvmeq-&gt;sq_cmds_io[tail], cmd, sizeof(*cmd));    }else{        memcpy(&amp;nvmeq-&gt;sq_cmds[tail], cmd, sizeof(*cmd));    }    if(++tail == nvmeq-&gt;q_depth)        tail = 0;    writel(tail, nvmeq-&gt;q_db);    nvmeq-&gt;sq_tail = tail;}</code></pre><p>BIO封装成的Command会顺序存入Submission Queue中。对于Submission Queue来说，使用Tail表示最后操作的Command Index（nvmeq-&gt;sq_tail）。每存入一个Command，Host就会更新Queue对应的Doorbell寄存器中的Tail值。Doorbell定义在BAR空间，通过QID可以索引到。NVMe没有规定Command存入队列的执行顺序，Controller可以一次取出多个Command进行批量处理，所以一个队列中的Command执行顺序是不固定的（可能导致先提交的请求后处理）。</p><h3 id="获得处理结果"><a href="#获得处理结果" class="headerlink" title="获得处理结果"></a>获得处理结果</h3><p>SSD Controller根据Doorbell的值，获取NVMe Command和对应数据，待处理完成后将结果存入Completion Queue中。Controller通过中断的方式通知Host，驱动为每一个Queue分配一个MSI/MSI-X中断。<br>驱动使用request_irq将中断注册到kernel，并且绑定中断处理函数nvme_irq。此中断处理函数调用__nvme_process_cq，先将Completion Command从Completion Queue中取出，然后把队列的head增加1，并调用上层的callback函数，完成IO处理。由于NVMe Command可以批量处理，这里使用while循环取出所有新的Completion Command。     </p><pre><code>/* * pci.c */static void __nvme_process_cq(struct nvme_queue *nvmeq, unsigned int *tag){    u16 head, phase;    head = nvmeq-&gt;cq_head;    phase = nvmeq-&gt;cq_phase;    while(nvme_cqe_valid(nvmeq, head, phase)){        struct nvme_completion cqe = nvmeq-&gt;cqes[head];        struct request *req;        if(++head == nvmeq-&gt;q_depth){            head = 0;            phase = !phase;        }        ...        if (unlikely(nvmeq-&gt;qid == 0 &amp;&amp;            cqe.command_id &gt;= NVME_AQ_BLKMQ_DEPTH)) {                nvme_complete_async_event(&amp;nvmeq-&gt;dev-&gt;ctrl,                                        cqe.status, &amp;cqe.result);                continue;        }        req = blk_mq_tag_to_rq(*nvmeq-&gt;tags, cqe.command_id);        nvme_req(req)-&gt;result = cqe.result;        blk_mq_complete_request(req, le16_to_cpu(cqe.status) &gt;&gt; 1);        }    if (head == nvmeq-&gt;cq_head &amp;&amp; phase == nvmeq-&gt;cq_phase)        return;    if (likely(nvmeq-&gt;cq_vector &gt;= 0))        writel(head, nvmeq-&gt;q_db + nvmeq-&gt;dev-&gt;db_stride);    nvmeq-&gt;cq_head = head;    nvmeq-&gt;cq_phase = phase;    nvmeq-&gt;cqe_seen = 1;}</code></pre><p>处理完Command后，往Completion Queue的Doorbell写入Head值，通知NVMe Controller操作完成。中断处理结束。</p><h3 id="Head-Tail机制"><a href="#Head-Tail机制" class="headerlink" title="Head/Tail机制"></a>Head/Tail机制</h3><p>我们知道，SQ和CQ都是队列，队列的头尾很重要，头决定谁会最先被服务，尾决定新到来命令的位置，所以，我们需要记录SQ和CQ的头尾位置。  </p><p><img src="http://i.imgur.com/bsa3KFs.png" alt=""></p><p>对一个SQ来说，它的生产者是Host，因为它往SQ的Tail位置写入命令；消费者是SSD，它从SQ的Head取出指令执行。CQ则刚刚相反，生产者是SSD，消费者是Host。     </p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文简单介绍了NVMe I/O请求的处理过程，其核心是多队列，Submission Queue/Completion Queue、Doorbell寄存器和Head/Tail机制。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NVMe SSD具有极高的I/O读写性能，不存在传统磁盘所具有的访问寻道、抖动问题。为了发挥NVMe SSD的性能，无论在软件还是在硬件上都需要采用多队列技术，通过多队列方式充分发挥NVMe SSD的性能。&lt;br&gt;
    
    </summary>
    
    
      <category term="NVMe over RDMA" scheme="http://blog.jonnydu.me/tags/NVMe-over-RDMA/"/>
    
  </entry>
  
  <entry>
    <title>理解TCP/IP网络栈</title>
    <link href="http://blog.jonnydu.me/2017/04/21/trace-linux-network-stack/"/>
    <id>http://blog.jonnydu.me/2017/04/21/trace-linux-network-stack/</id>
    <published>2017-04-21T08:45:24.000Z</published>
    <updated>2017-05-17T13:41:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>看到一篇讲解TCP/IP协议栈原理及其处理数据包流程很好的文章，特记录下来。<br><a id="more"></a><br>原文地址：<a href="http://yuedu.163.com/news_reader/#/~/source?id=50668df1-784d-42b8-8893-ab4063734506_1&amp;cid=47928decaeae4753923ba2fc552f0669_1" target="_blank" rel="noopener">理解TCP/IP网络栈&amp;编写网络应用</a>     </p><h3 id="数据传输"><a href="#数据传输" class="headerlink" title="数据传输"></a>数据传输</h3><p>网络栈有很多层。图1表示了这些层的类型：</p><p><img src="http://i.imgur.com/KrEHsC4.png" alt=""><br>图1 发送数据时网络栈中各层对数据的操作</p><p>这些层可以被大致归类到三个区域中：（1）用户区；（2）内核区；（3）设备区。用户区和内核区的任务由CPU执行。用户区和内核区被叫做“主机（host）”以区别于设备区。在这里，设备是发送和接收数据包的网络接口卡（Network Interface Card,NIC）。它有一个更常用的术语：网卡。<br>我们来了解一下用户区。首先，应用程序创建要发送的数据（图1中的“User Data”）并且调用write()系统调用来发送数据（在这里假设socket已经被创建了，对应图1中的“fd”）。当系统调用被调用之后上下文切换到内核区。<br>像Linux或者Unix这类POSIX系列的操作系统通过文件描述符（file descriptor）把socket暴露给应用程序。在这类系统中，socket是文件的一种。文件系统执行简单的检查并调用socket结构中指向的socket函数。<br>内核中的socket包含两个缓冲区：一个用于缓冲要发送的数据；一个用于缓冲要接收的数据。<br>当write()系统调用被调用时，用户区的数据被拷贝到内核内存中，并插入到socket的发送缓冲区，这样来保证发送的数据有序。之后，TCP被调用了。<br>socket会关联一个叫做TCP控制块（TCP Control Block，TCB）的结构，TCB包含了处理TCP连接所需的数据，包括连接状态，接收窗口，阻塞窗口，序列号等。<br>如果当前的TCP状态允许数据传输，一个新的TCP分段（TCP segment）将被创建，当然，如果由于流量控制或其他原因不能传输数据，系统调用将会结束，之后返回用户态，即将控制权交回到应用程序代码。<br>TCP分段有两部分：TCP头和携带的数据。     </p><p><img src="http://i.imgur.com/nGYgq7F.png" alt=""><br>图2 TCP帧的结构    </p><p>TCP数据包的payload部分会包含在socket发送缓冲区里的没有应答的数据。携带数据的最大长度是接收窗口、阻塞窗口和最大分段长度（MSS）中的最大值。之后会计算TCP校验值。在计算时，头信息（ip地址、分段长度和端口号）会包含在内。根据TCP状态可发送一个或多个数据包。事实上，当前的网络栈使用了校验卸载（checksum offload），TCP校验和会由NIC计算，而不是内核。但是，为了解释方便我们还是假设内核计算校验和。<br>被创建的TCP分段继续走到下面的IP层。IP层向TCP分段中增加了IP头并且执行了IP路由。IP路由是寻找到达目的IP的下一条IP地址的过程。<br>在IP层计算并增加了IP头校验和之后，它把数据发送到链路层。链路层通过地址解析协议（Address Resolution Protocol，ARP）搜索下一跳IP地址对应的MAC地址。之后它会向数据包中增加链路头，在增加链路头之后主机要发送的数据包就是完整的了。<br>在执行IP路由时，会选择一个传输接口（NIC）。接口被用于把数据包传送至下一跳IP。于是，用于发送的NIC驱动程序被调用了。<br>在这个时候，如果正在执行数据包捕获程序（例如tcpdump或wireshark）的话，内核将把数据包拷贝到这些程序的内存缓冲区中。相同的方式，接收的数据包直接在驱动被捕获。<br>在接收到数据包传输请求之后，NIC把数据包从系统内存中拷贝到它自己的内存中，之后把数据包发送到网络上。在此时，由于要遵守以太网标准（Ethernet standard），NIC会向数据包中增加帧间隙（Inter-Frame Gap，IFG），同步码（preamble）和crc校验和。帧间隙和同步码用于区分数据包的开始（网络术语叫做帧，framing），crc用于保护数据（与TCP或者IP校验和的目的相同）。NIC会基于以太网的物理速度和流量控制决定数据包开始传输的时间。<br>当NIC发送了数据包，NIC会在主机的CPU上产生中断。所有的中断会有自己的中断号，操作系统会用这个中断号查找合适的程序去处理中断。驱动程序在启动时会注册一个处理中断的函数。操作系统调用中断处理程序，之后中断处理程序会把已发送的数据包返回给操作系统。   </p><h3 id="数据接收"><a href="#数据接收" class="headerlink" title="数据接收"></a>数据接收</h3><p>现在我们看一下数据是如何被接收的。数据接收是网络栈如何处理流入数据包的过程。     </p><p><img src="http://i.imgur.com/969v08v.png" alt=""><br>图3 接收数据时网络栈中各层对数据的操作     </p><p>首先，NIC把数据包写入它自身的内存。通过CRC校验检查数据包是否有效，之后把数据包发送到主机的内存缓冲区。这里说的缓冲区是驱动程序提前向内核申请好的一块内存区域，用于存放接收的数据包。在缓冲区被系统分配之后，驱动会把这部分内存的地址和大小告知NIC。如果主机没有为驱动程序分配缓冲区，那么当NIC接收到数据包时有可能会直接丢弃它。<br>在把数据包发送到主机缓冲区之后，NIC会向主机发出中断。之后，驱动程序会判断它是否能处理新的数据包。到目前为止使用的是由制造商定义的网卡驱动的通讯协议。<br>当驱动应该向上层发送数据包时，数据包必须被放进一个操作系统能够理解和使用的数结构。例如Linux的sk_buff，或者BSD系列内核的mbuf，或者windows的NET_BUF_LIST，驱动会把数据包包装在指定的数据结构，并发送到上一层。<br>链路层会检查数据包是否有效并且解析出上层的协议（网络协议）。此时它会判断链路头中的以太网类型。（IPv4的以太网类型是0×0800）。它会把链路头删掉并且把数据包发送到IP层。<br>IP层同样会检查数据包是否有效。或者说，会检查IP头校验和。它会执行IP路由判断，判断是由本机处理数据包还是把数据包发送到其它系统。如果数据包必须由本地系统处理，IP层会通过IP header中引用的原型值（proto value）解析上层协议（传输协议）。TCP原型值为6。系统会删除IP头，并且把数据包发送到TCP层。<br>就像之前的几层，TCP层检查数据包是否有效，同时会检查TCP校验和。就像之前提到的，如果当前的网络栈使用了校验卸载，那么TCP校验和会由NIC计算，而不是内核。<br>之后它会查找数据包对应的TCP控制块（TCB），这时会使用数据包中的&lt;源ip，源端口，目的IP，目的端口&gt;做标识。在查询到对应的连接之后，会执行协议中定义的操作去处理数据包。如果接收到的是新数据，数据会被增加到socket的接收缓冲区。根据tcp连接的状态，此时也可以发送新的TCP包（比如发送ACK包）。此时，TCP/IP接收数据包的处理完成。<br>socket接收缓冲区的大小就是TCP接收窗口。TCP吞吐量会随着接收窗口变大而增加。过去socket缓冲区大小是应用或操作系统的配置的定值。最新的网络栈使用了一个函数去自动决定接收缓冲区的大小。<br>当应用程序调用read系统调用时，程序会切换到内核区，并且会把socket接收缓冲区中的数据拷贝到用户区。拷贝后的数据会从socket缓冲区中移除。之后TCP会被调用，TCP会增加接收窗口的大小（因为缓冲区有了新空间）。并且会根据协议状态发送数据包。如果没有数据包传送，系统调用结束。</p><h3 id="如何处理中断和接收数据包"><a href="#如何处理中断和接收数据包" class="headerlink" title="如何处理中断和接收数据包"></a>如何处理中断和接收数据包</h3><p>这一部分内容已经在<a href="http://blog.dujiong.net/2017/04/13/trace-linux-networking-stack-receiving-data/" target="_blank" rel="noopener">Linux网络协议栈数据处理流程</a>一文中详细叙述过。<br>中断处理过程是复杂的，但是你需要了解数据包接收和处理流程中的和性能相关的问题。图4展示了一次中断的处理流程。  </p><p><img src="http://i.imgur.com/dmtkwf5.png" alt=""><br>图4 处理中断、软中断和接收数据    </p><p>假设CPU0正在执行应用程序，在此时，NIC接收到了一个数据包并且在CPU0上产生了中断。CPU0执行了内核中断处理程序（irq）。这个处理程序关联了一个中断号并且内核会调用驱动里对应的中断处理程序。驱动在释放已经传输的数据包之后调用napi_schedule()函数去处理接收到的数据包，这个函数会请求软中断。在执行了驱动的中断处理程序后，控制权被转移到内核中断处理程序。内核中的处理程序会执行软中断的处理程序。<br>在中断上下文被执行之后，软中断的上下文会被执行。中断上下文和软中断上下文会通过相同的线程执行，但是它们会使用不同的栈。并且中断上下文会屏蔽硬件中断；而软中断上下文不会屏蔽硬件中断。<br>处理接收到的数据包的软中断处理程序是net_rx_action()函数。该函数会调用驱动程序的poll()函数。而poll()函数会调用netif_receive_skb()函数，并把接收到的数据包一个接一个的发送到上层。在软中断处理后，应用程序会从停止的位置重新开始执行。<br>因此，接收中断请求的CPU会负责处理接收数据包从始至终的整个过程。在Linux、BSD和Windows中，处理过程基本是类似的。    </p><h3 id="相关数据结构"><a href="#相关数据结构" class="headerlink" title="相关数据结构"></a>相关数据结构</h3><h4 id="sk-buff"><a href="#sk-buff" class="headerlink" title="sk_buff"></a>sk_buff</h4><p>首先，sk_buff结构或skb结构代表一个数据包，图5展现了sk_buff中的一些结构。随着功能变得更强大，它们也变得更复杂了。     </p><p><img src="http://i.imgur.com/r1C515B.png" alt=""><br>图5 数据包结构    </p><p>这个结构直接包含或者通过指针引用了数据包。一些必要的信息比如头和内容长度被保存在元数据区。例如，在图5中，mac_header、network_header和transport_header都有相应的指针，指向链路头、IP头和TCP头的起始地址。这种方式让TCP协议处理过程变得简单。<br>数据包在网络栈的各层中上升或下降时会增加或删除数据头。为了更有效率的处理而使用了指针。例如，要删除链路头只需要修改head pointer的值。     </p><h4 id="TCP控制块"><a href="#TCP控制块" class="headerlink" title="TCP控制块"></a>TCP控制块</h4><p>其次，有一个表示TCP连接的数据结构，之前它被抽象的叫做TCP控制块。Linux使用了tcp_sock这个数据结构。在图6中，你可以看到文件、socket和tcp_socket的关系。    </p><p><img src="http://i.imgur.com/BJI48GO.png" alt=""><br>图6 TCP Connection结构</p><p>当系统调用发生后，它会找到应用程序在进行系统调用时使用的文件描述符对应的文件。对Unix系的操作系统来说，文件本身和通用文件系统存储的设备都被抽象成了文件。因此，文件结构包含了必要的信息。对于socket来说，使用独立的socket结构保存socket相关的信息，文件系统通过指针来引用socket。socket又引用了tcp_sock。tcp_sock可以分为sock,inet_sock等等,用来支持除了TCP之外的协议，可以认为这是一种多态。<br>所有TCP协议用到的状态信息都被存在tcp_sock里。例如顺序号、接收窗口、阻塞控制和重发送定时器都保存在tcp_sock中。<br>socket的发送缓冲区和接收缓冲区由sk_buff链表组成并被包含在tcp_sock中。为防止频繁查找路由，也会在tcp_sock中引用IP路由结果dst_entry。通过dst_entry可以简单的查找到目标的MAC地址之类的ARP的结果。dst_entry是路由表的一部分，而路由表是个很复杂的结构，在这篇文档里就不再讨论了。用来传送数据的网卡(NIC)可以通过dst_entry找到。网卡通过net_device描述。<br>因此，仅通过查找文件和指针就可以很简单的查找到处理TCP连接的所有的数据结构（从文件到驱动）。这个数据结构的大小就是每个TCP连接占用内存的大小。这个结构占用的内存只有几kb大小（排除了数据包中的数据）。但随着一些功能被加入，内存占用也在逐渐增加。<br>最后，我们来看一下TCP连接查找表（TCP connection lookup table）。这是一个用来查找接收到的数据包对应tcp连接的哈希表。系统会用数据包的&lt;来源ip，目标ip，来源端口，目标端口&gt;和Jenkins哈希算法去计算哈希值。选择这个哈希函数的原因是为了防止对哈希表的攻击。</p><h3 id="追踪代码：传输数据"><a href="#追踪代码：传输数据" class="headerlink" title="追踪代码：传输数据"></a>追踪代码：传输数据</h3><p>我们将会通过追踪实际的Linux内核源码去检查协议栈中执行的关键任务。这里以发送数据的执行路径为例。<br>首先是应用程序调用write系统调用。当应用调用了write系统调用时，内核将在文件层执行write()函数。首先，内核会取出文件描述符对应的文件结构体，之后会调用aio_write，这是一个函数指针。在文件结构体中，你可以看到file_perations结构体指针。这个结构被通称为函数表（function table），其中包含了一些函数的指针，比如aio_read或者aio_write。对于socket来说，实际的表是socket_file_ops，aio_write对应的函数是sock_aio_write。在这里函数表的作用类似于java中的interface，内核使用这种机制进行代码抽象或重构。</p><pre><code>static ssize_t sock_aio_write(struct kiocb *iocb, const struct iovec *iov, ..) {    /* ... */    struct socket *sock = file-&gt;private_data;    /* ... */    return sock-&gt;ops-&gt;sendmsg(iocb, sock, msg, size);    struct socket {        /* ... */        struct file *file;        struct sock *sk;        const struct proto_ops *ops;    };        struct proto_ops {    `    /* ... */        int (*connect) (struct socket *sock, ...)        int (*accept) (struct socket *sock, ...)        int (*listen) (struct socket *sock, int len);        int (*sendmsg) (struct kiocb *iocb, struct socket *sock, ...)        int (*recvmsg) (struct kiocb *iocb, struct socket *sock, ...)        /* ... */    };};</code></pre><p>sock_aio_write()函数会从文件结构体中取出socket结构体并调用sendmsg，这也是一个函数指针。socket结构体中包含了proto_ops函数表。IPv4的TCP实现中，proto_ops的具体实现是inet_stream_ops，sendmsg的实现是tcp_sendmsg。<br>tcp_sengmsg会从socket中取得tcp_sock（也就是TCP控制块，TCB），并把应用程序请求发送的数据拷贝到socket发送缓冲中(根据发送数据创建sk_buff链表),当把数据拷贝到sk_buff中时，每个sk_buff会包含多少字节数据？在代码创建数据包时，每个sk_buff中会包含MSS字节(通过tcp_send_mss函数获取)，在这里MSS表示每个TCP数据包能携带数据的最大值。通过使用TSO(TCP Segment Offload)和GSO(Generic Segmentation Offload)技术，一个sk_buff可以保存大于MSS的数据。在这篇文章里就不详细解释了。<br>sk_stream_alloc_skb函数会创建新的sk_buff，之后通过skb_entail把新创建的sk_buff放到send_socket_buffer的末尾。skb_add_data函数会把应用层的数据拷贝到sk_buff的buffer中。通过重复这个过程（创建sk_buff然后把它加入到socket发送缓冲区）完成所有数据的拷贝。因此，大小是MSS的多个sk_buff会在socket发送缓冲区中形成一个链表。最终调用tcp_push把待发送的数据做成数据包，并且发送出去。<br>tcp_push函数会在TCP允许的范围内顺序发送尽可能多的sk_buff数据。首先会调用tcp_send_head取得发送缓冲区中第一个sk_buff，然后调用tcp<br>_cwnd_test和tcp_send_wnd_test检查堵塞窗口和接收窗口，判断接收方是否可以接收新数据。之后调用tcp_transmit_skb函数来创建数据包。</p><pre><code>static int tcp_transmit_skb(struct sock *sk, struct sk_buff *skb,int clone_it, gfp_t gfp_mask) {    const struct inet_connection_sock *icsk = inet_csk(sk);    struct inet_sock *inet;    struct tcp_sock *tp;    /* ... */    if (likely(clone_it)) {        if(unlikely(skb_cloned(skb)))            skb = pskb_copy(skb, gfp_mask);        else            skb = skb_clone(skb, gfp_mask);        if (unlikely(!skb))            return -ENOBUFS;    }    skb_push(skb, tcp_header_size);    skb_reset_transport_header(skb);    skb_set_owner_w(skb, sk);    /* Build TCP header and checksum it */    th = tcp_hdr(skb);    th-&gt;source = inet-&gt;inet_sport;    th-&gt;dest = inet-&gt;inet_dport;    th-&gt;seq = htonl(tcb-&gt;seq);    th-&gt;ack_seq = htonl(tp-&gt;rcv_nxt);    /* ... */    return net_xmit_eval(err);}</code></pre><p>tcp_transmit<br>_skb会创建指定sk_buff的拷贝（通过pskb_copy)，但它不会拷贝应用层发送的数据，而是拷贝一些元数据。之后会调用skb_push来确保和记录头部字段的值。send_check计算TCP校验和（如果使用校验和卸载checksum offload技术则不会做这一步计算）。最终调用queue_xmit把数据发送到IP层。IPv4中queue_xmit的实现函数是ip_queue_xmit。<br>ip_queue_xmit函数执行IP层的一些必要的任务。__sk_dst_check检查缓存的路由是否有效。如果没有被缓存的路由项，或者路由无效，它将会执行IP路由选择（IP routing)。之后调用skb_push来计算和记录IP头字段的值。之后，随着函数执行，ip_send_check计算IP头校验和并且调用netfilter功能。如果使用ip_finish_output函数会创建IP数据分片，但在使用TCP协议时不会创建分片，因此内核会直接调用ip_finish_output2来增加链路头，并完成数据包的创建。<br>最终的数据包会通过dev_queue_xmit函数完成传输。首先，数据包通过排队规则传递。如果使用了默认的排队规则并且队列是空的，那么会跳过队列而直接调用sch_direct_xmit把数据包发送到驱动。dev_hard_start_xmit会调用实际的驱动程序。在调用驱动之前，设备的发送被锁定，防止多个线程同时使用设备。由于内核锁定了设备的发送，驱动发送数据相关的代码就不需要额外的锁了。<br>ndo_start_xmit函数会调用驱动的代码。在这之前，你会看到ptype_all和dev_queue_\xmit_nit。ptype_all是个包含了一些模块的列表（比如数据包捕获）。如果捕获程序正在运行，数据包会被ptype_all拷贝到其它程序中。因此，tcpdump中显示的都是发送给驱动的数据包。当使用了校验和卸载(checksum offload)或TSO(TCP Segment Offload)这些技术时，网卡（NIC）会操作数据包，所以tcpdump得到的数据包和实际发送到网络的数据包有可能不一致。在结束了数据包传输以后，驱动中断处理程序会返回发送了的sk_buff。      </p><h3 id="驱动和网卡如何通信"><a href="#驱动和网卡如何通信" class="headerlink" title="驱动和网卡如何通信"></a>驱动和网卡如何通信</h3><p>驱动（driver）和网卡（NIC）之间的通讯处于协议栈的底层，大多数人并不关心。但是，为了解决性能问题，网卡会处理越来越多的任务。理解基础的处理方式会帮助你理解额外这些优化技术。<br>网卡和驱动之间使用异步通讯。首先，驱动请求数据传输时CPU不会等待结果而是会继续处理其它任务，之后网卡发送数据包并通知CPU，驱动程序返回通过事件接收的数据包（这些数据包可以看作是异步发送的返回值）。<br>和数据包传输类似，数据包的接收也是异步的。首先驱动请求接受接收数据包然后CPU去执行其它任务，之后网卡接收数据包并通知CPU，然后驱动处理接收到的数据包（返回数据）。<br>因此需要有一个空间来保存请求和响应（request and response）。大多数情况网卡会使用环形队列数据结构（ring structure）。环形队列类似于普通的队列，其中有固定数量的元素，每个元素会保存一个请求或一个相应数据。环形队列中元素是顺序的，“环形”的意思是队列虽然是定长的，但是其中的元素会按顺序重用。<br>图7展示了数据包的传输过程，会看到其中如何使用环形队列。    </p><p><img src="http://i.imgur.com/kVn0OjV.png" alt=""><br>图7 驱动和网卡之间的通讯：传输数据包  </p><p>驱动接收到上层发来的数据包并创建网卡能够识别的描述符。发送描述符（send descriptor）默认会包含数据包的大小和内存地址。这里网卡需要的是内存中的物理地址，驱动需要把数据包的虚拟地址转换成物理地址。之后，驱动会把发送描述符添加到发送环形队列（TX ring）（1），发送环形队列中包含的实际是发送描述符。<br>之后，驱动会把请求通知给网卡（2）。驱动直接把数据（通知）写入指定的网卡的寄存器地址中。<br>网卡被通知后从主机的发送队列中取得发送描述符（3）。这种设备直接访问内存而不需要调用CPU的内存访问方式叫做直接内存访问（Direct Memory Access，DMA）。<br>在取得发送描述符后，网卡会得到数据包的地址和大小并且从主机内存中取得实际的数据包（4）。如果有校验和卸载（checksum offload）的话，网卡会在拿到数据后计算数据包的校验和，因此开销不大。<br>网卡发送数据包（5）之后把发送数据包的数量写入主机内存（6）。之后它会触发一次中断，驱动程序会读取发送数据包的数量并根据数量返回已发送的数据包。     </p><p>图8展示了接收数据包的过程。</p><p><img src="http://i.imgur.com/zKf9nxU.png" alt=""><br>图8 驱动程序和网卡之间的通讯：接收数据包    </p><p>调优网络栈时，大部分人会说环形队列和中断的设置需要被调整。当发送环形队列很大时，很多次的发送请求可以一次完成；当接收环形队列很大，可以一次性接收多个数据包。更大的环形队列对于大流量数据包接收/发送是很有用的。由于CPU在处理中断时有大量开销，大量大多数情况下，网卡使用一个计时器来减少中断。为了避免对宿主机过多的中断，发送和接收数据包的时候中断会被收集起来并且定期调用（interrupt coalescing，中断聚合）。</p><h3 id="网络栈中的缓冲区和流量控制"><a href="#网络栈中的缓冲区和流量控制" class="headerlink" title="网络栈中的缓冲区和流量控制"></a>网络栈中的缓冲区和流量控制</h3><p>在网络栈中流量控制在几个阶段被执行。图9展示了传输数据时的一些缓冲区。首先，应用会创建数据并把数据加入到socket发送缓冲区。如果缓冲区中没有剩余空间的话，系统调用会失败或阻塞应用进程。因此，应用程序到内核的发送速率由socket缓冲区大小来限制。</p><p><img src="http://i.imgur.com/qu43rVe.png" alt=""><br>图9 数据发送相关的缓冲</p><p>TCP通过传输队列（qdisc）创建并把数据包发送给驱动程序。这是一个典型的先入先出队列，队列最大长度是txqueuelen，可以通过ifconfig命令来查看实际大小。通常来说，大约有几千个数据包。<br>驱动和网卡之间是传输环形队列（TX ring），它被认为是传输请求队列（transmission request queue）。如果队列中没有剩余空间的话就不会再继续创建传输请求，并且数据包会积累在传输队列中，如果数据包积累的太多，那么新的数据包会被丢弃。<br>网卡会把要发送的数据包保存在内部缓冲区中。这个队列中的数据包速度受网卡物理速度的影响（例如，1Gb/s的网卡不能承担10Gb/s的性能）。根据以太网流量控制，当网卡的接收缓冲区没有空间时，数据包传输会被停止。<br>当内核速度大于网卡时，数据包会堆积在网卡的缓冲区中。如果缓冲区中没有空间时会停止处理传输环形队列（TX ring）。越来越多的请求堆积在传输环形队列中，最终队列中空间被耗尽。驱动程序不能再继续创建传输请求数据包会堆积在传输队列（transmit queue）中。压力通过各种缓冲从底向上逐级反馈。<br>图10展示了接收数据包经过的缓冲区。数据包先被保存在网卡的接收缓冲区中。从流量控制的视角来看，驱动和网卡之间的接收环形缓冲区(RX ring)可以被看作是数据包的缓冲区。驱动程序从环形缓冲区取得数据包并把它们发送到上层。服务器系统的网卡驱动默认会使用NAPI，所以在驱动和上层之间没有缓冲区。因此，可以认为上层直接从接收环形缓冲区中取得数据，数据包的数据部分被保存在socket的接收缓冲区中。应用程序从socket接收缓冲区取得数据。 </p><p><img src="http://i.imgur.com/CB8RYpt.png" alt=""><br>图10 与接收数据包相关的缓冲   </p><p>不支持NAPI的驱动程序会把数据包保存在积压队列（backlog queue）中。之后，NAPI处理程序取得数据包，因此积压队列可以被认为是在驱动程序和上层之间的缓冲区。<br>如果内核处理数据包的速度低于网卡的速度，接收循环缓冲区队列(RX ring)会被写满，网卡的缓冲区空间(NIC internal buffer)也会被写满，当使用了以太流量控制（Ethernet flow control）时，接收方网卡会向发送方网卡发送请求来停止传输或丢弃数据包。<br>因为TCP支持端对端流量控制，所以不会出现由于socket接收队列空间不足而丢包的情况。但是，当使用UDP协议时，因为UDP协议不支持流量控制，如果应用程序处理速度不够的时候会出现socket接收缓冲区空间不足而丢包的情况。<br>在图9和图10中展示的传输环形队列（TX ring）和接收环形队列（RX ring）的大小可以用ethtool查看。在大多数看重吞吐量的负载情况下，增加环形队列的大小和socket缓冲区大小会有一些帮助。增加大小会减少高速收发数据包时由于缓冲区空间不足而造成的异常。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>原文写的很好，共勉！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;看到一篇讲解TCP/IP协议栈原理及其处理数据包流程很好的文章，特记录下来。&lt;br&gt;
    
    </summary>
    
    
      <category term="network/linux" scheme="http://blog.jonnydu.me/tags/network-linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux网络协议栈数据处理流程---收包</title>
    <link href="http://blog.jonnydu.me/2017/04/13/trace-linux-networking-stack-receiving-data/"/>
    <id>http://blog.jonnydu.me/2017/04/13/trace-linux-networking-stack-receiving-data/</id>
    <published>2017-04-13T06:15:59.000Z</published>
    <updated>2017-05-16T11:45:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文详细记录了从网卡驱动加载和初始化开始，一直到网络数据包到达，最后驱动程序将其递交到网络协议栈的整个处理过程。<br><a id="more"></a><br>下文中出现的代码均基于Linux内核3.13.0版本，网卡驱动igb。</p><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>从整体上看，Linux网络协议栈对数据包的处理流程如下：    </p><ol><li>网卡驱动的加载和初始化   </li><li>数据包到达网卡   </li><li>网卡将数据包通过DMA的方式写入到指定的内存地址，该地址由网卡驱动分配并初始化    </li><li>网卡通过硬件中断的方式通知CPU有数据到来        </li><li>在中断处理程序中，驱动程序调用NAPI进入软中断处理                        </li><li>软中断进程ksoftirqd通过调用NAPI函数poll循环接收数据包，软中断进程运行在系统的每个CPU上。        </li><li>数据被传递到skb buffer中，随后递交网络协议栈       </li><li>如果RPS（Receive Packet Steering）被打开或者NIC有多个接收队列，接收到的数据包将分散在多个CPU中，这里涉及到多CPU的负载均衡     </li><li>数据帧从队列中递交到网络协议栈   </li><li>协议栈处理数据   </li><li>数据被添加到socket缓冲区   </li></ol><p>下面将详细描述每一步所经过的处理。</p><h3 id="网卡驱动的加载和初始化"><a href="#网卡驱动的加载和初始化" class="headerlink" title="网卡驱动的加载和初始化"></a>网卡驱动的加载和初始化</h3><p>网卡驱动程序向内核注册一个初始化函数，当驱动加载的时候，该函数被内核调用。Linux内核中通过module_init宏进行注册。<br>网卡需要有驱动才能工作，驱动是加载到内核中的模块，负责衔接网卡和内核。驱动在加载的时候将自己注册进网络模块，当相应的网卡收到数据包时，网络模块会调用相应的驱动程序处理数据。<br>Linux内核中igb初始化函数(igb_init_module)和驱动注册函数(module_init）<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L676-L697" target="_blank" rel="noopener">源码地址</a>，可以看出，igb_init_module函数的大部分工作是通过pci_register_driver来完成的。<br><img src="http://i.imgur.com/DBOCp1s.png" alt="">     </p><h4 id="PCI设备列表"><a href="#PCI设备列表" class="headerlink" title="PCI设备列表"></a>PCI设备列表</h4><p>我们知道，一个驱动程序可以支持一个或多个设备，而一个设备只会绑定给一个驱动程序。因此，驱动程序将其支持的所有设备保存在一个列表（PCI device IDs）中，内核使用这些表来决定加载驱动的类型。<br>Linux内核中igb驱动程序所支持的PCI设备列表<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L79-L117" target="_blank" rel="noopener">源码地址</a></p><pre><code>static DEFINE_PCI_DEVICE_TABLE(igb_pci_tbl) = {    { PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_BACKPLANE_1GBPS) },      { PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_SGMII) },      { PCI_VDEVICE(INTEL, E1000_DEV_ID_I354_BACKPLANE_2_5GBPS) },      { PCI_VDEVICE(INTEL, E1000_DEV_ID_I211_COPPER), board_82575 },      { PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_COPPER), board_82575 },      { PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_FIBER), board_82575 },      { PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SERDES), board_82575 },      { PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SGMII), board_82575 },      { PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_COPPER_FLASHLESS), board_82575 },      { PCI_VDEVICE(INTEL, E1000_DEV_ID_I210_SERDES_FLASHLESS), board_82575 },    /* ... */};MODULE_DEVICE_TABLE(pci, igb_pci_tbl);</code></pre><p>如前文所述，igb初始化函数igb_init_module的大部分工作由pci_register_driver完成。后者通过一系列函数指针的赋值以及PCI设备列表的注册完成了PCI网卡设备的初始化。</p><pre><code>static struct pci_driver igb_driver = {    .name     = igb_driver_name,    .id_table = igb_pci_tbl,    .probe    = igb_probe,    .remove   = igb_remove,    /* ... */        };</code></pre><h4 id="PCI-probe"><a href="#PCI-probe" class="headerlink" title="PCI probe"></a>PCI probe</h4><p>一旦PCI设备通过其PCI ID被内核识别，内核就可以选择合适的驱动来控制该设备。这个过程是通过PCI驱动程序向内核PCI子系统注册的探测函数（probe function）完成的。其处理流程包括：       </p><ol><li>打开PCI设备       </li><li>申请内存和I/O端口     </li><li>设置DMA参数    </li><li>注册驱动支持的ethtool调用函数              </li><li>struct net_device_ops结构的创建、初始化和注册。该结构体包含了指向打开设备、发送数据、设置MAC地址等操作函数的函数指针。   </li><li>struct net_device结构体的创建、初始化和注册。sturct net_device代表一个网络设备。 </li></ol><p>下面是ibg驱动程序中igb_probe函数的部分代码。<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2016-L2429" target="_blank" rel="noopener">源码地址</a>   </p><pre><code>err = pci_enable_device_mem(pdev)/* ... */err = dma_set_mask_and_coherent(&amp;pdev-&gt;dev, DMA_BIT_MASK(64));  /* ... */err = pci_request_selected_regions(pdev, pci_select_bars(pdev, IORESOURCE_MEM), igb_driver_name)；pci_enable_pcie_error_report(pdev);pci_set_master(pdev);pci_save_state(pdev);/* ... */netdev-&gt;netdev_ops = &amp;igb_netdev_ops;</code></pre><p>以及net_device_ops结构体中函数指针的初始化。<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L1905-L1913" target="_blank" rel="noopener">源码地址</a></p><pre><code>struct const struct net_device_ops igb_netdev_ops = {    .ndo_open = igb_open,    .ndo_stop = igb_close,    .ndo_start_xmit = igb_xmit_frame,    .ndo_get_stats64 = igb_get_stats64,    .ndo_set_mac_address = igb_set_mac,    .ndo_change_mtu = igb_change_mtu,    .ndo_do_ioctl = igb_ioctl,    /* ... */</code></pre><p>下面说明驱动中注册ethtool调用函数的过程。<br>ethtool是一个命令行程序，可以使用它来获取并配置网卡硬件和驱动。在ubuntu系统下，可以使用<code>apt-get install ethtool</code>来安装ethtool。<br>ethtool最常见的用法是从网络设备中获取详细的统计数据。<br>ethtool通过使用ioctl系统调用来和网卡驱动打交道。网卡驱动通过注册一系列函数来运行ethtool的操作指令。当ethtool发起了一个ioctl系统调用，内核将会在合适的驱动中寻找注册的ethtool结构并执行注册的函数。<br>igb驱动程序仍然在igb_probe完成ethtool操作的注册。    </p><pre><code>static int igb_probe(stauct pci_dev* pdev, const struct pci_device_id *ent){    /* ... */    igb_set_ethtool_ops(netdev);}</code></pre><p>igb驱动中的ethtool操作的函数指针结构体及设置在 <a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_ethtool.c#L2970-L3015" target="_blank" rel="noopener">源码地址</a></p><pre><code>static const struct ethtool_ops igb_ethtool_ops = {    .get_settings = igb_get_settings,    .set_settings = igb_set_settings,    .get_drvinfo = igb_get_drvinfo,    .get_regs_len = igb_get_regs_len,    .get_regs = igb_get_regs,    /* ... */}static int igb_probe(struct pci_dev *pdev, const struct pci_devie_id *ent){    /* ... */    igb_set_ethtool_ops(netdev);}</code></pre><p>此外，需要说明的是，ethtool所实现的函数由驱动自身决定，因此，并不是所有的驱动都实现了ethtool的所有函数。</p><h4 id="更多关于Linux-PCI-PCIe驱动的信息"><a href="#更多关于Linux-PCI-PCIe驱动的信息" class="headerlink" title="更多关于Linux PCI/PCIe驱动的信息"></a>更多关于Linux PCI/PCIe驱动的信息</h4><p>PCI、PCIe设备驱动程序的细节可以参考<a href="http://wiki.osdev.org/PCI" target="_blank" rel="noopener">wiki</a>、<a href="https://github.com/torvalds/linux/blob/v3.13/Documentation/PCI/pci.txt" target="_blank" rel="noopener">text file in the linux kernel</a>等相关资料，这里不做更深层次讲解。      </p><h3 id="中断和NAPI"><a href="#中断和NAPI" class="headerlink" title="中断和NAPI"></a>中断和NAPI</h3><p>当数据通过DMA的方式被传送到RAM之后，NIC需要一种通知机制来通告数据的到来。传统的方式是NIC产生一个硬件中断给CPU。但是，这种方法的缺陷在于，如果有大量的数据包到达，将会产生很多次的硬件中断，极大的降低了CPU处理的效率。<br>在这种情况下，NAPI（New Api）被提出，用于减少（不是消除）数据包到达时硬中断的产生次数，下面将详细介绍NAPI机制。     </p><h4 id="NAPI机制"><a href="#NAPI机制" class="headerlink" title="NAPI机制"></a>NAPI机制</h4><p>NAPI的核心概念是不采用中断的方式读取数据，而是首先采用中断唤醒数据接收的服务程序，然后使用poll方法来轮询数据。这样可以防止高频率的中断影响系统的整体运行效率。<br>当然，NAPI也存在一些缺陷，比如，对于上层的应用程序而言，系统不能在每个数据包接收到的时候都可以及时地区处理它，而且随着传输速度增加，累计的数据包将会耗费大量的内存；对大的数据包处理比较困难，原因是大的数据包传送到网络层上的时候耗费的时间比短数据包长很多。<br>NAPI机制的大致处理流程如下：<br><img src="http://i.imgur.com/sPbd3RI.jpg" alt="">          </p><ol><li>NAPI被驱动使能，但是初始时处于OFF状态    </li><li>数据包到来并通过DMA方式传送到内存    </li><li>NIC产生硬中断，触发中断处理函数      </li><li>中断处理函数中，驱动通过软中断唤醒NAPI子系统，通过在一个单独执行线程上调用注册的poll函数接收数据包    </li><li>网卡驱动程序关掉硬中断，从而允许NAPI子系统在处理数据包期间不会被中断。    </li><li>一旦处理完成，NAPI子系统被关闭，网卡硬件中断打开。        </li></ol><h4 id="igb驱动程序实现"><a href="#igb驱动程序实现" class="headerlink" title="igb驱动程序实现"></a>igb驱动程序实现</h4><p>下面结合igb驱动程序详细说明。<br>1.注册poll函数<br>设备驱动程序实现了poll函数，并通过调用netif_napi_add函数将其注册到NAPI子系统。该过程发生在驱动初始化的过程中。igb驱动中注册的相关代码<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L1145-L1271" target="_blank" rel="noopener">源码地址</a>   </p><pre><code>static int igb_alloc_q_vector(struct igb_adapter* adapter, int v_count, int v_idx, int txr_count, int trx_idx, int rxr_count, int rxr_idx){    q_vector = kzalloc(size, GFP_KERNEL);        if(!q_vector)        return -ENOMEM;    netif_napi_add(adapter-&gt;netdev, &amp;q_vector-&gt;napi, igb_poll, 64);/* ... */}   </code></pre><p>2.打开网络设备<br>当网络设备被使能（比如，ifconfig eth0 up），net_device_ops中的ndo_open所指向的函数(igb驱动中的igb_open)将会被调用。完成以下处理：<br>（1）分配RX和TX队列的内存空间；<br>（2）打开NAPI<br>（3）注册中断处理函数<br>（4）打开硬中断<br>3.网卡环形缓冲区和多队列<br>现在，大多数网卡都采用基于环形缓冲区的队列来进行DMA的传送。因此，驱动程序需要同操作系统配合为NIC预留一块可以使用的内存区域。一旦该内存区域被分配，硬件将会被通知其地址并且到达的数据将会被写入到RAM。<br>但是，如果数据包接收速率很高以致于一个CPU不能处理呢？ 这种情况下，就会导致大量丢包的发生。所以，RSS(Receive Side Scaling)和网卡多队列技术被提出。<br>RSS是一种能够在多处理器系统下使接收报文在多个CPU之间高效分发的网卡驱动技术。主要思想是基于Hash来实现动态的负载均衡，关于RSS，后面还会详细介绍。<br>而网卡多队列是指有些网卡能够同时将数据包写入到不同的区域，每一个区域都是一个单独的队列。这种情况下，操作系统可以使用多CPU在硬件层面上并行的处理到来的数据。<br>Inter I350 NIC支持多队列，可以在igb驱动程序中找到对应函数(<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L2801-L2804" target="_blank" rel="noopener">源码地址</a>)   </p><pre><code>err = igb_setup_all_rx_resources(adapter);    if(err)         goto err_setup_rx;</code></pre><p>igb_setup_all_rx_resources函数调用igb_setup_rx_resources（<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L3098-L3122" target="_blank" rel="noopener">源码地址</a>）来为每一个RX队列DMA memory来存放网卡收到的数据。    </p><pre><code>static int igb_setup_rx_resources(struct igb_adapter* adapter){    struct pci_dev* pdev = adapter-&gt;pdev;    int i, err = 0;    for(i=0;i&lt;adapter-&gt;num_rx_queues;i++){        err = igb_setup_rx_resources(adapter-&gt;rx_ring[i]);        /* ... */    }    return err;}</code></pre><p>4.Enable NAPI<br>前面我们已经看到驱动程序如何将poll函数注册到NAPI子系统，但是，NAPI通常会等到设备被打开之后才会开始工作。<br>使能NAPI比较简单。在igb驱动中，调用napi_enable实现。  </p><pre><code>for(i=0;i&lt;adapter-&gt;num_q_vectors;i++){    napi_enable(&amp;(adapter-&gt;q_vector[i]-&gt;napi));}</code></pre><p>5.注册中断处理函数<br>使能NAPI后，接下来需要注册中断处理函数。通常情况下，设备可以采用不同的方式来通告中断: MSI-X, MSI和传统的中断方法。其中，MSI-X中断是较好的方法，特别是对于支持多RX队列的NIC，每个RX队列都有其特定分配的硬中断，可以被特定的CPU处理。关于这三种具体的中断机制，这里不再深究。<br>驱动程序必须采用device支持的中断方法注册合适的处理函数，以便当中断发生时，可以正常调用。<br>在igb驱动中，函数igb_msix_ring，igb_intr_msi，igb_intr分别是对应于MSI-X,MSI和legacy模式的中断处理函数。驱动程序将按照MSI-X-&gt;MSI和legacy的顺序尝试注册中断处理函数。<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L1360-L1413" target="_blank" rel="noopener">源码地址</a></p><pre><code>static int igb_request_irq(struct igb_adapter* adapter){    struct net_device* netdev = adapter-&gt;netdev;    struct pci_dev* pdev = adapter-&gt;pdev;    int err = 0;    if(adapter-&gt;msix_entries){        err = igb_request_msix(adapter);        if(!err)            goto request_done;        /* fall back to MSI */    }    /* ... */    if(adapter-&gt;flags &amp; IGB_FLAG_HAS_MSI){        err = request_irq(pdev-&gt;irq, igb_intr_msi, 0, netdev-&gt;name, adapter);        if(!err)            goto request_done;        /* fall back to MSI */    }    /* ... */    err = request_irq(pdev-&gt;irq, igb_intr, IRQF_SHARED, netdev-&gt;name, adapter);    /* ... */}</code></pre><p>以上便是igb驱动注册函数的过程，当NIC产生一个硬件中断信号表明数据包到来等待被处理时，这些函数将会执行。<br>6.Enable Interrupts<br>到这里为止，几乎所有的设置已经完成，除了打开NIC中断，等待数据包的到来。打开中断是一个硬件操作，igb驱动通过函数igb_irq_enable写寄存器实现。<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L1462-L1488" target="_blank" rel="noopener">源码地址</a></p><pre><code>static void igb_irq_enable(struct igb_adapter* adapter){    /* ... */    wr32(E1000_IMS, IMS_ENABLE_MASK | E1000_IMS_DRSTA);    wr32(E1000_IAM, IMS_ENABLE_MASK | E1000_IMS_DRSTA);    /* ... */}</code></pre><h3 id="接收数据包处理"><a href="#接收数据包处理" class="headerlink" title="接收数据包处理"></a>接收数据包处理</h3><h4 id="softirq"><a href="#softirq" class="headerlink" title="softirq"></a>softirq</h4><p>先来看下Linux内核中的软中断。<br>Linux内核中的软中断是一种在驱动层面上实现的在进程上下文之外执行中断处理函数的机制。该机制十分重要，因为硬件中断在中断处理程序执行期间可能会被关闭。关闭的时间越长，事件被错过处理的机会就越大。因此，在中断处理程序之外推迟长时间的事件处理十分重要，这样可以尽快完成中断处理程序并且重新打开设备中断。<br>软中断可以被想象成一系列的内核线程（每个CPU一个） ，这些内核线程执行针对不同软中断事件注册的事件处理函数。比如，使用top命令，在内核线程列表中看到的ksoftirqd/0，就是运行在CPU 0上的。<br>内核通过执行open_softirq函数来注册软中断处理函数。    </p><h4 id="ksoftirq"><a href="#ksoftirq" class="headerlink" title="ksoftirq"></a>ksoftirq</h4><p>软中断对于延迟设备驱动的事件处理非常重要，因此ksoftirq进程随着内核启动很早就会运行。<a href="https://github.com/torvalds/linux/blob/v3.13/kernel/softirq.c#L743-L758" target="_blank" rel="noopener">相关源码</a>     </p><pre><code>static struct smp_hotplug_thread_softirq_threads = {    .store             = &amp;ksoftirq,    .thread_should_run = ksoftirq_should_run,    .thread_fn         = run_ksoftirqd,    .thread_comm       = &quot;ksoftirqd/%u&quot;. };static __init int spawn_ksoftirqd(void) {    register_cpu_notifier(&amp;cpu_nfb);    BUG_ON(smpboot_register_percpu_thread(&amp;softirq_threads));    return 0;}  early_initcall(spawn_ksoftirqd);</code></pre><h4 id="Linux网络子系统"><a href="#Linux网络子系统" class="headerlink" title="Linux网络子系统"></a>Linux网络子系统</h4><p>到现在为止，我们已经分析了网络驱动和软中断的工作流程，接下来开始分析Linux netdevice子系统的初始化和网络数据包到达之后的处理流程。<br>netdevice子系统通过net_dev_init函数进行初始化。该函数将会对每个CPU创建struct softnet_data结构体，该结构体包含了指向（1）注册到该CPU的NAPI结构体列表；（2）数据处理的backlog值；（3）RPS（Receive packet steering）值等重要数据的指针。<br>此外，net_dev_init注册了两个软中断处理函数，分别用于处理到来和发送的数据包。</p><pre><code>static int __init net_dev_init(void){    /* ... */    open_softirq(NET_TX_SOFTIRQ, net_tc_action);    open_softirq(NET_RX_SOFTIRQ, net_rx_action);    /* ... */        }</code></pre><p>后文中，我们很快就会看到驱动处理函数怎么“触发”注册在NET_RX_SOFTIQ（NET_TX_SOFTIQ）上的net_rx_action(net_tx_action)函数。   </p><h4 id="数据包到来"><a href="#数据包到来" class="headerlink" title="数据包到来"></a>数据包到来</h4><p>终于，数据包到来了。<br>数据包到达RX队列之后，将通过DMA传输到RAM，并触发硬件中断。如前文所述，中断处理函数将尽量推迟更多的处理发生在中断上下文之外。因为在处理中断的过程中，其他中断可能会被阻塞。<br>下面是igb驱动程序中MSI-X中断处理函数的<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L5148-L5158" target="_blank" rel="noopener">源代码</a>，从中我们可以看出，中断处理程序的代码逻辑非常简单，调用igb_write_itr和napi_schedule两个函数完成快速处理。前者用于更新特定于硬件的寄存器。后者唤醒NAPI处理循环，用于在软中断中处理数据包。       </p><pre><code>static irqreturn_t igb_msix_ring(int irq, void* data){    struct igb_q_vector* q_vector = data;    /* Write the ITR value calculated from the previous interrupt */    igb_write_itr(q_vector);    napi_schedule(&amp;q_vector-&gt;napi);    return IRQ_HANDLED;}</code></pre><p>napi_schedule是__napi_schedule函数的封装。后者主要完成了两个处理，一个是struct napi_struct添加到当前CPU所关联的结构体softnet_data中的poll_list链表中，另一个则是调用__raise_softirq_irqoff来触发NET_RX_SOFTIRQ软中断，从而执行在netdevice子系统初始化期间注册的net_rx_action函数。后文我们会看到，软中断处理函数nx_rx_action将会调用NAPI的poll函数来接收数据包。     </p><pre><code>void __napi_schedule(struct napi_struct *n){    unsigned long flags;    local_irq_save(flags);    __napi_schedule(&amp;__get_cpu_var(softnet_data), n);    local_irq_restore(flags);}EXPORT_SYMBOL(__napi_schedule);static inline void __napi_schedule(struct softnet_data *sd, struct napi_struct* napi){    list_add_tail(&amp;napi-&gt;poll_list, &amp;sd-&gt;poll_list);    __raise_softirq_irqoff(NET_RX_SOFTIRQ);}    </code></pre><p>最后，需要说明的是，中断处理程序做快速操作和将数据包接收处理推迟到软中断都是在同一CPU上进行的。这也是通常将中断处理绑定到固定CPU的原因。  </p><h4 id="多队列"><a href="#多队列" class="headerlink" title="多队列"></a>多队列</h4><p>如果NIC支持RSS、multi queue，或者想更好的利用好数据的局部性原理，我们可以设置使用一些特定的CPU来处理NIC中断。<br>在决定调整IRQ处理的参数之前，应该首先检查是否已经在后台执行irqbalance，该进程将会尝试自动平衡IRQ和CPU之间的关系，并且会覆盖用户的设置。然后在/proc/interrupts中获取NIC中每个RX队列的IRQ numbers之后，就可以通过修改/proc/irq/IRQ_NUMBER/smp_affinity来调整CPU和IRQ的对应关系。</p><h4 id="数据包处理"><a href="#数据包处理" class="headerlink" title="数据包处理"></a>数据包处理</h4><p>一旦软中断softirq开始被处理并调用net_rx_action，数据包处理流程就正式开始了。<br>net_rx_action遍历当前CPU队列中的NAPI列表，取出队列中的NAPI结构，并依次对其进行操作。正如前面提到的，net_rx_action将会调用NAPI的poll函数来接收数据包。<br>igb驱动中net_rx_action的部分实现<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c" target="_blank" rel="noopener">代码</a>如下：</p><pre><code>/* initialize NAPI */netif_napi_add(adapter-&gt;netdev, &amp;q_vector-&gt;napi, igb_poll, 64);        //64: weight/* net_rx_action */wight = n-&gt;weight;work = 0; if(test_bit(NAPI_STATE_SCHED, &amp;n-&gt;state)){    work = n-&gt;poll(n, weight);    trace_napi_poll(n);}WARN_ON_ONCE(work-&gt;weight);budget-=work;</code></pre><p>其中,weight代表RX队列的处理权重，budget表示一种惩罚措施，用于多CPU多队列之间的公平性调度。<br>接收数据包的igb_poll函数的部分实现<a href="https://github.com/torvalds/linux/blob/v3.13/drivers/net/ethernet/intel/igb/igb_main.c#L5987-L6018" target="_blank" rel="noopener">代码</a></p><pre><code>static int igb_poll(struct napi_struct* napi, int budget){    struct igb_q_vector* q_vector = container_of(napi,struct igb_q_vector,napi);    bool clean_complete = true;    #ifdef CONFIG_IGB_DCA    if (q_vector-&gt;adapter-&gt;flags &amp; IGB_FLAG_DCA_ENABLED)            igb_update_dca(q_vector);    #endif    /* ... */    if (q_vector-&gt;rx.ring)            clean_complete &amp;= igb_clean_rx_irq(q_vector, budget);     /* If all work not completed, return budget and keep polling */    if (!clean_complete)            return budget;    /* If not enough Rx work done, exit the polling mode */    napi_complete(napi);    igb_ring_irq_enable(q_vector);    return 0;}</code></pre><p>其流程主要包括：<br>1.如果内核支持DCA（Direct Cache Access），CPU缓存将会被很好的命中；<br>2.调用igb_clean_rx_irq循环处理数据包，直到处理完毕或者达到budget，即一次调度时间完成。该循环中将会完成以下处理：<br>（1）当可用缓冲buffer的时候，为到来的数据包分配buffer；<br>（2）从RX队列中取buffer数据并存储在skb结构中；<br>（3）检查取出的buffer数据是否是“End of Packet”，如果是，递交下一步处理；<br>（4）验证数据头部等信息是否正确；<br>（5）构建好的skb通过napi_gro_receive函数调用被递交到网络协议栈；<br>3.检查clean_complete标志位判定是否所有的工作已经完成，如果是，返回budget值，否则，调用napi_complete关闭NAPI，并通过igb_ring_irq_enable重新使能中断，以保证下次中断到来会重新使能NAPI。</p><h4 id="Generic-Receive-Offloading-GRO"><a href="#Generic-Receive-Offloading-GRO" class="headerlink" title="Generic Receive Offloading(GRO)"></a>Generic Receive Offloading(GRO)</h4><p>GRO是硬件优化方法LRO（Large Receive Offloading）的软件实现。<br>两种方法的主要思想都是通过合并“类似”的数据包来减少传递给网络协议栈的数据包数量，达到降低CPU利用率的目的。<br>这类优化方法可能带来的问题是信息的丢失。如果一个数据包含有一些重要的选项或标志位，这些选项或标志位可能在它和其他数据包合并的时候丢失。这也是通常不推荐使用LRO的原因，而GRO针对数据包合并的规则比较宽松。<br>GRO作为LRO的软件实现，在数据包合并的规则上显得更加严格。<br>顺便说一句，如果在使用tcpdump抓包时，看到一些很大的数据包，很有可能是系统启用了GRO。<br>比如，TCP协议需要决定是否或者什么时候将ACK合并到已存在的数据包中。<br>一旦dev_gro_receive执行完成，napi_skb_finish被调用，该函数或者释放不再需要的数据结构，因为数据包已经被合并；或者调用netif_receive_skb将数据传输到网络协议栈。<br>napi_gro_receive将通过GRO完成网络数据的处理，并将数据传送到网络协议栈。其中大部分的处理逻辑通过调用函数dev_gro_receive来完成。<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3826-L3916" target="_blank" rel="noopener">相关源码</a><br>当dev_gro_receive执行完成后，napi_skb_finish函数被调用，完成多余数据结构的清理（因为数据包已经被合并），或者调用netif_receive_skb将数据传送给网络协议栈。 </p><h4 id="RSS和RPS"><a href="#RSS和RPS" class="headerlink" title="RSS和RPS"></a>RSS和RPS</h4><p>在讲解netif_receive_skb之前，我们先来了解下Receive Packet Steering（RPS）和Receive Slide Steering（RSS）这两种用于多CPU负载均衡的处理机制。    </p><p>RSS用于多队列网卡，能够将网络流量分载到多个CPU上，降低单个CPU的占有率。默认情况下，每个CPU核对应一个RSS队列，驱动程序将收到的数据包的源、目的IP和端口号等，交由网卡硬件计算出一个hash值，再根据这个hash值来决定将数据包分配到哪个队列中。<br>可以看出，RSS是和硬件相关联的，必须要有网卡的硬件进行支持。<br>RPS可以认为是RPS的软件实现。RPS主要是把软中断负载均衡到各个CPU。简单地说，是网卡驱动对每个流生成一个hash标识，然后由中断处理程序根据这个hash表示将流分配到相应的CPU上，这样就可以比较充分地发挥多核的能力。<br>可以看出，RPS是在软件层面模拟实现硬件的多队列网卡功能，主要针对单队列网卡多CPU环境，如果网卡本身支持多队列的话RPS就不会有任何的功能。</p><p>因此，netif_receive_skb将根据是否设置RPS对数据包进行不同的操作：<br>（1）no RPS<br>如果没有配置RPS，netif_receive_skb将会调用__netif_receive_skb，后者在做一些信息的记录之后，调用__netif_receive_skb_core将数据移动到协议栈。<br>（2）RPS<br>如果RPS被打开，netif_receive_skb将会调用get_rps_cpu来计算hash并决定使用哪个CPU的积压队列。<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3699-L3705" target="_blank" rel="noopener">源码地址</a>，具体的入队操作则由get_rps_cpu调用enqueue_to_backlog完成。   </p><pre><code>cpu = get\_rps\_cpu(skb-&gt;dev, &amp;rflows);if(cpu&gt;=0){    ret = enqueue_to_backlog(skb, cpu, &amp;rflow-&gt;last_qtail);    rcu_read_unlock();    return ret;    }</code></pre><p>enqueue_to_backlog首先得到指向CPU的softnet_data结构体的指针，该结构体中含有指向input_pkt_queue队列结构的指针。因此，enqueue_to_backlog函数首先检查input_pkt_queue队列长度<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3199-L3200" target="_blank" rel="noopener">源码</a>,只有当队列长度同时不超过netdev_max_backlog和flow limit的值，数据包才会入队，否则将会被丢弃。    </p><pre><code>qlen = skb_queue_len(&amp;sd-&gt;input_pkt_queue);if(qlen &lt;= netdev_max_backlog &amp;&amp; !skb_flow_limit(skb, qlen)){    if(skb_queue_len(&amp;sd-&gt;input_pkt_queue)){enqueue:        __skb_queue_tail(&amp;sd-&gt;input_pkt_queue, skb);        input_queue_tail_incr_save(sd, qtail);        rps_unlock(sd);        local_irq_restore(flags);        return NET_RX_SUCCESS;    }}</code></pre><p>同没有配置RPS时一样，backlog队列中的数据将通过__netif_receive_skb（实际处理发生在__netif_receive_skb_core）函数出队并传递给网络协议栈。__netif_receive_skb_core首先得到接收数据包的协议类型字段，然后遍历为该协议类型注册的传递函数列表进行数据传送。<a href="https://github.com/torvalds/linux/blob/v3.13/net/core/dev.c#L3548-L3554" target="_blank" rel="noopener">部分源码</a>    </p><pre><code>type = skb-&gt;protocol;list_for_each_entry_rcu(ptype, &amp;ptype_all, list) {    if (!ptype-&gt;dev || ptype-&gt;dev == skb-&gt;dev) {        if (pt_prev)            ret = deliver_skb(skb, pt_prev, orig_dev);        pt_prev = ptype;    }}</code></pre><h4 id="协议栈处理"><a href="#协议栈处理" class="headerlink" title="协议栈处理"></a>协议栈处理</h4><p>接下来，数据包将依次经过Linux内核中的TCP/IP协议栈，在处理完成之后添加到socket缓冲区（或者被转发、丢弃等）、等待应用层程序的读取。该过程将会在后续的文章中详细分析。   </p><h3 id="附"><a href="#附" class="headerlink" title="附"></a>附</h3><p><a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/" target="_blank" rel="noopener">Monitoring and Tuning the Linux Networking Stack: Receiving Data</a>     </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文详细记录了从网卡驱动加载和初始化开始，一直到网络数据包到达，最后驱动程序将其递交到网络协议栈的整个处理过程。&lt;br&gt;
    
    </summary>
    
    
      <category term="network/linux" scheme="http://blog.jonnydu.me/tags/network-linux/"/>
    
  </entry>
  
  <entry>
    <title>NVMe over RDMA浅析</title>
    <link href="http://blog.jonnydu.me/2017/04/05/NVMe-over-RDMA/"/>
    <id>http://blog.jonnydu.me/2017/04/05/NVMe-over-RDMA/</id>
    <published>2017-04-05T12:49:59.000Z</published>
    <updated>2017-05-06T08:50:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>NVMe是一种Host与SSD之间的通信协议，为了把NVMe扩展到端到端的跨网络传输，NVMe的开发者提出了NVMe over Fabrics，用于解决将NVMe置于各种传输环境下所遇到的问题。<br><a id="more"></a> </p><h3 id="NVMe-over-Fabrics整体架构"><a href="#NVMe-over-Fabrics整体架构" class="headerlink" title="NVMe over Fabrics整体架构"></a>NVMe over Fabrics整体架构</h3><p>NVMe over Fabrics的整体架构如下图所示。<br><img src="http://i.imgur.com/vvD1rC7.png" alt=""><br>可以看出，NVMe Host(Controller)-side Transport Abstraction这两层便是NVMe over Fabrics协议的实现层。该协议只是一个用于NVMe Transport的抽象层而已，它并不实现真正的命令和数据传输功能，它只是为命令和数据传输定义了统一的规范，因此该协议只是“指导方针”。它是构建在”Fabrics”之上的，即它并不关心实际的Fabrics到底是什么，它只是提供了Fabrics通用的对接NVMe的接口，完成了对NVMe接口和命令在各种Fabrics而非只是PCIe上（NVMe Base协议只涉及PCIe这一种Fabric）的拓展。因此，为了使NVMe可以架构于不同的Fabric之上，各Fabric还需开发专用的功能实现层，真正实现基于此Fabric的数据传输功能，并完成和Transport Abstraction抽象层（即NVMe over Fabrics协议的实现层）的对接以使得传输抽象层可以调用到这些函数。         </p><h3 id="NVMe-over-RDMA"><a href="#NVMe-over-RDMA" class="headerlink" title="NVMe over RDMA"></a>NVMe over RDMA</h3><h4 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h4><p><a href="http://blog.dujiong.net/2017/02/27/RDMA/" target="_blank" rel="noopener">RDMA技术浅析</a>一文中介绍了RDMA这种”Fabric”以及它的几种实现方式。鉴于后续研究需要，这里选择RoCE（如无特别说明，均指的是v2版本）这种”Fabric”来展开说明。<br>NVMe over RoCE的架构如下图所示。<br><img src="http://i.imgur.com/Ukm4No4.png" alt=""><br>NVMe Host和NVMe Subsystem Controller是NVMe Base协议扩展到NVMe over Fabrics的部分；NVMe Host(Controller)-side Transport Abstraction则是NVMe over Fabrics传输抽象层的实现。RoCE层则是支持RoCE技术的网卡及相关驱动和RDMA协议栈，而不论InfiniBand、RoCE或者iWarp何种具体的RDMA实现形式，都约定提供统一的操作接口，RDMA Verbs便是RDMA技术向上层提供的接口。NVMe RDMA则是实现将RDMA的接口Verbs和NVMe对接的关键粘合层，简言之，其作用是将NVMe Transport Abstraction传输抽象层提供的传输接口可以调用到下层RDMA提供的传输接口（即verbs）。  </p><h4 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h4><p>此架构的具体操作原理将结合Linux 4.8版本内核的实现来解读。在4.8版本的Linux内核中加入了符合上图逻辑的NVMe over RDMA的代码。因此对于我们来说，只要编译最新内核并添加相关模块完成配置后，内核的代码完全不需要添加或修改。对于用户空间的应用程序来说，使用NVMe over RDMA的操作与传统的文件读写操作并无差异。这得益于Linux内核的软件分层架构，虚拟文件系统层、文件系统层和通用块层的架构可以屏蔽底层的硬件细节，且对上层提供统一的接口，这样上层用户空间的应用程序自然不会（也不应该）关注到底层硬件及使用协议的差别。Host端的Linux kernel的架构如下图所示。<br><img src="http://i.imgur.com/a9Nzq2g.png" alt=""><br>如果用户使用NVMe特有的命令，即当用户下发NVMe commands时，这需要通过Linux内核的IOCTL接口，该接口的目的本就是实现用户和底层驱动打交道。可以看到，NVMe commands通过IOCTL机制便到达NVMe Common代码层，该层代码的任务就是解析并执行各种NVMe command。在NVMe Base标准的定义中，命令是被封装在capsule这一数据结构中传输，应用将capsule压入内存中的NVMe Submission Queue，接着controller取出capsule并解析应用的command，处理之后将回复capsule压入NVMe Completion Queue，应用取出回复消息完成一次通信。具体过程参考前文<a href="http://blog.dujiong.net/2017/02/17/NVMe/" target="_blank" rel="noopener">NVMe技术浅析</a><br>而在NVMe over RDMA的环境下，传输过程如下图所示。<br><img src="http://i.imgur.com/zqCUGcT.png" alt=""><br>首先，host的command被封装进capsule后压入Host NVMe Submission Queue；接着该capsule会被放入Host RDMA Send Queue变成RDMA_SEND消息的消息负载；接着消息被host的RDMA网卡发包，当被target网卡接收后，capsule被放到target网卡的RDMA ReceIve Queue；接下来该capsule被放置到target端的内存中并由target处理该command；待处理完后，target生成回复信息（Respond Command）并封装进capsule然后将该回复capsule压入target端的NVMe Controller的Submission Queue，并经由target的网卡发送给host。这样，一次NVMe over RDMA的通信就完成了。</p><h4 id="NVMe-over-RDMA读写文件原理"><a href="#NVMe-over-RDMA读写文件原理" class="headerlink" title="NVMe over RDMA读写文件原理"></a>NVMe over RDMA读写文件原理</h4><p>下面以host读文件为例讨论数据流的传输。<br>事实上，在NVMe over Fabrics协议中，提出了两种数据传输方式，一种是将data附到capsule中，只要在传输command时负载需要传输的data即可。另一种则是直接的内存传输方式。这种方式原理上是将要传输的数据的地址和长度等元信息负载到capsule中。如用户发出read请求后，最终的capsule消息负载中会包含数据的存储地址、要读取的数据的长度以及要读到的内存地址。利用RDMA技术，这种传输的特性将表现的更加淋漓尽致。<br>由Linux内核知识可知，虚拟文件系统（VFS）向上层提供统一的文件操作接口，当用户空间进程读取文件时便调用read API；接着VFS调用到真正的文件系统（例如Ext4等）真正的read函数，真正文件系统的作用是确定数据的位置，简言之就是根据用户要读取的文件、长度、偏移量等确定文件的逻辑块号（在真实的文件存储中，一个文件是被分割成若干块存储的）；接下来内核利用通用块层（Block Layer）启动IO操作来传送所请求的数据，通用块层为所有的块设备提供了一个抽象视图，隐藏了硬件块设备间的差异，而每次IO操作由一个“块I/O”结构（struct bio）的对象来描述。至此，所有的读文件操作都是这套同样的流程。由于NVMe或AHCI都是更底层的接口标准，因此差异从通用块层之下才开始。<br>此后，进入到NVMe Transport Abstraction（即NVMe over Fabrics协议的实现层）或者NVMe over PCIe，这时便会由此生成NVMe（base或over Fabrics）标准的Read Command，由NVMe Base标准对Read Command的定义，Read Command中包含了数据应该读到的内存区域的地址（Read Command的Data Pointer字段）、以及存储数据的逻辑块号的起始地址（Starting LBA字段）等其他在read操作中必需的字段（显然地，这些信息从上层通用块层传递的bio对象中获取），并将该Read Command封装进NVMe的通信单元capsule中。接着便进入到下一层的RDMA stack。RDMA Stack之下便是支持RDMA技术的网卡驱动，最底层便是不同技术（IB/RoCE/iWarp）实现的RDMA网卡。内核的NVMe RDMA层代码完成调用RDMA的接口verbs将该capsule封装为RDMA_SEND消息的负载。<br>Target的controller处理完read command后，数据流传输便开始了。数据流的传输则完全借助于RoCE技术。由于此前在target端注册了host的内存，接下来从target的SSD中读出的数据便直接封装为RDMA_WRITE消息的负载（注意host的Read Command触发的却是target的RDMA_WRITE操作），然后将这部分数据直接从target端写入到host的内存中，而写入的地址在Read Command中便已指明。该过程的实现得益于RDMA的数据零复制技术。      </p><p>说了这么多，通过一幅图来直观展示上述NVMe over RDMA读文件的过程。<br><img src="http://i.imgur.com/Mi5Okr5.png" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文分析了将NVMe扩展到端到端的跨网络传输—NVMe over RDMA的架构和实现原理。以读文件为例，详细分析了数据流的传输过程。后续将进入实战阶段，在服务器上搭建小型NVMe存储系统，并通过RDMA进行跨网络传输。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NVMe是一种Host与SSD之间的通信协议，为了把NVMe扩展到端到端的跨网络传输，NVMe的开发者提出了NVMe over Fabrics，用于解决将NVMe置于各种传输环境下所遇到的问题。&lt;br&gt;
    
    </summary>
    
    
      <category term="NVMe over RDMA" scheme="http://blog.jonnydu.me/tags/NVMe-over-RDMA/"/>
    
  </entry>
  
  <entry>
    <title>LevelDB原理剖析</title>
    <link href="http://blog.jonnydu.me/2017/03/27/leveldb/"/>
    <id>http://blog.jonnydu.me/2017/03/27/leveldb/</id>
    <published>2017-03-27T08:49:41.000Z</published>
    <updated>2017-05-06T10:52:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>LevelDB是能够处理十亿级别规模Key-Value型数据持久性存储的C++程序库，是谷歌两位大牛Jeff Dean和Sanjay Ghemawat发起的开源项目。自己已经将拜读LevelDB纳入了项目完成后的学习计划。最近偶然看到一篇关于LevelDB很好的博客，特记录下来，希望对后面的学习有帮助。<br><a id="more"></a><br>转自：<a href="http://www.ezlippi.com/blog/2014/11/leveldb.html" target="_blank" rel="noopener">leveldb原理剖析</a></p><h3 id="LevelDB剖析之一：介绍"><a href="#LevelDB剖析之一：介绍" class="headerlink" title="LevelDB剖析之一：介绍"></a>LevelDB剖析之一：介绍</h3><p>LevelDB有如下一些特点：<br>首先，LevelDB是一个持久化存储的KV系统，和Redis这种内存型的KV系统不同，LevelDB不会像Redis一样狂吃内存，而是将大部分数据存储到磁盘上。<br>其次，LevleDB在存储数据时，是根据记录的key值有序存储的，就是说相邻的key值在存储文件中是依次顺序存储的，而应用可以自定义key大小比较函数，LevleDB会按照用户定义的比较函数依序存储这些记录。<br>再次，像大多数KV系统一样，LevelDB的操作接口很简单，基本操作包括写记录，读记录以及删除记录。也支持针对多条操作的原子批量操作。<br>另外，LevelDB支持数据快照（snapshot）功能，使得读取操作不受写操作影响，可以在读操作过程中始终看到一致的数据。<br>除此外，LevelDB还支持数据压缩等操作，这对于减小存储空间以及增快IO效率都有直接的帮助。<br>LevelDB性能非常突出，官方网站报道其随机写性能达到40万条记录每秒，而随机读性能达到6万条记录每秒。总体来说，LevelDB的写操作要大大快于读操作，而顺序读写操作则大大快于随机读写操作。      </p><h3 id="LevelDB剖析之二：整体架构"><a href="#LevelDB剖析之二：整体架构" class="headerlink" title="LevelDB剖析之二：整体架构"></a>LevelDB剖析之二：整体架构</h3><p>LevelDB本质上是一套存储系统以及在这套存储系统上提供的一些操作接口。为了便于理解整个系统及其处理流程，我们可以从两个不同的角度来看待LevleDB：静态角度和动态角度。从静态角度，可以假想整个系统正在运行过程中（不断插入删除读取数据），此时我们给LevelDB照相，从照片可以看到之前系统的数据在内存和磁盘中是如何分布的，处于什么状态等；从动态的角度，主要是了解系统是如何写入一条记录，读出一条记录，删除一条记录的，同时也包括除了这些接口操作外的内部操作比如compaction，系统运行时崩溃后如何恢复系统等等方面。<br>本节所讲的整体架构主要从静态角度来描述，之后接下来的几节内容会详述静态结构涉及到的文件或者内存数据结构，LevelDB剖析后半部分主要介绍动态视角下的LevelDB，就是说整个系统是怎么运转起来的。<br>LevelDB为存储系统，数据记录的存储介质包括内存以及磁盘文件，如果像上面说的，当LevelDB运行了一段时间，此时我们给LevelDB进行透视拍照，那么您会看到如下一番景象：<br><img src="http://i.imgur.com/CiivZ3q.png" alt=""><br>图1.1：LevelDB结构</p><p>从图中可以看出，构成LevelDB静态结构的包括六个主要部分：内存中的MemTable和Immutable MemTable以及磁盘上的几种主要文件：Current文件，Manifest文件，log文件以及SSTable文件。当然，LevelDB除了这六个主要部分还有一些辅助的文件，但是以上六个文件和数据结构是LevelDB的主体构成元素。<br>LevelDB的Log文件和Memtable与Bigtable论文中介绍的是一致的，当应用写入一条Key:Value记录的时候，LevelDB会先往log文件里写入，成功后将记录插进Memtable中，这样基本就算完成了写入操作，因为一次写入操作只涉及一次磁盘顺序写和一次内存写入，所以这是为何说LevelDB写入速度极快的主要原因。<br>Log文件在系统中的作用主要是用于系统崩溃恢复而不丢失数据，假如没有Log文件，因为写入的记录刚开始是保存在内存中的，此时如果系统崩溃，内存中的数据还没有来得及Dump到磁盘，所以会丢失数据（Redis就存在这个问题）。为了避免这种情况，LevelDB在写入内存前先将操作记录到Log文件中，然后再记入内存中，这样即使系统崩溃，也可以从Log文件中恢复内存中的Memtable，不会造成数据的丢失。<br>当Memtable插入的数据占用内存到了一个界限后，需要将内存的记录导出到外存文件中，LevleDB会生成新的Log文件和Memtable，原先的Memtable就成为Immutable Memtable，顾名思义，就是说这个Memtable的内容是不可更改的，只能读不能写入或者删除。新到来的数据被记入新的Log文件和Memtable，LevelDB后台调度会将Immutable Memtable的数据导出到磁盘，形成一个新的SSTable文件。SSTable就是由内存中的数据不断导出并进行Compaction操作后形成的，而且SSTable的所有文件是一种层级结构，第一层为Level 0，第二层为Level 1，依次类推，层级逐渐增高，这也是为何称之为LevelDB的原因。<br>SSTable中的文件是Key有序的，就是说在文件中小key记录排在大Key记录之前，各个Level的SSTable都是如此，但是这里需要注意的一点是：Level 0的SSTable文件（后缀为.sst）和其它Level的文件相比有特殊性：这个层级内的.sst文件，两个文件可能存在key重叠，比如有两个level 0的sst文件，文件A和文件B，文件A的key范围是：{bar, car}，文件B的Key范围是{blue,samecity}，那么很可能两个文件都存在key=”blood”的记录。对于其它Level的SSTable文件来说，则不会出现同一层级内.sst文件的key重叠现象，就是说Level L中任意两个.sst文件，那么可以保证它们的key值是不会重叠的。这点需要特别注意，后面您会看到很多操作的差异都是由于这个原因造成的。<br>SSTable中的某个文件属于特定层级，而且其存储的记录是key有序的，那么必然有文件中的最小key和最大key，这是非常重要的信息，LevelDB应该记下这些信息。Manifest就是干这个的，它记载了SSTable各个文件的管理信息，比如属于哪个Level，文件名称叫啥，最小key和最大key各自是多少。下图是Manifest所存储内容的示意：<br><img src="http://i.imgur.com/5JI6jhp.png" alt=""><br>图2.1：Manifest存储示意图</p><p>图中只显示了两个文件（manifest会记载所有SSTable文件的这些信息），即Level 0的test.sst1和test.sst2文件，同时记载了这些文件各自对应的key范围，比如test.sstt1的key范围是“an”到 “banana”，而文件test.sst2的key范围是“baby”到“samecity”，可以看出两者的key范围是有重叠的。<br>Current文件是干什么的呢？这个文件的内容只有一个信息，就是记载当前的manifest文件名。因为在LevleDB的运行过程中，随着Compaction的进行，SSTable文件会发生变化，会有新的文件产生，老的文件被废弃，Manifest也会跟着反映这种变化，此时往往会新生成Manifest文件来记载这种变化，而Current则用来指出哪个Manifest文件才是我们关心的那个Manifest文件。<br>以上介绍的内容就构成了LevelDB的整体静态结构，在LevelDB剖析接下来的内容中，我们会首先介绍重要文件或者内存数据的具体数据布局与结构。</p><h3 id="LevelDB剖析之三：log文件"><a href="#LevelDB剖析之三：log文件" class="headerlink" title="LevelDB剖析之三：log文件"></a>LevelDB剖析之三：log文件</h3><p>上节内容讲到log文件在LevelDB中的主要作用是系统故障恢复时，能够保证不会丢失数据。因为在将记录写入内存的Memtable之前，会先写入Log文件，这样即使系统发生故障，Memtable中的数据没有来得及Dump到磁盘的SSTable文件，LevelDB也可以根据log文件恢复内存的Memtable数据结构内容，不会造成系统丢失数据，在这点上LevelDB和Bigtable是一致的。<br>下面我们带大家看看log文件的具体物理和逻辑布局是怎样的，LevelDB对于一个log文件，会把它切割成以32K为单位的物理Block，每次读取的单位以一个Block作为基本读取单位，下图展示的log文件由3个Block构成，所以从物理布局来讲，一个log文件就是由连续的32K大小Block构成的。<br><img src="http://i.imgur.com/lYMLMrA.png" alt=""><br>图3.1 log文件布局  </p><p>在应用的视野里是看不到这些Block的，应用看到的是一系列的Key:Value对，在LevelDB内部，会将一个Key:Value对看做一条记录的数据，另外在这个数据前增加一个记录头，用来记载一些管理信息，以方便内部处理。<br>记录头包含三个字段，ChechSum是对“类型”和“数据”字段的校验码，为了避免处理不完整或者是被破坏的数据，当LevelDB读取记录数据时候会对数据进行校验，如果发现和存储的CheckSum相同，说明数据完整无破坏，可以继续后续流程。“记录长度”记载了数据的大小，“数据”则是上面讲的Key:Value数值对，“类型”字段则指出了每条记录的逻辑结构和log文件物理分块结构之间的关系，具体而言，主要有以下四种类型：FULL/FIRST/MIDDLE/LAST。<br>如果记录类型是FULL，代表了当前记录内容完整地存储在一个物理Block里，没有被不同的物理Block切割开；如果记录被相邻的物理Block切割开，则类型会是其他三种类型中的一种。我们以图3.1所示的例子来具体说明。<br>假设目前存在三条记录，Record A，Record B和Record C，其中Record A大小为10K，Record B 大小为80K，Record C大小为12K，那么其在log文件中的逻辑布局会如图3.1所示。Record A是图中蓝色区域所示，因为大小为10K&lt;32K，能够放在一个物理Block中，所以其类型为FULL；Record B 大小为80K，而Block 1因为放入了Record A，所以还剩下22K，不足以放下Record B，所以在Block 1的剩余部分放入Record B的开头一部分，类型标识为FIRST，代表了是一个记录的起始部分；Record B还有58K没有存储，这些只能依次放在后续的物理Block里面，因为Block 2大小只有32K，仍然放不下Record B的剩余部分，所以Block 2全部用来放Record B，且标识类型为MIDDLE，意思是这是Record B中间一段数据；Record B剩下的部分可以完全放在Block 3中，类型标识为LAST，代表了这是Record B的末尾数据；图中黄色的Record C因为大小为12K，Block 3剩下的空间足以全部放下它，所以其类型标识为FULL。<br>从这个小例子可以看出逻辑记录和物理Block之间的关系，LevelDB一次物理读取为一个Block，然后根据类型情况拼接出逻辑记录，供后续流程处理。</p><h3 id="LevelDB剖析之四：SSTable文件"><a href="#LevelDB剖析之四：SSTable文件" class="headerlink" title="LevelDB剖析之四：SSTable文件"></a>LevelDB剖析之四：SSTable文件</h3><p>SSTable是Bigtable中至关重要的一块，对于LevelDB来说也是如此，对LevelDB的SSTable实现细节的了解也有助于了解Bigtable中一些实现细节。<br>本节内容主要讲述SSTable的静态布局结构，我们曾在“LevelDB剖析之二：整体架构”中说过，SSTable文件形成了不同Level的层级结构，至于这个层级结构是如何形成的我们放在后面Compaction一节细说。本节主要介绍SSTable某个文件的物理布局和逻辑布局结构，这对了解LevelDB的运行过程很有帮助。<br>LevelDB同层级有很多SSTable文件（以后缀.sst为特征），所有.sst文件内部布局都是一样的。上节介绍Log文件是物理分块的，SSTable也一样会将文件划分为固定大小的物理存储块，但是两者逻辑布局大不相同，根本原因是：Log文件中的记录是Key无序的，即先后记录的key大小没有明确大小关系，而.sst文件内部则是根据记录的Key由小到大排列的，从下面介绍的SSTable布局可以体会到Key有序是为何如此设计.sst文件结构的关键。<br><img src="http://i.imgur.com/zM7RpqK.png" alt=""><br>图4.1 .sst文件的分块结构  </p><p>图4.1展示了一个.sst文件的物理划分结构，同Log文件一样，也是划分为固定大小的存储块，每个Block分为三个部分，红色部分是数据存储区， 蓝色的Type区用于标识数据存储区是否采用了数据压缩算法（Snappy压缩或者无压缩两种），CRC部分则是数据校验码，用于判别数据是否在生成和传输中出错。<br>以上是.sst的物理布局，下面介绍.sst文件的逻辑布局，所谓逻辑布局，就是说尽管大家都是物理块，但是每一块存储什么内容，内部又有什么结构等。图4.2展示了.sst文件的内部逻辑解释。<br><img src="http://i.imgur.com/IOpRF3E.png" alt=""><br>图4.2 逻辑布局   </p><p>从图4.2可以看出，从大的方面，可以将.sst文件划分为数据存储区和数据管理区，数据存储区存放实际的Key:Value数据，数据管理区则提供一些索引指针等管理数据，目的是更快速便捷的查找相应的记录。两个区域都是在上述的分块基础上的，就是说文件的前面若干块实际存储KV数据，后面数据管理区存储管理数据。管理数据又分为四种不同类型：紫色的Meta Block，红色的MetaBlock 索引和蓝色的数据索引块以及一个文件尾部块。<br>LevelDB 1.2版对于Meta Block尚无实际使用，只是保留了一个接口，估计会在后续版本中加入内容，下面我们看看数据索引区和文件尾部Footer的内部结构。<br><img src="http://i.imgur.com/I141rvW.png" alt=""><br>图4.3 数据索引   </p><p>图4.3是数据索引的内部结构示意图。再次强调一下，Data Block内的KV记录是按照Key由小到大排列的，数据索引区的每条记录是对某个Data Block建立的索引信息，每条索引信息包含三个内容，以图4.3所示的数据块i的索引Index i来说：红色部分的第一个字段记载大于等于数据块i中最大的Key值的那个Key，第二个字段指出数据块i在.sst文件中的起始位置，第三个字段指出Data Block i的大小（有时候是有数据压缩的）。后面两个字段好理解，是用于定位数据块在文件中的位置的，第一个字段需要详细解释一下，在索引里保存的这个Key值未必一定是某条记录的Key,以图4.3的例子来说，假设数据块i 的最小Key=“samecity”，最大Key=“the best”;数据块i+1的最小Key=“the fox”,最大Key=“zoo”,那么对于数据块i的索引Index i来说，其第一个字段记载大于等于数据块i的最大Key(“the best”)同时要小于数据块i+1的最小Key(“the fox”)，所以例子中Index i的第一个字段是：“the c”，这个是满足要求的；而Index i+1的第一个字段则是“zoo”，即数据块i+1的最大Key。<br>metaindex_handle指出了metaindex block的起始位置和大小；index、_handle指出了index Block的起始地址和大小；这两个字段可以理解为索引的索引，是为了正确读出索引值而设立的，后面跟着一个填充区和魔数。<br>上面主要介绍的是数据管理区的内部结构，下面我们看看数据区的一个Block的数据部分内部是如何布局的（图4.1中的红色部分），图4.4是其内部布局示意图。<br><img src="http://i.imgur.com/UB90CaD.png" alt=""><br>图4.4 数据Block内部结构    </p><p>从图中可以看出，其内部也分为两个部分，前面是一个个KV记录，其顺序是根据Key值由小到大排列的，在Block尾部则是一些“重启点”（Restart Point）,其实是一些指针，指出Block内容中的一些记录位置。<br>“重启点”是干什么的呢？我们一再强调，Block内容里的KV记录是按照Key大小有序的，这样的话，相邻的两条记录很可能Key部分存在重叠，比如key i=“the Car”，Key i+1=“the color”,那么两者存在重叠部分“the c”，为了减少Key的存储量，Key i+1可以只存储和上一条Key不同的部分“olor”，两者的共同部分从Key i中可以获得。记录的Key在Block内容部分就是这么存储的，主要目的是减少存储开销。“重启点”的意思是：在这条记录开始，不再采取只记载不同的Key部分，而是重新记录所有的Key值，假设Key i+1是一个重启点，那么Key里面会完整存储“the color”，而不是采用简略的“olor”方式。Block尾部就是指出哪些记录是这些重启点的。<br><img src="http://i.imgur.com/fApkG8i.png" alt=""><br>图4.5 记录格式   </p><p>在Block内容区，每个KV记录的内部结构是怎样的？图4.5给出了其详细结构，每个记录包含5个字段：key共享长度，比如上面的“olor”记录， 其key和上一条记录共享的Key部分长度是“the c”的长度，即5；key非共享长度，对于“olor”来说，是4；value长度指出Key:Value中Value的长度，在后面的Value内容字段中存储实际的Value值；而key非共享内容则实际存储“olor”这个Key字符串。<br>上面讲的这些就是.sst文件的全部内部奥秘。</p><h3 id="LevelDB剖析之五：MemTable详解"><a href="#LevelDB剖析之五：MemTable详解" class="headerlink" title="LevelDB剖析之五：MemTable详解"></a>LevelDB剖析之五：MemTable详解</h3><p>LevelDB剖析前述小节大致讲述了磁盘文件相关的重要静态结构，本小节讲述内存中的数据结构Memtable，Memtable在整个体系中的重要地位也不言而喻。总体而言，所有KV数据都是存储在Memtable，Immutable Memtable和SSTable中的，Immutable Memtable从结构上讲和Memtable是完全一样的，区别仅仅在于其是只读的，不允许写入操作，而Memtable则是允许写入和读取的。当Memtable写入的数据占用内存到达指定数量，则自动转换为Immutable Memtable，等待Dump到磁盘中，系统会自动生成新的Memtable供写操作写入新数据，理解了Memtable，那么Immutable Memtable自然不在话下。<br>LevelDB的MemTable提供了将KV数据写入，删除以及读取KV记录的操作接口，但是事实上Memtable并不存在真正的删除操作,删除某个Key的Value在Memtable内是作为插入一条记录实施的，但是会打上一个Key的删除标记，真正的删除操作是Lazy的，会在以后的Compaction过程中去掉这个KV。<br>需要注意的是，LevelDB的Memtable中KV对是根据Key大小有序存储的，在系统插入新的KV时，LevelDB要把这个KV插到合适的位置上以保持这种Key有序性。其实，LevelDB的Memtable类只是一个接口类，真正的操作是通过背后的SkipList来做的，包括插入操作和读取操作等，所以Memtable的核心数据结构是一个SkipList。<br>SkipList是平衡树的一种替代数据结构，但是和红黑树不相同的是，SkipList对于树的平衡的实现是基于一种随机化的算法的，这样也就是说SkipList的插入和删除的工作是比较简单的。LevelDB的SkipList基本上是一个具体实现，并无特殊之处。<br>SkipList不仅是维护有序数据的一个简单实现，而且相比较平衡树来说，在插入数据的时候可以避免频繁的树节点调整操作，所以写入效率是很高的，LevelDB整体而言是个高写入系统，SkipList在其中应该也起到了很重要的作用。Redis为了加快插入操作，也使用了SkipList来作为内部实现数据结构。 </p><h3 id="LevelDB剖析之六：写入与删除记录"><a href="#LevelDB剖析之六：写入与删除记录" class="headerlink" title="LevelDB剖析之六：写入与删除记录"></a>LevelDB剖析之六：写入与删除记录</h3><p>在之前的五节LevelDB剖析中，我们介绍了LevelDB的一些静态文件及其详细布局，从本节开始，我们看看LevelDB的一些动态操作，比如读写记录，Compaction，错误恢复等操作。<br>本节介绍levelDB的记录更新操作，即插入一条KV记录或者删除一条KV记录。levelDB的更新操作速度是非常快的，源于其内部机制决定了这种更新操作的简单性。<br><img src="http://i.imgur.com/buYgI4P.png" alt=""><br>图6.1 LevelDB写入记录 </p><p>图6.1是levelDB如何更新KV数据的示意图，从图中可以看出，对于一个插入操作Put(Key,Value)来说，完成插入操作包含两个具体步骤：首先是将这条KV记录以顺序写的方式追加到之前介绍过的log文件末尾，因为尽管这是一个磁盘读写操作，但是文件的顺序追加写入效率是很高的，所以并不会导致写入速度的降低；第二个步骤是:如果写入log文件成功，那么将这条KV记录插入内存中的Memtable中，前面介绍过，Memtable只是一层封装，其内部其实是一个Key有序的SkipList列表，插入一条新记录的过程也很简单，即先查找合适的插入位置，然后修改相应的链接指针将新记录插入即可。完成这一步，写入记录就算完成了，所以一个插入记录操作涉及一次磁盘文件追加写和内存SkipList插入操作，这是为何levelDB写入速度如此高效的根本原因。<br>从上面的介绍过程中也可以看出：log文件内是key无序的，而Memtable中是key有序的。那么如果是删除一条KV记录呢？对于levelDB来说，并不存在立即删除的操作，而是与插入操作相同的，区别是，插入操作插入的是Key:Value 值，而删除操作插入的是“Key:删除标记”，并不真正去删除记录，而是后台Compaction的时候才去做真正的删除操作。<br>levelDB的写入操作就是如此简单。真正的麻烦在后面将要介绍的读取操作中。 </p><h3 id="LevelDB剖析之七：读取记录"><a href="#LevelDB剖析之七：读取记录" class="headerlink" title="LevelDB剖析之七：读取记录"></a>LevelDB剖析之七：读取记录</h3><p>LevelDB是针对大规模Key/Value数据的单机存储库，从应用的角度来看，LevelDB就是一个存储工具。而作为称职的存储工具，常见的调用接口无非是新增KV，删除KV，读取KV，更新Key对应的Value值这么几种操作。LevelDB的接口没有直接支持更新操作的接口，如果需要更新某个Key的Value,你可以选择直接生猛地插入新的KV，保持Key相同，这样系统内的key对应的value就会被更新；或者你可以先删除旧的KV， 之后再插入新的KV，这样比较委婉地完成KV的更新操作。<br>假设应用提交一个Key值，下面我们看看LevelDB是如何从存储的数据中读出其对应的Value值的。图7-1是LevelDB读取过程的整体示意图。<br><img src="http://i.imgur.com/FF4f9KB.png" alt=""><br>图7-1  LevelDB读取记录流程  </p><p>LevelDB首先会去查看内存中的Memtable，如果Memtable中包含key及其对应的value，则返回value值即可；如果在Memtable没有读到key，则接下来到同样处于内存中的Immutable Memtable中去读取，类似地，如果读到就返回，若是没有读到,那么只能万般无奈下从磁盘中的大量SSTable文件中查找。因为SSTable数量较多，而且分成多个Level，所以在SSTable中读数据是相当蜿蜒曲折的一段旅程。总的读取原则是这样的：首先从属于level 0的文件中查找，如果找到则返回对应的value值，如果没有找到那么到level 1中的文件中去找，如此循环往复，直到在某层SSTable文件中找到这个key对应的value为止（或者查到最高level，查找失败，说明整个系统中不存在这个Key)。<br>那么为什么是从Memtable到Immutable Memtable，再从Immutable Memtable到文件，而文件中为何是从低level到高level这么一个查询路径呢？道理何在？之所以选择这么个查询路径，是因为从信息的更新时间来说，很明显Memtable存储的是最新鲜的KV对；Immutable Memtable中存储的KV数据对的新鲜程度次之；而所有SSTable文件中的KV数据新鲜程度一定不如内存中的Memtable和Immutable Memtable的。对于SSTable文件来说，如果同时在level L和Level L+1找到同一个key，level L的信息一定比level L+1的要新。也就是说，上面列出的查找路径就是按照数据新鲜程度排列出来的，越新鲜的越先查找。<br>为啥要优先查找新鲜的数据呢？这个道理不言而喻，举个例子。比如我们先往levelDB里面插入一条数据 {key=”<a href="http://www.samecity.com&quot;" target="_blank" rel="noopener">www.samecity.com&quot;</a>  value=”我们”},过了几天，samecity网站改名为：69同城，此时我们插入数据{key=”<a href="http://www.samecity.com&quot;" target="_blank" rel="noopener">www.samecity.com&quot;</a>  value=”69同城”}，同样的key,不同的value；逻辑上理解好像levelDB中只有一个存储记录，即第二个记录，但是在levelDB中很可能存在两条记录，即上面的两个记录都在levelDB中存储了，此时如果用户查询key=”<a href="http://www.samecity.com&quot;,我们当然希望找到最新的更新记录，也就是第二个记录返回，这就是为何要优先查找新鲜数据的原因。" target="_blank" rel="noopener">www.samecity.com&quot;,我们当然希望找到最新的更新记录，也就是第二个记录返回，这就是为何要优先查找新鲜数据的原因。</a><br>前文有讲：对于SSTable文件来说，如果同时在level L和Level L+1找到同一个key，level L的信息一定比level L+1的要新。这是一个结论，理论上需要一个证明过程，否则会招致如下的问题：为神马呢？从道理上讲呢，很明白：因为Level L+1的数据不是从石头缝里蹦出来的，也不是做梦梦到的，那它是从哪里来的？Level L+1的数据是从Level L 经过Compaction后得到的（如果您不知道什么是Compaction，那么……..也许以后会知道的），也就是说，您看到的现在的Level L+1层的SSTable数据是从原来的Level L中来的，现在的Level L比原来的Level L数据要新鲜，所以可证，现在的Level L比现在的Level L+1的数据要新鲜。<br>SSTable文件很多，如何快速地找到key对应的value值？在LevelDB中，level 0一直都爱搞特殊化，在level 0和其它level中查找某个key的过程是不一样的。因为level 0下的不同文件可能key的范围有重叠，某个要查询的key有可能多个文件都包含，这样的话LevelDB的策略是先找出level 0中哪些文件包含这个key（manifest文件中记载了level和对应的文件及文件里key的范围信息，LevelDB在内存中保留这种映射表）， 之后按照文件的新鲜程度排序，新的文件排在前面，之后依次查找，读出key对应的value。而如果是非level 0的话，因为这个level的文件之间key是不重叠的，所以只从一个文件就可以找到key对应的value。<br>最后一个问题,如果给定一个要查询的key和某个key range包含这个key的SSTable文件，那么levelDB是如何进行具体查找过程的呢？levelDB一般会先在内存中的Cache中查找是否包含这个文件的缓存记录，如果包含，则从缓存中读取；如果不包含，则打开SSTable文件，同时将这个文件的索引部分加载到内存中并放入Cache中。 这样Cache里面就有了这个SSTable的缓存项，但是只有索引部分在内存中，之后levelDB根据索引可以定位到哪个内容Block会包含这条key，从文件中读出这个Block的内容，在根据记录一一比较，如果找到则返回结果，如果没有找到，那么说明这个level的SSTable文件并不包含这个key，所以到下一级别的SSTable中去查找。<br>从之前介绍的LevelDB的写操作和这里介绍的读操作可以看出，相对写操作，读操作处理起来要复杂很多，所以写的速度必然要远远高于读数据的速度，也就是说，LevelDB比较适合写操作多于读操作的应用场合。而如果应用是很多读操作类型的，那么顺序读取效率会比较高，因为这样大部分内容都会在缓存中找到，尽可能避免大量的随机读取操作。     </p><h3 id="LevelDB剖析之八：Compaction操作"><a href="#LevelDB剖析之八：Compaction操作" class="headerlink" title="LevelDB剖析之八：Compaction操作"></a>LevelDB剖析之八：Compaction操作</h3><p>前文有述，对于LevelDB来说，写入记录操作很简单，删除记录仅仅写入一个删除标记就算完事，但是读取记录比较复杂，需要在内存以及各个层级文件中依照新鲜程度依次查找，代价很高。为了加快读取速度，levelDB采取了compaction的方式来对已有的记录进行整理压缩，通过这种方式，来删除掉一些不再有效的KV数据，减小数据规模，减少文件数量等。<br>levelDB的compaction机制和过程与Bigtable所讲述的是基本一致的，Bigtable中讲到三种类型的compaction: minor ，major和full。所谓minor Compaction，就是把memtable中的数据导出到SSTable文件中；major compaction就是合并不同层级的SSTable文件，而full compaction就是将所有SSTable进行合并。<br>LevelDB包含其中两种，minor和major。先来看看minor Compaction的过程。minor compaction 的目的是当内存中的memtable大小到了一定值时，将内容保存到磁盘文件中，图8.1是其机理示意图。<br><img src="http://i.imgur.com/9nVKQV2.png" alt=""><br>图8.1 minor compaction    </p><p>从8.1可以看出，当memtable数量到了一定程度会转换为immutable memtable，此时不能往其中写入记录，只能从中读取KV内容。之前介绍过，immutable memtable其实是一个多层级队列SkipList，其中的记录是根据key有序排列的。所以这个minor compaction实现起来也很简单，就是按照immutable memtable中记录由小到大遍历，并依次写入一个level 0 的新建SSTable文件中，写完后建立文件的index 数据，这样就完成了一次minor compaction。从图中也可以看出，对于被删除的记录，在minor compaction过程中并不真正删除这个记录，原因也很简单，这里只知道要删掉key记录，但是这个KV数据在哪里?那需要复杂的查找，所以在minor compaction的时候并不做删除，只是将这个key作为一个记录写入文件中，至于真正的删除操作，在以后更高层级的compaction中会去做。<br>当某个level下的SSTable文件数目超过一定设置值后，levelDB会从这个level的SSTable中选择一个文件（level&gt;0），将其和高一层级的level+1的SSTable文件合并，这就是major compaction。<br>我们知道在大于0的层级中，每个SSTable文件内的Key都是由小到大有序存储的，而且不同文件之间的key范围（文件内最小key和最大key之间）不会有任何重叠。Level 0的SSTable文件有些特殊，尽管每个文件也是根据Key由小到大排列，但是因为level 0的文件是通过minor compaction直接生成的，所以任意两个level 0下的两个sstable文件可能再key范围上有重叠。所以在做major compaction的时候，对于大于level 0的层级，选择其中一个文件就行，但是对于level 0来说，指定某个文件后，本level中很可能有其他SSTable文件的key范围和这个文件有重叠，这种情况下，要找出所有有重叠的文件和level 1的文件进行合并，即level 0在进行文件选择的时候，可能会有多个文件参与major compaction。<br>LevelDB在选定某个level进行compaction后，还要选择是具体哪个文件要进行compaction，LevelDB在这里有个小技巧， 就是说轮流来，比如这次是文件A进行compaction，那么下次就是在key range上紧挨着文件A的文件B进行compaction，这样每个文件都会有机会轮流和高层的level 文件进行合并。<br>如果选好了level L的文件A和level L+1层的文件进行合并，那么问题又来了，应该选择level L+1哪些文件进行合并？LevelDB选择L+1层中和文件A在key range上有重叠的所有文件来和文件A进行合并。<br>也就是说，选定了level L的文件A,之后在level L+1中找到了所有需要合并的文件B,C,D…等等。剩下的问题就是具体是如何进行major合并的？就是说给定了一系列文件，每个文件内部是key有序的，如何对这些文件进行合并，使得新生成的文件仍然Key有序，同时抛掉哪些不再有价值的KV数据。<br>图8.2说明了这一过程。<br><img src="http://i.imgur.com/abldSaG.png" alt=""><br>图8.2 SSTable Compaction </p><p>Major compaction的过程如下：对多个文件采用多路归并排序的方式，依次找出其中最小的Key记录，也就是对多个文件中的所有记录重新进行排序。之后采取一定的标准判断这个Key是否还需要保存，如果判断没有保存价值，那么直接抛掉，如果觉得还需要继续保存，那么就将其写入level L+1层中新生成的一个SSTable文件中。就这样对KV数据一一处理，形成了一系列新的L+1层数据文件，之前的L层文件和L+1层参与compaction 的文件数据此时已经没有意义了，所以全部删除。这样就完成了L层和L+1层文件记录的合并过程。<br>那么在major compaction过程中，判断一个KV记录是否抛弃的标准是什么呢？其中一个标准是:对于某个key来说，如果在小于L层中存在这个Key，那么这个KV在major compaction过程中可以抛掉。因为我们前面分析过，对于层级低于L的文件中如果存在同一Key的记录，那么说明对于Key来说，有更新鲜的Value存在，那么过去的Value就等于没有意义了，所以可以删除。</p><p>##LevelDB剖析之九：LevelDB中的Cache<br>书接前文，前面讲过对于LevelDB来说，读取操作如果没有在内存的memtable中找到记录，要多次进行磁盘访问操作。假设最优情况，即第一次就在level 0中最新的文件中找到了这个key，那么也需要读取2次磁盘，一次是将SSTable的文件中的index部分读入内存，这样根据这个index可以确定key是在哪个block中存储；第二次是读入这个block的内容，然后在内存中查找key对应的value。<br>LevelDB中引入了两个不同的Cache:Table Cache和Block Cache。其中Block Cache是配置可选的，即在配置文件中指定是否打开这个功能。<br><img src="http://i.imgur.com/8ZUqBxO.png" alt=""><br>图9.1 table cache </p><p>图9.1是table cache的结构。在Cache中，key值是SSTable的文件名称，Value部分包含两部分，一个是指向磁盘打开的SSTable文件的文件指针，这是为了方便读取内容；另外一个是指向内存中这个SSTable文件对应的Table结构指针，table结构在内存中，保存了SSTable的index内容以及用来指示block cache用的cache_id ,当然除此外还有其它一些内容。<br>比如在get(key)读取操作中，如果LevelDB确定了key在某个level下某个文件A的key range范围内，那么需要判断是不是文件A真的包含这个KV。此时，LevelDB会首先查找Table Cache，看这个文件是否在缓存里，如果找到了，那么根据index部分就可以查找是哪个block包含这个key。如果没有在缓存中找到文件，那么打开SSTable文件，将其index部分读入内存，然后插入Cache里面，去index里面定位哪个block包含这个Key 。如果确定了文件哪个block包含这个key，那么需要读入block内容，这是第二次读取。<br>Block Cache是为了加快这个过程的。其中的key是文件的cache_id加上这个block在文件中的起始位置block_offset。而value则是这个Block的内容。<br>如果LevelDB发现这个block在block cache中，那么可以避免读取数据，直接在cache里的block内容里面查找key的value就行，如果没找到呢？那么读入block内容并把它插入block cache中。LevelDB就是这样通过两个cache来加快读取速度的。从这里可以看出，如果读取的数据局部性比较好，也就是说要读的数据大部分在cache里面都能读到，那么读取效率应该还是很高的，而如果是对key进行顺序读取效率也应该不错，因为一次读入后可以多次被复用。但是如果是随机读取，您可以推断下其效率如何。     </p><h3 id="LevelDB剖析之十：Version、VersionEdit、VersionSet"><a href="#LevelDB剖析之十：Version、VersionEdit、VersionSet" class="headerlink" title="LevelDB剖析之十：Version、VersionEdit、VersionSet"></a>LevelDB剖析之十：Version、VersionEdit、VersionSet</h3><p>Version保存了当前磁盘以及内存中所有的文件信息，一般只有一个Version叫做”current” version（当前版本）。LevelDB还保存了一系列的历史版本，这些历史版本有什么作用呢？<br>当一个Iterator创建后，Iterator就引用到了current version(当前版本)，只要这个Iterator不被delete那么被Iterator引用的版本就会一直存活。这就意味着当你用完一个Iterator后，需要及时删除它。<br>当一次Compaction结束后（会生成新的文件，合并前的文件需要删除），LevelDB会创建一个新的版本作为当前版本，原先的当前版本就会变为历史版本。<br>VersionSet是所有Version的集合，管理着所有存活的Version。<br>VersionEdit 表示Version之间的变化，相当于delta 增量，表示有增加了多少文件，删除了文件。他们之间的关系为：Version0 +VersionEdit–&gt;Version1。VersionEdit会保存到MANIFEST文件中，当做数据恢复时就会从MANIFEST文件中读出来重建数据。<br>LevelDB的这种版本的控制，让我想到了双buffer切换，双buffer切换来自于图形学中，用于解决屏幕绘制时的闪屏问题，在服务器编程中也有用处。比如我们的服务器上有一个字典库，每天我们需要更新这个字典库，我们可以新开一个buffer，将新的字典库加载到这个新buffer中，等到加载完毕，将字典的指针指向新的字典库。<br>LevelDB的version管理和双buffer切换类似，但是如果原version被某个iterator引用，那么这个version会一直保持，直到没有被任何一个iterator引用，此时就可以删除这个version。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;LevelDB是能够处理十亿级别规模Key-Value型数据持久性存储的C++程序库，是谷歌两位大牛Jeff Dean和Sanjay Ghemawat发起的开源项目。自己已经将拜读LevelDB纳入了项目完成后的学习计划。最近偶然看到一篇关于LevelDB很好的博客，特记录下来，希望对后面的学习有帮助。&lt;br&gt;
    
    </summary>
    
    
      <category term="LevelDB" scheme="http://blog.jonnydu.me/tags/LevelDB/"/>
    
  </entry>
  
  <entry>
    <title>CentOS配置双网卡（Mellanox和传统以太网卡）</title>
    <link href="http://blog.jonnydu.me/2017/03/20/config-two-network-interfaces/"/>
    <id>http://blog.jonnydu.me/2017/03/20/config-two-network-interfaces/</id>
    <published>2017-03-20T09:57:25.000Z</published>
    <updated>2017-05-09T04:52:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>刚给服务器装上了操作系统，期待已久的Mellanox网卡终于到了，关于该网卡这里就不做过多的介绍了，可以参考<a href="http://www.mellanox.com/" target="_blank" rel="noopener">Mellanox官网</a>，下面记录在服务器上Mellanox网卡和传统以太网的配置。<br><a id="more"></a></p><h3 id="安插Mellanox网卡并安装相应驱动"><a href="#安插Mellanox网卡并安装相应驱动" class="headerlink" title="安插Mellanox网卡并安装相应驱动"></a>安插Mellanox网卡并安装相应驱动</h3><p>首先，在服务器背板安插Mellanox网卡，然后执行<code>ifconfig</code>查看是否多了一张新的网卡。初始状态下<code>ifconfig</code>看到的Mellanox网卡的名字是ib0，这说明该网卡的默认链路层协议时InfiniBand协议，而不是Ethernet协议。由于我们后续需要使用NVMe over RoCEv2，所以需要将Mellanox的链路层协议改为Ethernet。因此，需要先下载并安装相应的驱动。<br>在官网上确定好操作系统对应的驱动版本号（比如CentOS7.2对应的版本是MLNX_OFED-3.4-2.0.0.0-rhel7.2-x86_64）后，下载并安装驱动。</p><pre><code>wget http://www.mellanox.com/downloads/ofed/MLNX_OFED-3.4-2.0.0.0/MLNX_OFED_LINUX-3.4-2.0.0.0-rhel7.2-x86_64.tgztar -xvf MLNX_OFED_LINUX-3.4-2.0.0.0-rhel7.2-x86_64.tgzcd MLNX_OFED_LINUX-3.4-2.0.0.0-rhel7.2-x86_64/./mlnxofedinstall  (--add-kernel-support)       ###kernel 4.8.15可能需要添加特定的选项/etc/init.d/openibd restart</code></pre><h3 id="修改Mellanox网卡配置"><a href="#修改Mellanox网卡配置" class="headerlink" title="修改Mellanox网卡配置"></a>修改Mellanox网卡配置</h3><p>如前文所述，Mellanox网卡的默认链路层协议是Infiniband协议，不是Ethernet协议。所以，接下来修改网卡的链路层协议。<br>在修改之前，可以先通过<code>lspci | grep Mellanox</code>查看网卡信息，确定是否为InfiniBand协议。</p><pre><code>mst startibv_devinfo | grep vendor_part_id    ##得到vendor_part_id(我这里是4115)mlxconfig -d /dev/mst/mt4115_pciconf0 q  ##再次查询网卡信息mlxconfig -d /dev/mst/mt4115_pciconf0 set LINK_TYPE_P1=2  ##修改为Ethernet协议</code></pre><p>修改并重新启动后，可以看到网卡的名称发生了变化（这里是enp133s0）。最后，使用<code>lspci | grep Mellanox</code>确认修改。</p><h3 id="CentOS双网卡配置"><a href="#CentOS双网卡配置" class="headerlink" title="CentOS双网卡配置"></a>CentOS双网卡配置</h3><p>现在服务器的网卡连接情况是一张Mellanox网卡通过专用网线连接到EdgeCore 100G白牌交换机上，另外一张传统以太网卡连接到以太网交换机（192.168.1.1）。接下来双网卡配置双IP，一个192.168.1.x，可通过以太网交换机连接外网，另一个192.168.5.x，连接到100G白牌交换机。   </p><p>以太网卡在服务器上对应的名称为eno1。修改其配置文件。</p><pre><code>vim /etc/sysconfig/network-scripts/ifcfg-eth0</code></pre><p>修改和添加以下内容：</p><pre><code>BOOTPROTO=staticHWADDR=6c:92:bf:42:27:2e    #mac addressIPADDR=192.168.1.152NETMASK=255.255.255.0GATEWAY=192.168.1.1ONBOOT=yes</code></pre><p>修改后的配置文件如下图所示。<br><img src="http://i.imgur.com/WqcbomA.png" alt=""><br>通过<code>service network restart</code>重启网络后，就成功的将该以太网卡设置IP设置成了静态IP（192.168.1.152）。</p><p>Mellanox网卡的配置和以太网卡一样。所以，这里只列举出修改后的配置文件（/etc/sysconfig/network-scripts/ifcfg-enp133s0）<br><img src="http://i.imgur.com/jcLC6h9.png" alt=""></p><p>配置好后的服务器网络接口情况如下图所示。<br><img src="http://i.imgur.com/rNo8Xe3.png" alt=""></p><h3 id="配置路由表"><a href="#配置路由表" class="headerlink" title="配置路由表"></a>配置路由表</h3><p>目前系统默认网关是192.168.1.1，所以需要增加两个路由表，实现双网关正常访问。<br>    vim /etc/iproute2/rt_tables<br>增加两行内容：<br>    252 net2<br>    251 net3<br>在/etc/rc.local添加静态路由规则。</p><pre><code>ip route flush table net2ip route add default via 192.168.1.1 dev eno1 src 192.168.1.152 table net2ip rule add from 192.168.1.152 table net2ip route flush table net3ip route add default via 192.168.5.1 dev enp133s0 src 192.168.5.86 table net3ip rule add from 192.168.5.86 table net3</code></pre><p>这时，双网卡双IP应该就配置好了，<code>service network restart</code>重启网络就可以实现不同网卡对应不同网络访问。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;刚给服务器装上了操作系统，期待已久的Mellanox网卡终于到了，关于该网卡这里就不做过多的介绍了，可以参考&lt;a href=&quot;http://www.mellanox.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Mellanox官网&lt;/a&gt;，下面记录在服务器上Mellanox网卡和传统以太网的配置。&lt;br&gt;
    
    </summary>
    
    
      <category term="network/linux" scheme="http://blog.jonnydu.me/tags/network-linux/"/>
    
      <category term="NVMe over RDMA" scheme="http://blog.jonnydu.me/tags/NVMe-over-RDMA/"/>
    
  </entry>
  
  <entry>
    <title>RDMA技术浅析</title>
    <link href="http://blog.jonnydu.me/2017/02/27/RDMA/"/>
    <id>http://blog.jonnydu.me/2017/02/27/RDMA/</id>
    <published>2017-02-27T02:22:39.000Z</published>
    <updated>2017-05-05T12:11:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>RDMA，即Remote DMA，最直观的解释就是将发生在本机的直接内存访问扩展到主机与主机之间。<br><a id="more"></a></p><h3 id="DMA"><a href="#DMA" class="headerlink" title="DMA"></a>DMA</h3><p>首先，对DMA技术做简单的复习和总结。<br>在最初的PC体系结构中，CPU是系统中唯一的总线主控器，也就是说，为了提取和存储RAM存储单元的值，CPU是唯一可以驱动地址/数据总线的硬件设备。而随着更多诸如PCI的现代总线体系结构的出现，如果提供合适的电路，每一个外围设备都可以充当总线主控器。因此，现在所有的PC都包含一个辅助的DMA电路，它可以用来控制在RAM和IO设备之间数据的传送。DMA一旦被CPU激活，就可以自行传送数据；在数据传输完成之后，DMA发出一个中断请求，再由CPU接管。当CPU和DMA同时访问同一内存单元时，所产生的的冲突由一个名为内存仲裁器的硬件电路解决。<br>由于DMA的设置时间比较长，所以使用DMA最多的是磁盘驱动器和其他需要一次传送大量字节的设备，而在传送数量很少的数据时直接使用CPU效率更高。   </p><h3 id="RDMA"><a href="#RDMA" class="headerlink" title="RDMA"></a>RDMA</h3><p>传统的TCP/IP技术在数据包处理过程中，要经过操作系统及其他软件层，需要占用大量的服务器资源和内存总线带宽，数据在系统内存、处理器缓存和网络控制器缓存之间来回进行复制移动，给服务器的CPU和内存造成了沉重负担。尤其是网络带宽、处理器速度与内存带宽三者的严重”不匹配性”，更加剧了网络延迟效应。<br>RDMA是一种新的内存访问技术，RDMA让计算机可以直接存取其他计算机的内存，而不需要经过处理器耗时的处理。RDMA将数据从一个系统快速移动到远程系统存储器中，而不对操作系统造成任何影响。<br>RDMA技术的原理及其与TCP/IP架构的对比如下图所示。<br><img src="http://i.imgur.com/G5E7u7t.png" alt="">  </p><p>因此，RDMA可以简单理解为利用相关的硬件和网络技术，服务器1的网卡可以直接读写服务器2的内存，最终达到高带宽、低延迟和低资源利用率的效果。如下图所示，应用程序不需要参与数据传输过程，只需要指定内存读写地址，开启传输并等待传输完成即可。<br><img src="http://i.imgur.com/cFfY963.jpg" alt=""></p><p>在实现上，RDMA实际上是一种智能网卡与软件架构充分优化的远端内存直接高速访问技术，通过在网卡上将RDMA协议固化于硬件，以及支持零复制网络技术和内核内存旁路技术这两种途径来达到其高性能的远程直接数据存取的目标。<br>（1）零复制：零复制网络技术使网卡可以直接与应用内存相互传输数据，从而消除了在应用内存与内核之间复制数据的需要。因此，传输延迟会显著减小。<br>（2）内核旁路：内核协议栈旁路技术使应用程序无需执行内核内存调用就可向网卡发送命令。在不需要任何内核内存参与的条件下，RDMA请求从用户空间发送到本地网卡并通过网络发送给远程网卡，这就减少了在处理网络传输流时内核内存空间与用户空间之间环境切换的次数。</p><p>在具体的远程内存读写中，RDMA操作用于读写操作的远程虚拟内存地址包含在RDMA消息中传送，远程应用程序要做的只是在其本地网卡中注册相应的内存缓冲区。远程节点的CPU除在连接建立、注册调用等之外，在整个RDMA数据传输过程中并不提供服务，因此没有带来任何负载。</p><h3 id="RDMA的不同实现"><a href="#RDMA的不同实现" class="headerlink" title="RDMA的不同实现"></a>RDMA的不同实现</h3><p>如下图所示，RDMA的实现方式主要分为InfiniBand和Ethernet两种传输网络。而在以太网上，又可以根据与以太网融合的协议栈的差异分为iWARP和RoCE（包括RoCEv1和RoCEv2）。<br><img src="http://i.imgur.com/EgvDMTD.png" alt=""><br>其中，InfiniBand是最早实现RDMA的网络协议，被广泛应用到高性能计算中。但是InfiniBand和传统TCP/IP网络的差别非常大，需要专用的硬件设备，承担昂贵的价格。鉴于此，这里不对InfiniBand做过多的讨论。<br>在基于以太网的版本中，下面重点选择RoCEv2来讨论。<br>可以看出，RoCEv2的协议栈包括IB传输层、TCP/UDP、IP和Ethernet，其中，后面三层都使用了TCP/IP中相应层次的封包格式。RoCEv2的封包格式如下图所示。<br><img src="http://i.imgur.com/0KMeGlN.png" alt=""><br>其中，UDP包头中，目的端口号为4791即代表是RoCEv2帧。IB BTH即InfiniBand Base Transport Header，定义了IB传输层的相应头部字段。IB Payload即为消息负载。ICRC和FCS分别对应冗余检测和帧校验。<br>IB BTH格式和字段定义如下图。其中，Opcode用于表明该包的type或IB payload中更高层的协议类型。S是Solicited Event的缩写，表明回应者产生应该产生一个事件。M是MigReq的缩写，一般用于迁移状态。Pad表明有多少额外字节被填充到IB payload中。TVer即Transport Header Version，表明该包的版本号。Partition Key用来表征与本packet关联的逻辑内存分区。rsvd是reserved的缩写，该字段是保留的。Destination QP表明目的端Queue Pair序号。A是Acknowledge Request，表示该packet的应答可由响应者调度。PSN是Packet Sequence Number，用来检测丢失或重复的数据包。<br><img src="http://i.imgur.com/uPjVonk.png" alt=""><br>最后，顺带说下RDMA网卡的出包。如前文所述，RDMA是一种智能网卡与软件架构充分优化的远端内存直接高速访问技术，通过将RDMA技术固化于网卡上实现，即，在RoCEv2协议栈中，IB BTH、UDP、IP以及Ethernet Layer全是固化在网卡上的。用户空间的Application通过OFA Stack（亦或其他组织编写的RDMA stack）提供的verbs编程接口（比如WRITE、READ、SEND等）形成IB payload，接下来便直接进入硬件，由RDMA网卡实现负载的层层封装。</p><h3 id="附"><a href="#附" class="headerlink" title="附"></a>附</h3><p><img src="http://i.imgur.com/vwXu2Zm.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;RDMA，即Remote DMA，最直观的解释就是将发生在本机的直接内存访问扩展到主机与主机之间。&lt;br&gt;
    
    </summary>
    
    
      <category term="NVMe over RDMA" scheme="http://blog.jonnydu.me/tags/NVMe-over-RDMA/"/>
    
  </entry>
  
  <entry>
    <title>NVMe技术浅析</title>
    <link href="http://blog.jonnydu.me/2017/02/17/NVMe/"/>
    <id>http://blog.jonnydu.me/2017/02/17/NVMe/</id>
    <published>2017-02-17T12:09:40.000Z</published>
    <updated>2017-05-06T06:39:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="http://blog.dujiong.net/2017/02/13/storage/" target="_blank" rel="noopener">存储的那些词儿</a>一文中已经提到了NVMe是使用PCIe通道的一种逻辑设备接口标准（是接口标准，不是接口！），本文将进一步地分析NVMe的设计及其带来的性能提升。<br><a id="more"></a></p><h3 id="NVMe设计"><a href="#NVMe设计" class="headerlink" title="NVMe设计"></a>NVMe设计</h3><p>NVMe指定了Host与SSD之间通信的命令，以及命令执行的流程。NVMe由两种命令组成，一种是Admin Command，用于Host管理和控制SSD；另外一种是I/O Command，用于Host和SSD之间数据的传输。<br>同ATA中定义的命令相比，NVMe的命令个数少了很多，完全就是为SSD量身定制的。这里放一张NVMe1.2支持的IO Command列表。<br><img src="http://i.imgur.com/INLzJpX.png" alt=""><br>除了指定Host与SSD之间的通信命令以外，NVMe还定义了命令的执行流程。NVMe中定义了三个关键组件用于命令和数据的处理：Submission Queue（SQ）、Completion Queue（CQ）和Doorbell Register（DB）。SQ和CQ位于Host的内存中，DB位于SSD的控制器内部。在详细说明NVMe命令的处理流程之前，先上一张图宏观感受下PCIe系统和NVMe的结合。图中的NVMe Subsystem一般就是SSD，作为一个PCIe Endpoint通过PCIe连接着Root Complex（RC），然后RC连接着CPU和内存。<br><img src="http://i.imgur.com/vl7O3dS.png" alt=""><br>下面通过SQ、CQ和DB这三个关键组件来总体认识NVMe是如何处理命令的。SQ位于内存中，Host要发送命令时，先把准备好的命令放在SQ中（每个命令条目大小是64字节），然后通知SSD来取；在通知中发挥作用的就是DB寄存器，Host通过写SSD端的DB寄存器来告知SSD执行命令；此外，CQ也是位于Host内存中，一个命令执行完成，成功或失败，SSD总会往CQ中写入命令完成状态（每个条目是16字节）。<br>下图展示了NVMe一次完整处理指令的流程，共八步：<br>（1）Host将指令写到SQ；<br>（2）Host写DB，通知SSD取指令；<br>（3）SSD收到通知，从SQ中取走指令；<br>（4）SSD执行指令；<br>（5）指令执行完成，SSD往CQ写入指令执行结果；<br>（6）SSD通知Host指令执行完成；<br>（7）Host收到通知，处理CQ，查看指令完成状态；<br>（8）Host处理完CQ中的指令执行结果，通过DB回复SSD。<br><img src="http://i.imgur.com/Z25eGGp.png" alt="">   </p><h4 id="SQ-CQ-DB详细分析"><a href="#SQ-CQ-DB详细分析" class="headerlink" title="SQ/CQ/DB详细分析"></a>SQ/CQ/DB详细分析</h4><p>如前文所述，SQ/CQ/DB是NVMe处理命令的关键。Host往SQ中写入命令，SSD往CQ中写入命令完成结果。NVMe中有两种SQ和CQ，一种是Admin SQ，用于放Admin命令；一种是I/O SQ，放I/O命令。如下图所示，系统中只有一对Admin SQ/CQ，它们是一一对应的关系；I/O SQ/CQ却可以有很多，最大值65535（64K-1）。此外，Host端每个Core可以有一个或多个SQ，但只有一个CQ，即SQ与CQ可以是一对一或多对一的关系。这样设计的原因有二：一是性能需求，一个Core中有多线程，可以做到一个线程独享一个SQ；二是QoS需求，可以对多个SQ设置不同的优先级。<br><img src="http://i.imgur.com/bIIfhg0.png" alt=""><br>此外，作为队列，每个SQ和CQ都有一定的深度，对Admin SQ/CQ来说，其深度可以是2-4096（4K），对I/O SQ/CQ，队列深度可以是2-65536（64K）。<br>综上，NVMe中SQ/CQ的个数可以配置，每个SQ/CQ的深度也可以配置，即NVMe的性能是可以通过配置队列个数和队列深度来灵活调节的。这一点和AHCI相比是巨大的提升，因为我们知道，AHCI只有一个命令队列，且队列深度是固定的32。<br>说了这么多SQ和CQ，DB呢？我们知道，SQ和CQ都是队列（且是环形队列），队列的头尾很重要，头决定谁会最先被服务，尾决定新到来命令的位置。所以，我们需要记录SQ和CQ的头尾位置，这就是DB的作用之一。DB是在SSD端的寄存器，记录SQ和CQ的头尾位置。每个SQ或CQ，都有两个对应的DB：Head DB和Tail DB。<br><img src="http://i.imgur.com/8IzJSna.png" alt=""><br>从上图可以看出，这是一个生产者/消费者模型。对一个SQ来说，它的生产者是Host，因为它往SQ的Tail位置写入命令，消费者是SSD，它从SQ的Head取出指令执行。CQ则刚好相反，生产者是SSD，消费者是Host。<br>DB的另一个作用是通知，Host更新SQ Tail DB的同时，也是在告知SSD有新的命令需要处理；Host更新CQ Head DB的同时，也是在告知SSD返回的命令完成状态信息已经被处理。<br>下面以一个实例来说明SQ/CQ/DB三者配合的详细过程。<br>（1）开始时假设SQ1和CQ1是空的，Head=Tail=0；<br><img src="http://i.imgur.com/L8WPeKE.png" alt=""><br>（2）Host往SQ1中写入三个命令，SQ1的Tail变为3。Host在往SQ1写入三个命令后，同时去更新SSD Controller的SQ1 Tail DB寄存器，值为3。Host更新这个寄存器的同时，也是在告诉SSD Controller，有新命令了。<br><img src="http://i.imgur.com/sRJyKnc.png" alt=""><br>（3）SSD Controller收到通知后，于是派人去SQ1把3个命令都取回来执行。SSD把SQ1的三个命令都消费了，SQ1的Head从而也调整为3，SSD Controller会把这个Head值写入到本地的SQ1 Head DB寄存器。<br><img src="http://i.imgur.com/riHdErF.png" alt=""><br>（4）SSD执行完了两个命令，于是往CQ1中写入两个命令完成信息，同时更新CQ1对应的Tail DB寄存器，值为2。同时，SSD发消息给Host告知有命令完成。<br><img src="http://i.imgur.com/UmkUyR6.png" alt=""><br>（5）Host收到SSD的通知后，从CQ1中取出那两条完成信息处理。待处理完毕，Host往CQ1 Head DB寄存器中写入CQ1的head，值为2。<br><img src="http://i.imgur.com/vd75vaC.png" alt=""><br>这样，就完成了一次完整的命令处理。    </p><h3 id="NVMe总结"><a href="#NVMe总结" class="headerlink" title="NVMe总结"></a>NVMe总结</h3><p>NVMe所带来的重大改进主要包含以下几方面：一是低延迟，低延时和良好的并行性可以让SSD的随机性能大幅提升；其次是支持多队列和更高的队列深度，多队列让CPU的性能得到更好的释放，而队列深度从32提升到最大64K，则大幅提升了SSD的IOPS能力；然后是NVMe的低功耗，其加入了自动功耗状态切换和动态能耗管理功能；最后是其驱动的适用性广，解决了不同PCIe SSD之间的驱动适用性问题。支持NVMe标准的PCIe SSD可适用于多个不同平台，也不需要厂商独立提供驱动支持。目前Windows、Linux、Solaris、Unix、VMware、UEFI等都加入了对NVMe SSD的支持。</p><h3 id="附"><a href="#附" class="headerlink" title="附"></a>附</h3><p>特别鸣谢：<br><a href="http://www.ssdfans.com/" target="_blank" rel="noopener">ssdfans</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在&lt;a href=&quot;http://blog.dujiong.net/2017/02/13/storage/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;存储的那些词儿&lt;/a&gt;一文中已经提到了NVMe是使用PCIe通道的一种逻辑设备接口标准（是接口标准，不是接口！），本文将进一步地分析NVMe的设计及其带来的性能提升。&lt;br&gt;
    
    </summary>
    
    
      <category term="NVMe over RDMA" scheme="http://blog.jonnydu.me/tags/NVMe-over-RDMA/"/>
    
  </entry>
  
  <entry>
    <title>存储的那些词儿</title>
    <link href="http://blog.jonnydu.me/2017/02/13/storage/"/>
    <id>http://blog.jonnydu.me/2017/02/13/storage/</id>
    <published>2017-02-13T07:06:23.000Z</published>
    <updated>2017-05-05T12:08:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>海量数据的迸发和传统存储技术所面临的性能瓶颈，促进了新型存储技术和系统架构的高速发展。<br><a id="more"></a></p><h3 id="存储的那些词儿"><a href="#存储的那些词儿" class="headerlink" title="存储的那些词儿"></a>存储的那些词儿</h3><p>本文简单介绍下在存储领域出现频率最高的几个词语:SATA、PCIe、AHCI和NVMe（Non-Volatile Memory Express，非易失性存储器标准）。</p><h4 id="SATA和PCIe"><a href="#SATA和PCIe" class="headerlink" title="SATA和PCIe"></a>SATA和PCIe</h4><p>SATA和PCIe大家应该比较熟悉，这两个都是总线标准。<br>SATA由IDE/ATA标准发展而来，主要用途是把存储设备连接到主板。SATA的发展主要经历了以下版本：<br>SATA revision 1.0 (1.5 Gbit/s, 150 MB/s)<br>SATA revision 2.0 (3 Gbit/s, 300 MB/s)<br>SATA revision 3.0 (6 Gbit/s, 600 MB/s)<br>SATA revision 3.1<br>SATA revision 3.2 (16 Gbit/s, 1969 MB/s)<br>SATA在发展过程中，考虑了向下兼容的问题，比如主板上SATA-3的接口，可以连接SATA-2的硬盘。但同时，向下兼容也造成了其发展缓慢。<br>出于向下兼容的考虑，SATA可以工作在两种模式：传统模式和AHCI模式。传统模式是为了兼容以前的 IDE/ATA。AHCI模式则比较新，支持SATA独有的功能，如热插拔、原生命令队列（NCQ）等。      </p><p>PCIe是另一种总线标准，由AGP、PCI、PCI-x发展而来，而这些总线的发展，主要的动力是显卡的发展。AGP就是Accelerated Graphics Port（加速图像端口）的缩写。由于显卡需要很大的带宽和速度，PCI总线标准就不断升级来满足要求。当然，除了显卡外，PCI总线还用于其他的扩展卡，如网卡（包括有线网卡、无线网卡、3G/4G卡等）。  </p><h4 id="AHCI和NVMe"><a href="#AHCI和NVMe" class="headerlink" title="AHCI和NVMe"></a>AHCI和NVMe</h4><p>AHCI和NVMe是逻辑（或者说软件、驱动程序）上的标准。<br>从上面 SATA的不同版本可以看到，提速是一个主要任务（当然也有其他的改进）。但进入SSD时代后，SATA的改版速度（由于要考虑向下兼容），已经跟不上传输速度的要求了。这时候，业界就考虑采用PCIe来连接存贮设备。但在驱动程序层面，仍然采用AHCI。这是因为AHCI已经非常成熟，广泛被各种操作系统（如Windows、Linux）所采用。<br>AHCI是为了发挥SATA的潜能而设计的，当时算是“高大上”了。但当时仍然是机械硬盘统治市场，因此AHCI的设计是基于机械硬盘的特性（旋转式磁性盘片）。虽然AHCI也可以用于SSD，但却不能发挥极致。因为SSD更像内存，而不像“盘片”。譬如说，机械硬盘，如果磁头错过了一个扇区，那就得等盘片转一圈回来才能访问。SSD就不存在这个问题。因此，业界重新设计一个新的NVMe协议，希望发挥 SSD的潜能。下面是AHCI和NVMe的对比：<br><img src="http://i.imgur.com/4uOqsG8.png" alt="">   </p><h4 id="物理接口"><a href="#物理接口" class="headerlink" title="物理接口"></a>物理接口</h4><p>说完了总线和协议，下面说说物理接口。无论采用什么总线和协议，主板总得连接到存储设备上。这里所说的物理接口，指是是物理尺寸和形状，电气特征不作讨论。接口分为主机端和设备端，种类繁多，这里挑几个常见的。<br>1.SATA接口。采用这种接口的，只能使用SATA总线，不能使用PCIe总线。大部分2.5”SSD就是这种接口。<br>2.M.2接口。采用这种接口的SSD，可以使用SATA或者PCIe总线（取决于主板和SSD）。如果采用PCIe总线，又分为AHCI和NVMe两种协议。<br>3.SATA Express接口。SATA Express使用的是PCIe总线，向下兼容SATA总线。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>不同存储总线标准/协议之间的组合如下图所示。<br><img src="http://i.imgur.com/J22ZZLp.png" alt=""><br>可以看出，AHCI和NVMe是驱动程序层面的。NVMe只适用于SSD（SSD和主板也要支持NVMe才行）。AHCI则适用于机械硬盘和SSD。<br>在主板芯片层面，有AHCI控制器和PCIe控制器。有趣的是AHCI驱动程序“居然”可以使用PCIe控制器（中间那条橙色的线）。这个其实是个过渡方案，目的是在利用PCIe高带宽的同时，保持对上层软件的兼容性。<br>绿色那个框是主板上的物理接口。注意即使是同样的物理接口，也可以选择不同的总线和协议（如果主机和设备支持的话）。<br>右下角的PCIe SSD设备则可以有两种不同的控制器（最下面的两个框）：AHCI和NVMe。因此，同样是PCIe的SSD，也可以有不同的传输效能。<br>说了这么多，相信大家已经晕了。下面按传输效率做个排序。<br>1.PCIe NVMe<br>这个是最高大上的。在笔记本市场，根据效能，可以再细分为两个等级：<br>（1）M.2尺寸的NVMe（如三星 950 PRO）。可以有四条PCIe通道，速度最快。但由于电路板面积限制，容量和发热都是个问题。<br>（2）2.5″尺寸的NVMe（如东芝XG3），采用SATA Express接口，可以有两条PCIe通道，传输速率较低。此外，由于2.5″体积较大，容量和发热比 M.2 要好。<br>2.PCIe AHCI<br>效能比1稍低，是由于AHCI协议的滞后性决定的。笔记本上只有M.2外形，没有2.5″外形。<br>3.SATA AHCI<br>效能最低，但兼容性最好，根据外形可分为两类。这两类的传输效能是一样的，无分高低。<br>（1）M.2外形的设备，如三星850EVO的M.2盘。<br>（2）2.5″外形的设备，如目前广泛使用的机械硬盘，固态硬盘等。</p><h3 id="附"><a href="#附" class="headerlink" title="附"></a>附</h3><p>接下来将详细分析NVMe的设计及其带来的性能提升。   </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;海量数据的迸发和传统存储技术所面临的性能瓶颈，促进了新型存储技术和系统架构的高速发展。&lt;br&gt;
    
    </summary>
    
    
      <category term="NVMe over RDMA" scheme="http://blog.jonnydu.me/tags/NVMe-over-RDMA/"/>
    
  </entry>
  
  <entry>
    <title>分配排序（桶排序和基数排序）总结</title>
    <link href="http://blog.jonnydu.me/2017/01/20/Distribute-Sort/"/>
    <id>http://blog.jonnydu.me/2017/01/20/Distribute-Sort/</id>
    <published>2017-01-20T11:51:47.000Z</published>
    <updated>2017-04-24T13:52:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>分配排序，是指不需要进行两两之间的比较，而根据记录自己的关键码的分配来进行排序的一类方法，因此，在进行分配排序时，我们通常需要知道记录序列的一些具体情况，比如关键码的分布。分配排序主要包括桶排序和基数排序。<br><a id="more"></a></p><h3 id="桶排序"><a href="#桶排序" class="headerlink" title="桶排序"></a>桶排序</h3><p>首先来介绍桶排序。如果我们知道序列中的记录都位于一个比较小的区间范围之内，那我们可以把相同值的记录分配到同一个桶里面，然后依次收集这些桶就能得到一个有序的序列。<br>例如，下图中，我们知道待排序数组的记录值在0~9的范围之内，因此，我们可以设定10个桶，然后就把这些记录值分配到各个桶里面。<br><img src="http://i.imgur.com/SVkuDWp.png" alt=""><br>此外，我们还维护了前若干桶的一个累加值，并且，在进行排序的时候，我们从数组记录值的最右边开始遍历，这样来保持排序的稳定性。<br><img src="http://i.imgur.com/yGcdDMb.png" alt=""></p><h4 id="桶排序的实现"><a href="#桶排序的实现" class="headerlink" title="桶排序的实现"></a>桶排序的实现</h4><p>在弄清楚了原理之后，桶排序的实现就很简单了。</p><pre><code> void BucketSort(vector&lt;int&gt;&amp; nums, int max){     int n=nums.size();     vector&lt;int&gt; temp(begin(nums), end(nums));     vector&lt;int&gt; count(max);     for(int i=0;i&lt;max;i++){         count[i]=0;     }     for(int i=0;i&lt;n;i++){         count[nums[i]]++;     }     for(int i=0;i&lt;max;i++){         count[i]=count[i-1]+count[i];     }     for(int i=n-1;i&gt;=0;i--){             nums[--count[temp[i]]]=temp[i];     }}</code></pre><h4 id="桶排序性能分析"><a href="#桶排序性能分析" class="headerlink" title="桶排序性能分析"></a>桶排序性能分析</h4><p>由前面的分析可知，桶排序使用的场景是：待排序的数组长度为n，所有数组记录值位于[0,m)上，且m相对于n很小。<br>因为排序过程需要遍历原数组和count数组，所以总的时间复杂度为O（n+m）。空间代价为O（n+m），包括临时数组和计数器。桶排序是稳定的。   </p><h3 id="静态基数排序"><a href="#静态基数排序" class="headerlink" title="静态基数排序"></a>静态基数排序</h3><p>我们已经知道，桶排序只适合m很小的情况，那如果出现m很大，甚至大于n的情况，应该怎么办呢？不难想象，我们可以把这些记录的关键码认为地拆成几个部分，然后再运用几次桶排序，从而完成整个排序过程，这就是基数排序的思想，基数排序又可以分为静态基数排序和链式基数排序。<br>比如，我们需要对0到9999之间的整数进行排序，这时直接使用桶排序显然是代价较高的。我们可以将四位数看做是由四个排序码决定，个位为最低排序码。基数等于10。然后，按照千，百，十，个位数字依次进行4次桶排序。<br>下面以一个记录值都是两位数的数组的排序过程来说明静态基数排序两趟桶排的具体过程，这里和后面的代码都采用的是低位优先法。<br><img src="http://i.imgur.com/gPF11Hv.png" alt=""><br><img src="http://i.imgur.com/hr8v9TV.png" alt=""></p><h4 id="静态基数排序的实现"><a href="#静态基数排序的实现" class="headerlink" title="静态基数排序的实现"></a>静态基数排序的实现</h4><p>同样，给出静态基数排序的实现。</p><pre><code>void RadixSort(vector&lt;int&gt;&amp; nums, int d, int r){    int n=nums.size();    vector&lt;int&gt; temp(n,0);    vector&lt;int&gt; count(r,0);        //r:radix,10    int radix=1;    int i,j,k;    for(i=1;i&lt;=d;i++){        //d:排序码的个数        for(j=0;j&lt;r;j++){            count[j]=0;        }        for(j=0;j&lt;n;j++){            k=(nums[j]/radix)%r;            count[k]++;        }        for(j=1;j&lt;r;j++){            count[j]=count[j]+count[j-1];        }        for(j=n-1;j&gt;=0;j--){            k=(nums[j]/radix)%r;            count[k]--;            temp[count[k]]=nums[j];        }        for(j=0;j&lt;n;j++){            nums[j]=temp[j];        }        radix*=r;    }}</code></pre><h4 id="静态基数排序性能分析"><a href="#静态基数排序性能分析" class="headerlink" title="静态基数排序性能分析"></a>静态基数排序性能分析</h4><p>同样的，静态基数排序也需要一个临时数组和r个计数器，所以总的空间代价为O（n+r）。时间代价则为O（d*(n+r)），因为它相当于进行了d次桶式排序。此外，静态基数排序也是稳定的。</p><h3 id="链式基数排序"><a href="#链式基数排序" class="headerlink" title="链式基数排序"></a>链式基数排序</h3><p>基于静态链的基数排序相对于静态基数排序的差别在于将分配出来的子序列存放在r个静态链组织的队列中。<br>下面，同样以一个例子来说明链式基数排序的过程。<br>假设有待排序数组[97, 53, 88, 59, 26, 41, 88, 31, 22]，首先按照个位进行第一趟分配，分配完元素后的队列如下图所示。<br><img src="http://i.imgur.com/D5ZCnQk.png" alt=""><br>然后进行第一趟收集，即个位有序：[41, 31, 22, 53, 26, 97, 88, 88]。<br>接下来，再按照十位进行第二趟分配。<br><img src="http://i.imgur.com/BUeHB5r.png" alt=""><br>最后进行第二趟收集，即完成最终的排序。</p><h4 id="链式基数排序的实现"><a href="#链式基数排序的实现" class="headerlink" title="链式基数排序的实现"></a>链式基数排序的实现</h4><p>下面给出链式基数排序的实现。</p><pre><code>typedef struct Node{    int key;    int next;    //下一个节点在数组中的下标}Node;typedef struct StaticQueue{    int head;    int tail;}StaticQueue;void Distribute(vector&lt;Node&gt;&amp; node, int first, int i, int r, StaticQueue* queue);void Collect(vector&lt;Node&gt;&amp; node, int&amp; first, int r, StaticQueue* queue);void RadixSort(vector&lt;int&gt;&amp; nums, int d, int r){    int i, first=0;    int n=nums.size();    vector&lt;Node&gt; node(n);    StaticQueue* queue = new StaticQueue[r];    for(i=0;i&lt;n-1;i++){        node[i].key=nums[i];        node[i].next=i+1;    }    node[n-1].key=nums[n-1];    node[n-1].next=-1;    for(i=0;i&lt;d;i++){            //d趟的分配和收集        Distribute(node, first, i, r, queue);            //分配到不同队列        Collect(node, first, r, queue);                    //聚合    }    for(i=0;i&lt;r;i++){        if(queue[i].head==-1) continue;        else break;    }    int j=queue[i].head;    while(node[j].next!=-1){        //排序结果        cout &lt;&lt; node[j].key &lt;&lt; endl;        j = node[j].next;    }    cout &lt;&lt; node[j].key &lt;&lt; endl;    delete[] queue;}void Distribute(vector&lt;Node&gt;&amp; node, int first, int i, int r, StaticQueue* queue){    int current=first;    for(int j=0;j&lt;r;j++){        queue[j].head=-1;    }    while(current!=-1){        int k = node[current].key;        for(int a=0;a&lt;i;a++){            k=k/r;        }        k=k%r;        if(queue[k].head==-1){            queue[k].head=current;        }else{            node[queue[k].tail].next=current;        }        queue[k].tail=current;        current=node[current].next;    }}void Collect(vector&lt;Node&gt;&amp; node, int&amp; first, int r, StaticQueue* queue){    int last,k=0;    while(queue[k].head==-1) k++;    first=queue[k].head;    last=queue[k].tail;    while(k&lt;r-1){        k++;        while(k&lt;r-1 &amp;&amp; queue[k].head==-1){            k++;        }        if(queue[k].head!=-1){            node[last].next=queue[k].head;            last=queue[k].tail;        }    }    node[last].next=-1;}</code></pre><h4 id="链式基数排序性能分析"><a href="#链式基数排序性能分析" class="headerlink" title="链式基数排序性能分析"></a>链式基数排序性能分析</h4><p>由前面的分析和代码可知，链式基数排序在分配和收集的过程中，不需要移动记录本身，只是在做记录next指针的修改。因此，它的时间代价和空间代价与静态基数排序一致，分别为O（d*(n+r)）和O（n+r）。并且，根据队列的先入先出的特点，链式基数排序也是稳定的。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文总结了分配排序中最常见的桶排序和基数排序，并给出了具体的实现和相关分析。可以看出，分配排序不会对记录值进行两两比较，因此不受O（nlgn）时间复杂度的限制。从另外一个角度看，分配排序也可以看做是以空间换时间的典型方法。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;分配排序，是指不需要进行两两之间的比较，而根据记录自己的关键码的分配来进行排序的一类方法，因此，在进行分配排序时，我们通常需要知道记录序列的一些具体情况，比如关键码的分布。分配排序主要包括桶排序和基数排序。&lt;br&gt;
    
    </summary>
    
    
      <category term="数据结构与算法" scheme="http://blog.jonnydu.me/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>搭建Redis集群</title>
    <link href="http://blog.jonnydu.me/2017/01/15/Redis-Cluster/"/>
    <id>http://blog.jonnydu.me/2017/01/15/Redis-Cluster/</id>
    <published>2017-01-15T07:08:05.000Z</published>
    <updated>2017-03-29T13:53:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>Redis集群是Redis提供的分布式数据库方案，集群通过分片来进行数据共享，并提供复制和故障转移功能。<br><a id="more"></a></p><h3 id="Redis集群概述"><a href="#Redis集群概述" class="headerlink" title="Redis集群概述"></a>Redis集群概述</h3><p>Redis集群使用数据分片而非一致性哈希来实现，一个Redis集群包含16384个哈希槽（slot），数据库中的每个键都属于这16384个哈希槽中的其中一个，集群中的每个节点可以处理0个或最多16384个槽，当数据库中的16384个槽都有节点在处理时，集群处于上线状态；而如果数据库中有任何一个槽没有得到处理，那么集群处于下线状态。<br>集群中的每个节点负责处理一部分哈希槽，比如，现在有三个独立的节点127.0.0.1:7000,127.0.0.1:7001,127.0.0.1:7002，其中各节点处理的哈希槽关系：节点7000负责处理0到5000号哈希槽，节点7001负责处理5001到10000号哈希槽，节点7002负责处理10001到16384号哈希槽。<br>从而，当向集群中添加或删除节点时，集群只需在对应节点的哈希槽做移动即可。不会造成节点阻塞、集群下线。<br>当然，为了使得集群在一部分节点下线的情况下仍然可以正常运作，Redis集群对节点提供了主从复制功能，集群中的每个节点都有1到N个复制节点，形成主-从模型。</p><h3 id="Redis集群架构图"><a href="#Redis集群架构图" class="headerlink" title="Redis集群架构图"></a>Redis集群架构图</h3><p>Redis集群架构如下图所示：<br><img src="http://i.imgur.com/O4QfdDF.jpg" alt=""><br>Redis集群架构的主要特点有:<br>（1）所有节点批次互联（PING-PONG机制），没有中心控制协调节点，内部使用二进制协议优化传输速度和带宽；<br>（2）节点的失效通过集群中超过半数的节点“投票”监测；<br>（3）客户端与Redis节点直连，不需要中间proxy层，客户端连接集群中任何一个可用节点即可；    </p><h3 id="Redis集群实践"><a href="#Redis集群实践" class="headerlink" title="Redis集群实践"></a>Redis集群实践</h3><p>下面搭建一个简单的Redis集群环境，集群共包含6个节点，其中3个主节点，3个从节点。节点都部署在本机（ubuntu 14.04 x86-64），以端口号区分，分别为127.0.0.1:7000~127.0.0.1:7005。<br>1.首先安装3.0版本之后的Redis（因为Redis集群是在3.0版本提出的功能），这里采用源码(3.2.8版本)安装。</p><pre><code>wget http://download.redis.io/releases/redis-3.2.8.tar.gz</code></pre><p>2.然后解压，安装</p><pre><code>tar xf redis-3.2.8.tar.gz    cd redis-3.2.8       make &amp;&amp; make install</code></pre><p>3.安装完成后查看Redis版本信息，验证安装是否成功。<br><img src="http://i.imgur.com/6QuQXPQ.png" alt=""></p><p>4.创建存放多个节点实例的目录</p><pre><code>mkdir redis-data/cluster -pcd redis-data/clustermkdir 7000 7001 7002 7003 7004 7005</code></pre><p>5.复制并修改配置文件redis.conf</p><pre><code>cp /etc/redis/redis.conf redis-data/cluster/7000</code></pre><p>修改配置文件中下面选项</p><pre><code>port 7000daemonize yescluster-enabled yescluster-config-file nodes.confcluster-node-timeout 5000appendonly yes</code></pre><p>然后把修改完成的redis.conf复制到7001-7005目录下，端口修改成和文件夹对应。 </p><p>6.分别启动6个Redis实例</p><pre><code>cd redis-data/cluster/7000redis-server redis.conf...cd redis-data/cluster/7001redis-server redis.conf</code></pre><p>7.查看进程是否运行<br><img src="http://i.imgur.com/nWUeGum.png" alt=""></p><p>8.接下来创建集群，首先安装依赖包</p><pre><code>apt-get install ruby</code></pre><p>然后安装gem-redis，下载地址<a href="https://rubygems.org/gems/redis/versions/3.3.0" target="_blank" rel="noopener">https://rubygems.org/gems/redis/versions/3.3.0</a></p><pre><code>gem install redis-3.3.0.gem</code></pre><p>接下来复制集群管理程序到/usr/local/bin</p><pre><code>cp redis-3.2.8/src/redis-trib.rb /usr/local/bin/redis-trib</code></pre><p>就可以使用redis-trib创建集群了。</p><pre><code>redis-trib create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005</code></pre><p>其中，命令create表示创建一个新的集群，选项–replicas 1表示未集群中的每个主节点创建一个从节点。即，上述命令运行后，redis-trib将创建一个包含三个主节点和三个从节点的集群。<br><img src="http://i.imgur.com/6jhul4S.png" alt=""><br>键入“yes”后一个三主三从的集群架构就创建完毕。<br><img src="http://i.imgur.com/WTTZPh8.png" alt=""></p><h3 id="一些简单的测试"><a href="#一些简单的测试" class="headerlink" title="一些简单的测试"></a>一些简单的测试</h3><p>创建完毕后，可以随时通过redis-cli -p 7000(node number) cluster nodes来查看集群中各节点的信息。包括唯一的节点ID，主从关系，每个主节点分配的slots范围。<br><img src="http://i.imgur.com/jJcOqwB.png" alt=""></p><h4 id="集群查询"><a href="#集群查询" class="headerlink" title="集群查询"></a>集群查询</h4><p>做一个简单的测试，    在7000节点上存储K-V数据，然后分别在其从节点和其他主节点上获取该数据。<br><img src="http://i.imgur.com/X3GWKHJ.png" alt=""><br><img src="http://i.imgur.com/a0P55nA.png" alt=""><br>要找的数据没有在当前节点上时，cluster发送MOVED指令指示到对应的节点上穿，可以通过-c参数指定查询时接收到MOVED指令自动跳转。</p><h4 id="删除节点"><a href="#删除节点" class="headerlink" title="删除节点"></a>删除节点</h4><p>Slave节点的作用是提供冗余和高可用，大部分情况下用于分担读的压力。移除Slave节点无须多余的动作，直接删除即可。<br><img src="http://i.imgur.com/Hr2xMz6.png" alt=""><br>而如果是Master节点，则需要将其负责的slots范围迁移到其他节点上。保证数据不丢失。<br>所以，先使用redis-trib reshard …来将待删除的节点上的数据迁移到其他节点，然后删除该节点。<br><img src="http://i.imgur.com/xLG6ctH.png" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Redis Cluster是3.0版本之后提供的新功能，采用了P2P的去中心化架构，而没有采用像Codis之类的Proxy解决方案中的中心协调节点设计。本文只是简单搭建了一个Redis集群环境，后续还将在此基础上，进一步深入研究高可用、可扩展的Redis集群方案。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Redis集群是Redis提供的分布式数据库方案，集群通过分片来进行数据共享，并提供复制和故障转移功能。&lt;br&gt;
    
    </summary>
    
    
      <category term="Redis" scheme="http://blog.jonnydu.me/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis Sentinel原理及实践</title>
    <link href="http://blog.jonnydu.me/2017/01/11/Redis-sentinel/"/>
    <id>http://blog.jonnydu.me/2017/01/11/Redis-sentinel/</id>
    <published>2017-01-11T07:21:58.000Z</published>
    <updated>2017-03-29T13:53:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>Sentinel（哨兵）是Redis的高可用性解决方案：由一个或多个Sentinel实例组成的Sentinel系统可以监视任意多个主服务器，以及这些主服务器属下的多个从服务器，并在被监视的主服务器进入下线状态时，自动将下线主服务器属下的某个从服务器升级为新的主服务器。<br><a id="more"></a></p><h3 id="Redis-Sentinel系统结构"><a href="#Redis-Sentinel系统结构" class="headerlink" title="Redis Sentinel系统结构"></a>Redis Sentinel系统结构</h3><p>下图展示了一个Sentinel系统监视服务器的例子。当前系统包括主服务器server1，从服务器server2、server3、server4和Sentinel监听系统。<br><img src="http://i.imgur.com/Q5Keiy2.png" alt=""><br>假设某时刻server1进入下线状态，那么从服务器对主服务器的复制操作将中止，并且Sentinel系统会察觉到server1已下线，并在server1的下线时长超过用户设定的上限时，Sentinel系统会执行故障转移操作：<br>（1）首先，Sentinel会挑选一个从服务器，将其升级为新的主服务器，这里涉及到选举算法；<br>（2）然后Sentienl向server1所有的从服务器发送新的复制指令，让它们成为新的主服务器的从服务器；<br>（3）Sentinel还会继续监视已下线的server1，并在它重新上线时，将它设置为新的服务器的从服务器。<br><img src="http://i.imgur.com/l2q4BV4.jpg" alt=""></p><h3 id="Sentinel与主从Redis通信细节"><a href="#Sentinel与主从Redis通信细节" class="headerlink" title="Sentinel与主从Redis通信细节"></a>Sentinel与主从Redis通信细节</h3><h4 id="获取主服务器信息"><a href="#获取主服务器信息" class="headerlink" title="获取主服务器信息"></a>获取主服务器信息</h4><p>Sentinel默认会以每十秒一次的频率，通过命令连接向被监视的主服务器发送INFO命令，并通过分析INFO命令的回复来获取主服务器的当前信息。<br>通过分析主服务器返回的INFO命令回复，Sentinel可以获取以下两方面的信息：<br>（1）主服务器本身的信息，包括服务器运行ID，服务器角色等；<br>（2）主服务器属下的所有从服务器信息。包括从服务器的IP地址、端口号等，根据这些信息，Sentinel无须用户提供从服务器的地址信息，就可以自动发现从服务器。   </p><h4 id="获取从服务器信息"><a href="#获取从服务器信息" class="headerlink" title="获取从服务器信息"></a>获取从服务器信息</h4><p>当Sentinel发现主服务器有新的从服务器出现时，Sentinel除了会为这个新的从服务器创建相应的实例结构之外，Sentinel还会创建连接到从服务器的命令连接和订阅连接。<br>在创建命令连接之后，Sentinel在默认情况下，会以每十秒一次的频率通过命令向从服务器发送INFO命令。并收到服务器信息的回复，根据这些信息，对从服务器的实例结构进行更新。   </p><h4 id="与所有主从服务器通信"><a href="#与所有主从服务器通信" class="headerlink" title="与所有主从服务器通信"></a>与所有主从服务器通信</h4><p>默认情况下，Sentinel会以每两秒一次的频率，通过命令连接向所有被监视的主服务器和从服务器发送命令。信息格式为：</p><pre><code>PUBLISH __sentinel__:hello &lt;s_ip&gt;,&lt;s_port&gt;,&lt;s_runid&gt;,&lt;s_epoch&gt;,&lt;m_name&gt;,&lt;m_ip&gt;,&lt;m_port&gt;,&lt;m_epoch&gt;</code></pre><p>其中，以<code>s_</code>开头的参数是Sentinel本身的信息。而m_开头的参数则是主服务器的信息。<br>另外，当Sentinel与一个主服务器或者从服务器建立起订阅连接之后，Sentinel就会通过订阅连接，向服务器发送命令： <code>SUBSCRIBE __sentinel__:hello</code>，Sentinel会在其与服务器之间的连接断开前一直订阅该频道。<br>因此，每个与Sentinel连接的服务器，Sentinel既通过命令连接向服务器的 <code>__sentinel__:hello</code>频道发送信息，又通过订阅连接从服务器的<code>__sentinel__:hello</code>频道接收信息。<br>比如，有sentinel1、sentinel2、sentinel3三个sentinel在监视同一个服务器，当sentinel<br>1向服务器的<code>__sentinel__:hello</code>频道发送一条信息时，所有订阅了该频道的sentine（包括自己）都会收到这条信息。这些信息用于更新其他Sentinel对发送信息Sentinel的认知，也会被用于更新其他Sentinel对监视服务器的认知。<br>此</p><h3 id="选举领头Sentinel"><a href="#选举领头Sentinel" class="headerlink" title="选举领头Sentinel"></a>选举领头Sentinel</h3><p>默认情况下，Sentinel会以每秒一次的频率向所有与它创建了命令连接的实例发送PING命令，并通过实例返回的PING命令回复来判断实例是否在线。<br>当Sentinel将一个主服务器判断为主观下线之后，为了确认这个主服务器是否真的下线，它会向同样监视这一主服务器的其他Sentinel进行询问。当从其他Sentinel那里接收到足够数量的已下线判断之后，Sentinel就会将从服务器判定为客观下线，并对主服务器进行故障转移操作。<br>当一个主服务器被判断为客观下线时，监视这个下线服务器的各个Sentinel会进行协商，选举出一个领头Sentinel，并由领头Sentinel对下线主服务器执行故障转移操作（详细的选举算法这里不详述）。<br>在选举出领头Sentinel之后，领头Sentinel将对已下线的主服务器执行文章开头所述的故障转移操作。</p><h3 id="Sentinel实践"><a href="#Sentinel实践" class="headerlink" title="Sentinel实践"></a>Sentinel实践</h3><p>以下实验均是基于ubuntu 14.04 x86-64bits平台，Redis通过apt-get install redis-server简易安装，版本2.8.x，主从服务器都位于本机。</p><h4 id="实验架构和目录结构"><a href="#实验架构和目录结构" class="headerlink" title="实验架构和目录结构"></a>实验架构和目录结构</h4><p><img src="http://i.imgur.com/lXobqNP.png" alt=""><br><img src="http://i.imgur.com/6dgbBnr.png" alt=""></p><h4 id="搭建Reis主从结构"><a href="#搭建Reis主从结构" class="headerlink" title="搭建Reis主从结构"></a>搭建Reis主从结构</h4><p>首先配置master对应的配置redis.conf，重点需要关注的地方有：</p><pre><code>pidfile /var/run/redis.pidport 7003tcp-keepalive 60logfile /var/log/redis/redis-server.log</code></pre><p>其余很多选项保持默认即可。然后<code>redis-server master/redis.conf</code>启动master。<br>然后搭建slave，slave的配置文件和master基本一致，只需要修改相应的pidfile、端口(8003)、日志文件名，并配上master的地址<code>slaveof 127.0.0.1:7003</code></p><h4 id="Sentinel配置"><a href="#Sentinel配置" class="headerlink" title="Sentinel配置"></a>Sentinel配置</h4><p>接下来配置三个哨兵。以sentinel1.conf为例</p><pre><code>port 26371daemonize yeslogfile &quot;./sentinel1.log&quot;dir &quot;/home/dujiong/redis-data/sentinel&quot;sentinel monitor Master 127.0.0.1 7003 1sentinel down-after-milliseconds Master 1500sentinel failover-timeout Master 10000sentinel config-epoch Master 15sentinel known-slave Master 127.0.0.1 8003sentinel known-sentinel Master 127.0.0.1 26372 0aca3a57038e2907c8a07be2b3c0d15171e44da5sentinel known-sentinel Master 127.0.0.1 26373 e7625d74a5a4b142c495baa8ca522517bd08c65b</code></pre><p>其余两个哨兵在此基础上修改相应的参数即可。<br>配置好环境后，使用ps命令查看进程运行情况。<br><img src="http://i.imgur.com/QbnRZN8.png" alt=""></p><h4 id="查看Sentinel监控的主从服务器"><a href="#查看Sentinel监控的主从服务器" class="headerlink" title="查看Sentinel监控的主从服务器"></a>查看Sentinel监控的主从服务器</h4><p>在Sentinel中查看当前的主从服务器状态如下：<br><img src="http://i.imgur.com/EqCwUhI.png" alt=""><br><img src="http://i.imgur.com/UtcunJV.png" alt=""><br><img src="http://i.imgur.com/O2zmFTF.png" alt="">  、</p><h4 id="故障转移"><a href="#故障转移" class="headerlink" title="故障转移"></a>故障转移</h4><p>接下来，使用<code>redis-cli -p 7003 shutdown</code>关闭master，按照前面的理论，Sentinel将会进行故障转移操作，选择slave作为主服务器。<br><img src="http://i.imgur.com/tAqniY8.png" alt=""><br>最后，再次启动master服务器（127.0.0.1:7003），Sentinel将它设置为新的服务器的从服务器。<br><img src="http://i.imgur.com/rNhhyy6.png" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Sentinel是一个运行在特殊模式下的Redis服务器，用以提供Redis的高可用解决方案。在3.0之后的Redis版本中，Redis Cluster重用了Sentinel的代码逻辑，不需要单独启动一个Sentinel集群，Cluster本身就能自动进行Master选举和Failover切换。   </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Sentinel（哨兵）是Redis的高可用性解决方案：由一个或多个Sentinel实例组成的Sentinel系统可以监视任意多个主服务器，以及这些主服务器属下的多个从服务器，并在被监视的主服务器进入下线状态时，自动将下线主服务器属下的某个从服务器升级为新的主服务器。&lt;br&gt;
    
    </summary>
    
    
      <category term="Redis" scheme="http://blog.jonnydu.me/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis事务</title>
    <link href="http://blog.jonnydu.me/2017/01/05/Redis-transaction/"/>
    <id>http://blog.jonnydu.me/2017/01/05/Redis-transaction/</id>
    <published>2017-01-05T12:25:52.000Z</published>
    <updated>2017-03-29T13:53:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>在关系数据库的事务中，用户首先向数据库服务器发送BEGIN，然后执行各个相互一致的写操作和读操作，最后，用户可以选择发送COMMIT来确认之前所做的修改，或者发送ROLLBACK来放弃那些修改。<br><a id="more"></a></p><h3 id="Redis事务处理流程"><a href="#Redis事务处理流程" class="headerlink" title="Redis事务处理流程"></a>Redis事务处理流程</h3><p>Redis通过MULTI、EXEC、WATCH等命令来实现事务功能。事务提供了一种将多个命令请求打包，然后一次性、按顺序地执行多个命令的机制，并且在事务执行期间，服务器不会中断事务而该去执行其他客户端的命令请求，它会将事务中的所有命令都执行完毕，然后才去处理其他客户端的命令请求。<br>一个事务从开始到结束会经历如下三个阶段：<br>（1）事务开始<br>MULTI命令标志着事务的开始，其将执行该命令的客户端从非事务状态切换至事务状态。<br>（2）命令入队<br>当一个客户端处于非事务状态时，这个客户端发送的命令会立即被服务器执行，而当其切换到事务状态时，服务器会根据客户端发来的不同命令来执行不同的操作：如果是MULTI、EXEC、DISCARD、WATCH四个命令中的一个，那么服务器立即执行；如果不是，那么服务器不立即执行命令，而是将其放入一个服务队列里面，然后向客户端返回QUEUED回复。<br>这些命令保存在每个客户端所持有的一个事务队列中，该队列以FIFO的方式保存入队的命令。<br>（3）执行事务<br>当处于事务状态的客户端向服务器发送EXEC命令时，EXEC命令将立即被服务器执行。服务器会遍历这个客户端的事务队列，执行队列中保持的所有命令，最后将执行结果全部返回给客户端。<br>但是，MULTI和EXEC只能保证它们中间的那些入队的命令不会被其他客户端掺杂在一起，却没有锁的逻辑，即不能解决干扰。比如，现有一个key为1的数字，客户端1需要在事务中连续做两次自增操作，但在EXEC之前，客户端2对key进行了修改<code>set key 100</code>。<br><img src="http://i.imgur.com/6PQWySj.png" alt=""><br>可以看出，在事务执行的过程中，读到了脏数据，为了保障不被干扰，需要对数据加锁。        </p><h3 id="乐观锁WATCH"><a href="#乐观锁WATCH" class="headerlink" title="乐观锁WATCH"></a>乐观锁WATCH</h3><p>在访问以写入为目的数据的时候，关系数据库会对被访问的数据加锁，直到事务被提交或者被回滚为止。如果有其他客户端试图对被加锁的数据行进行写入，那么该客户端将被阻塞，直到第一个事务执行完毕为止。这种做法称为悲观锁。<br>而Redis为了减少客户端的等待时间，并不会在执行WATCH命令时对数据加锁。相反地，Redis只会在数据已经被其他客户端抢先修改了的情况下，通知执行了WATCH的客户端，这种做法称为乐观锁。当被监视的键至少有一个已经被修改过了，服务器将拒绝执行事务，并向客户端返回代表事务执行失败的空回复。<br>对于上面的例子，使用WATCH监视key。<br><img src="http://i.imgur.com/PMcavUU.png" alt=""><br>当客户端执行EXEC时，服务器发现WATCH监视的键“name”已经被修改，因此服务器拒绝执行客户端A的事务，并向客户端A返回空回复。<br>但是，需要注意的是，WATCH命令的作用只是当被监控的键值被修改后阻止之后一个事务的执行，而不能保证其他客户端不修改这一键值，所以，通常情况下在EXEC执行失败后会重新执行整个函数。   </p><h4 id="利用WATCH实现原子自增"><a href="#利用WATCH实现原子自增" class="headerlink" title="利用WATCH实现原子自增"></a>利用WATCH实现原子自增</h4><p>使用伪代码表示为：</p><pre><code>WATCH keyval = GET keyval = val + 1MULTISET key $valEXEC</code></pre><h3 id="事务的ACID性质"><a href="#事务的ACID性质" class="headerlink" title="事务的ACID性质"></a>事务的ACID性质</h3><p>（1）原子性<br>对于Redis的事务功能来说，事务队列中的命令要么就全部都执行，要么就一个都不执行，因此，从这一点来看，Redis的事务是具有原子性的。<br>下图是事务因为命令入队出错而被服务器拒绝执行，事务中的所有命令都不会被执行。<br><img src="http://i.imgur.com/TnIfK7G.png" alt=""><br>但是，Redis事务不支持关系数据库的事务回滚机制，即事务队列中的某个命令在执行期间（非语法错误，执行错误）出现了错误，事务的后续命令也会继续执行下去，并且之前执行的命令没有影响。因此，在这方面是不符合原子性的。<br><img src="http://i.imgur.com/igmOQqS.png" alt=""><br>（2）一致性<br>事务的一致性是指，如果数据库在执行事务之前是一致的，那么在事务执行之后，无论事务是否执行成功，数据库也应该是一致的。<br>如前所述，当出现入队错误或是执行错误的情况，Redis分别通过拒绝执行和进行错误处理的方式来保证事务的一致性。<br>当Redis服务器在执行事务的过程中停机，那么根据服务器使用的持久化方式，分以下几种情况讨论：<br>a. 如果当前Redis采用的是内存模式，那么重启后Redis数据库是空的，满足一致性条件；<br>b. 如果当前Redis采用RDB模式存储，在执行事务时，Redis不会中断事务去执行保存RDB的工作，只有在事务执行之后，保存RDB的工作才有可能开始。所以当RDB模式下的Redis服务器进程在事务中途被杀死时，事务内执行的命令，不管成功了多少，都不会被保存到RDB文件里。恢复数据库需要使用现有的RDB文件，而这个RDB文件的数据保存的是最近一次的数据库快照，所以它的数据可能不是最新的，但只要RDB文件本身没有因为其他问题而出错，那么还原后的数据库就是一致的；<br>c. 如果当前Redis采用的AOF模式存储，那么可能事务的内容还未写入到AOF文件，那么此时肯定是满足一致性的，如果事务的内容有部分写入到AOF文件中，那么需要用工具把AOF中事务执行部分成功的指令移除，这时，移除之后的AOF文件也是满足一致性的。<br>（3）隔离性<br>事务的隔离性是指，即使数据库有多个事务并发地执行，各个事务之间也不会互相影响，并发状态下执行的事务和串行执行的事务产生的结果完全相同。<br>因为Redis采用单线程的方式来执行事务，并且服务器保证，在执行事务期间不会对事务进行中断，因此，Redis的事务总是以串行的方式运行的，且事务总是具有隔离性。<br>（4）持久性<br>事务的持久性是指，当一个事务执行完毕时，执行这个事务所得的结果已经被保存到永久性存储介质里面了，即使服务器在事务执行完毕之后停机，执行事务所得的结果也不会丢失。<br>Redis事务只有当服务器运行在AOF持久化模式下，并且appendfsync选项为always时，才是具有持久性的。因为程序总会在执行命令之后调用同步函数，将命令数据真正地保存到硬盘里。<br>当然，不论Redis以什么模式运行，在一个事务的最后加上SAVE命令总可以保证持久性，但这种做法效率太低。<br>所以，Redis只满足ACID中的一致性和隔离性。 </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在关系数据库的事务中，用户首先向数据库服务器发送BEGIN，然后执行各个相互一致的写操作和读操作，最后，用户可以选择发送COMMIT来确认之前所做的修改，或者发送ROLLBACK来放弃那些修改。&lt;br&gt;
    
    </summary>
    
    
      <category term="Redis" scheme="http://blog.jonnydu.me/tags/Redis/"/>
    
  </entry>
  
  <entry>
    <title>更好地使用STL关联容器</title>
    <link href="http://blog.jonnydu.me/2016/12/30/Effective-STL-3/"/>
    <id>http://blog.jonnydu.me/2016/12/30/Effective-STL-3/</id>
    <published>2016-12-30T02:13:50.000Z</published>
    <updated>2017-01-19T07:13:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>在STL的使用过程中，一直对关联容器掌握的不够熟练。这一篇就来总结下使用关联容器时的一些注意问题。<br><a id="more"></a></p><h3 id="理解等价关系"><a href="#理解等价关系" class="headerlink" title="理解等价关系"></a>理解等价关系</h3><p>在STL中，对两个对象进行比较，看它们的值是否相等，这样的操作随处可见。在实际操作中，相等的概念是基于operator==的。如果表达式“x==y”返回真，则x和y的值相等。<br>等价关系是以“在以排序的区间中对象值的相对顺序”为基础的。如果从每个标准关联容器的排列顺序来考虑等价关系，那么这将是非常有意义的。对于两个对象x和y，如果按照关联容器c的排列顺序，每个都不在另一个的前面，那么称这两个对象按照c的排列顺序由等价的值。<br>下面以一个实例进行说明：    </p><pre><code>bool ciCharLess(char c1, char c2){    return tolower(static_cast&lt;unsigned char&gt;(c1))&lt;tolower(static_cast&lt;unsigned char&gt;(c2));}bool ciStringCompare(const string&amp; s1, const string&amp; s2){    return lexicographical_compare(s1.begin(), s1.end(), s2.begin(), s2.end(), ciCharLess);}struct CIStringCompare : public binary_function&lt;string, string, bool&gt;{    bool operator()(const string&amp; lhs, const string&amp; rhs){        return ciStringCompare(lhs, rhs);     }}int main(){    set&lt;string, CIStringCompare&gt; s;    s.insert(&quot;STL&quot;);    s.insert(&quot;stl&quot;);    for(auto n:s){        cout &lt;&lt; n &lt;&lt; endl;            //STL    }    if(s.find(&quot;stl&quot;)!=s.end()){        cout &lt;&lt; &quot;success&quot; &lt;&lt; endl;        //success    }else{        cout &lt;&lt; &quot;fail&quot; &lt;&lt; endl;    }    if(std::find(s.begin(), s.end(), &quot;stl&quot;)!=s.end()){        cout &lt;&lt; &quot;success&quot; &lt;&lt; endl;    }else{        cout &lt;&lt; &quot;fail&quot; &lt;&lt; endl;            //fail    }    return 0;}</code></pre><p>s是一个不区分大小写的set<string>，即当set的比较函数忽略字符串中字符的大小写时的set<string>。这样一个比较函数将把“STL”和“stl”看做是等价的。因此，在先后插入“STL”和“stl”时，只有“STL”会成功插入。如果使用set的find成员函数来查找“stl”时，该查找会成功；而如果是使用非成员的find算法，则查找将失败。因为“STL”和“stl”是等价的。顺便说一句，该例子从一个方面解释了为什么应该优先选用成员函数而不是与之对应的非成员函数。</string></string></p><h3 id="为包含指针的关联容器指定比较类型"><a href="#为包含指针的关联容器指定比较类型" class="headerlink" title="为包含指针的关联容器指定比较类型"></a>为包含指针的关联容器指定比较类型</h3><p>假如有一个包含string*指针的set，把一些动物的名字插入到该集合中：     </p><pre><code>set&lt;string*&gt; ssp;ssp.insert(new string(&quot;Anteater&quot;));ssp.insert(new string(&quot;Wombat&quot;));ssp.insert(new string(&quot;Lemur&quot;));ssp.insert(new string(&quot;Penguin&quot;));</code></pre><p>因为集合中所包含的是指针，所以，可能会想到使用下面的代码来打印出动物的名字：</p><pre><code>for(set&lt;string*&gt;::const_iterator i=ssp.begin(); i!=ssp.end(); ++i){    cout &lt;&lt; **i &lt;&lt; endl;}</code></pre><p>没错，动物的名称会被打印出来，但它们以字母顺序出现的概率仅为1/24。ssp会按顺序保存它的内容，但因为它包含的是指针，所以会按指针的值而不是按字符串的值进行排序，4个指针的值共有24个可能的排列方式，所以对要存储的指针会有24种可能的排列。<br>为了解决这个问题，需要知道set&lt;string*&gt; ssp是如下代码set&lt;string*,less&lt;string<em>&gt;&gt; ssp的缩写，当然，更精确的讲是set&lt;string\</em>,less&lt;string*&gt;,allocator&lt;string*&gt;&gt;的缩写，只是这里不考虑分配子的影响。<br>因此，如果想让string*指针在集合中按字符串的值排序，那么不能使用默认的比较函数子类。必须自己编写比较函数子类。        </p><pre><code>struct StringPtrLess : public binary_function&lt;const string*, const string*, bool&gt;{    bool operator() (const string* s1, const string* s2) const{        return *s1 &lt; *s2;    }};  set&lt;string*, StringPtrLess&gt; ssp;/*void print(const string* ps){    cout &lt;&lt; *ps &lt;&lt; endl;}for_each(ssp.begin(), ssp.end(), print);*/</code></pre><p>现在上述的打印循环可以做到预期的事情了。<br>所以，当需要创建包含指针的关联容器时，容器将会按照指针的值进行排序。绝大多数情况下，这不会是所希望的，这种情况下，几乎肯定要创建自己的函数子类作为该关联容器的比较类型。</p><h3 id="考虑用排序的vector替代关联容器"><a href="#考虑用排序的vector替代关联容器" class="headerlink" title="考虑用排序的vector替代关联容器"></a>考虑用排序的vector替代关联容器</h3><p>个人使用STL的经历中，当需要一个可提供快速查找功能的数据结构时，都会立刻想到标准关联容器，即set、multiset、map和multimap。但是，它们并不总是适合的。比如，如果查找速度真的很重要，那么，非标准的散列容器（unordered_map等）几乎是值得考虑的。因为通过适当的散列函数，散列容器几乎能提供常数时间的查找能力，优于set、multiset、map和multimap的确定的对数时间查找能力。<br>但是，即使确定的对数时间查找能力满足需求，标准关联容器可能也不是最好的选择。标准关联容器的效率比vector还低的情况并不少见。标准关联容器通常被实现为平衡的二叉查找树。二叉查找树这种数据结构对插入、删除和查找的混合操作做了优化，也就是，它所适合的那些应用程序的主要特征是插入、删除和查找混在一起。即没办法预测出针对这棵树的下一个操作是什么。<br>而还有很多应用程序使用其数据结构的方式并不这么混乱。它们使用其数据结构的过程可以明显地分为三个阶段：<br>（1）设置阶段。创建新的数据结构，并插入大量元素。<br>（2）查找阶段。查询该数据结构以找到特定的信息。<br>（3）重组阶段。改变该数据结构的内容。<br>对于以这种方式使用其数据结构的应用程序来说，vector可能比关联容器提供了更好的性能。但是不是任意的vector，而必须是排序的vector，因为只有对排序的容器才能够正确地使用查找算法binary_search、lower_bound和equal_range等。<br>那么，为什么通过排序的vector执行的二分搜索，比通过二叉查找树执行的二分搜索具有更好的性能呢？<br>其原因主要是：关联容器几乎肯定在使用平衡二叉树。这就意味着在一个关联容器中存储一个类型所伴随的空间开销至少是三个指针（父指针，左儿子，右儿子）。相反，存储在vector中则不会有任何的额外开销；只是简单地存储一个类型。<br>当然，对于排序的vector，最不利的地方在于它必须保持有序！当一个新的元素被插入时，新元素之后的所有元素都必须向后移动一个元素的位置。当一个元素从vector中删除了，则在它之后的所有元素也都要向前移动。插入和删除操作对于vector来说是昂贵的，但对于关联容器却是廉价的。这就是为什么当“对数据结构的使用方式是：查找操作几乎从不跟插入和删除操作混在一起”时，再考虑使用排序的vector而不是关联容器才是合理的。     </p><h3 id="在map-operator-和map-insert之间选择"><a href="#在map-operator-和map-insert之间选择" class="headerlink" title="在map::operator[]和map::insert之间选择"></a>在map::operator[]和map::insert之间选择</h3><p>map的operator[]函数与众不同。它与vector、deque和string的operator[]函数无关，与用于数组的内置operator[]也没有关系。相反，map::operator[]的设计目的是为了提供“添加和更新”的功能，也就是说，对于map&lt;K,V&gt; m；来说，表达式m[k]=v;检查键k是否已经在map中了，如果没有，它就被加入，并以v作为相应的值。如果k已经在映射表中了，则与之关联的值被更新为v。<br>下面以一个例子来说明：      </p><pre><code>class Widget {    public:        Widget();        Widget(double weight);        Widget&amp; operator=(double weight);    private:        double weight_;    ...        };map&lt;int, Widget&gt; m;m[1]=1.50;</code></pre><p>表达式m[1]是m.operator[](1)的缩写形式，即对map::operator[]的调用。该函数必须返回一个指向Widget的引用，因为m所映射的值类型是Widget。这时，m中什么都没有，所以键1没有对应的值对象。因此，operator[]默认构造了一个Widget，作为与1相关联的值，然后返回一个指向该Widget的引用。最后，这个Widget成了赋值的目标。因此，m[1]=1.50在功能上等价于以下代码：      </p><pre><code>typedef map&lt;int, Widget&gt; IntWidgetMap;pair&lt;IntWidgetMap::iterator, bool&gt; result = m.insert(IntWidgetMap::value_type(1, Widget()));result.first-&gt;second = 1.50;</code></pre><p>因此，使用operator[]会降低效率。因为我们先默认构造了一个Widget，然后立刻赋给它新的值。而如果我们换成对insert的直接调用：    </p><pre><code>m.insert(IntWidgetMap::value_type(1, 1.50));</code></pre><p>最终效果和前面相同，但是它通常会节省三个函数调用：一个用于创建默认构造的临时Widget对象，一个用于析构该临时对象，还有一个是调用Widget的赋值描述符。<br>而operator[]的设计目的是为了提供“添加和更新”的功能，现在我们已经知道，当做为“添加”操作时，insert比operator[]效率更高，而当我们做更新操作时，即当一个等价的键已经在映射表中时，形势就反过来了。因为调用insert时，必须构造和析构一个pair类型的对象，需要付出一个pair构造函数和一个pair析构函数的代价。而这又会导致对Widget的构造和析构动作。而operator[]不使用pair对象，所以它不会构造和析构任何pair或Widget。<br>总结：当向映射表中添加元素时，优先选用insert而不是operator[]；而当更新已经在映射表中的元素的值时，要优先选择operator[]。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在STL的使用过程中，一直对关联容器掌握的不够熟练。这一篇就来总结下使用关联容器时的一些注意问题。&lt;br&gt;
    
    </summary>
    
    
      <category term="C++" scheme="http://blog.jonnydu.me/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>协程</title>
    <link href="http://blog.jonnydu.me/2016/12/23/Coroutine/"/>
    <id>http://blog.jonnydu.me/2016/12/23/Coroutine/</id>
    <published>2016-12-23T10:41:42.000Z</published>
    <updated>2017-01-19T08:53:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>协程，顾名思义，是“协作的例程”。跟具有操作系统概念的线程不一样，协程是在用户空间利用程序语言的语法语义就能实现逻辑上类似多任务的编程技巧。协程可以在运行期间的某个点上暂停执行，并在恢复运行时从暂停的点上继续执行。协程已经被证明是一种非常有用的程序组件，不仅被Python、lua、ruby等脚本语言广泛语言，还被新一代面多核的编程语言如golang等作为并发的基本单位。<br><a id="more"></a>   </p><h3 id="协程的特点"><a href="#协程的特点" class="headerlink" title="协程的特点"></a>协程的特点</h3><p>协程的调度完全由用户控制，一个线程可以有多个协程，每个协程都是循环按照指定的任务清单顺序完成不同的任务，当任务被阻塞的时候执行下一个任务，当恢复的时候再回来执行这个任务，任务之间的切换只需要保存每个任务的上下文内容，就像直接操作栈一样，这样就完全没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。<br>因此，与传统的抢占式线程相比，协程主要具有以下两个优点：<br>（1）与线程不同，线程是自己主动让出CPU，并交付它期望的下一个协程运行，而不是在任何时候都有可能被系统调度打断。因此协程的使用更加清晰易懂，并且大多数情况下不需要锁机制。<br>（2）与线程相比，协程的切换由程序控制，发生在用户空间而非内核空间，因此切换的代价非常小。    </p><h3 id="进程与线程"><a href="#进程与线程" class="headerlink" title="进程与线程"></a>进程与线程</h3><p>下面简单回顾下进程与线程的概念。   </p><h4 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h4><p>进程是具有一定独立功能的程序关于某个数据集合上的一次活动，进程是系统进行资源分配和调度的单位。<br>进程之间不共享任何状态，进程的调度由操作系统完成，每个进程都有自己的独立的内存空间，而进程间的通信主要是通过信号传递的方式来实现的，实现的方式有多种，如信号量、管道等，但是任何一种方式的通信都需要通过内核，因此效率比较低。同时，由于进程拥有的是独立的内存空间，所以在进行上下文切换的时候需要先保存调用栈的信息，CPU各寄存器的信息，虚拟内存以及打开的句柄等信息，所以导致进程间切换开销很大。</p><h4 id="线程"><a href="#线程" class="headerlink" title="线程"></a>线程</h4><p>线程是进程的一个实体，是CPU调度的基本单位，它是比进程更小的能独立运行的基本单位。线程自己基本上不拥有系统资源，只拥有一点再运行中比不可少的资源（如程序计数器、一些寄存器和栈），它与同一个进程的其他线程共享所拥有的全部资源。<br>线程之间共享变量，解决了通信麻烦的问题，但同时，多个线程对变量的访问需要进行同步与互斥操作。线程的调度也主要由操作系统完成，一个进程可以拥有多个线程，每个线程会共享父进程向操作系统申请的资源，包括虚拟内存，文件等，因此创建线程所需的资源要比进程小很多，相应的可创建的线程数量也变得多很多。线程之间的通信除了可以使用进程之间通信的方式之外还可以通过共享内存的方式，此外，在线程调度方面，由于线程间共享进程的资源，所以上下文切换的时候需要保存的东西相对少些，上下文也因此高效一些。     </p><h3 id="构建C协程"><a href="#构建C协程" class="headerlink" title="构建C协程"></a>构建C协程</h3><p>C/C++不直接支持协程语义，但目前已有不少开源的协程库，本文以其中最常用的使用glibc的ucontext组件的实现方式进行说明。</p><h4 id="ucontext组件"><a href="#ucontext组件" class="headerlink" title="ucontext组件"></a>ucontext组件</h4><p>ucontext组件是GNU C库提供的一组用于创建、保存、切换用户态执行”上下文”的API。主要包括以下两个结构体和四个函数：</p><pre><code>//mcontest_t类型与机器相关，并且不透明typedef struct ucontext {    struct ucontext* uc_link;    //链接下一个执行的上下文    sigset_t uc_sigmask;    //阻塞信号集合    stack_t uc_stack;        //该上下文中使用的栈    mcontext_t uc_mcontest;    ...} ucontext_t;//初始化一个ucontext_t类型的结构，即用户执行上下文。//函数指针func指明了该context的入口函数，argc指入口参数个数    void makecontext(ucontext_t *ucp, void (*func)(), int argc, ...);//&quot;原子&quot;地完成旧状态的保存和切换到新状态的工作int swapcontext(ucontext_t *oucp, ucontext_t *ucp);//将当前执行上下文保存到ucp中，若后续调用setcontext或swapcontext恢复状态，//则程序会沿着getcontext调用点之后继续执行，看起来好像刚从getcontext函数返回一样。int getcontext(ucontext_t *ucp);//将当前程序执行线索切换到ucp所指向的上下文状态，//在执行正确的情况下，该函数直接切入到新的执行状态，不再回返回int setcontext(const ucontext_t *ucp);</code></pre><p>来看一个简单的实例：</p><pre><code>#include &lt;stdio.h&gt;#include &lt;ucontext.h&gt;#include &lt;unistd.h&gt;int main(){    ucontext_t context;    getcontext(&amp;context);        puts(&quot;hello world&quot;);    sleep(1);    setcontext(&amp;context);    return 0;}         </code></pre><p>运行结果如下：<br><img src="http://i.imgur.com/SvtkRI7.png" alt="">               </p><p>程序通过getcontext保存了一个上下文，然后输出”hello world”，睡一秒后执行到setcontext，恢复上下文到getcontext之后，重新执行代码，所以程序不断输出“hello world”。   </p><h4 id="使用ucontext实现线程切换"><a href="#使用ucontext实现线程切换" class="headerlink" title="使用ucontext实现线程切换"></a>使用ucontext实现线程切换</h4><p>虽然我们称协程是一个用户态的轻量级线程，但实际上多个协程同属于一个线程。任意一个时刻，同一个线程不可能同时运行两个协程。<br>接下来通过一个实例说明协程与主函数的切换，即实现协程的调度。 </p><pre><code>#include &lt;ucontext.h&gt;#include &lt;stdio.h&gt;void func1(void *arg){    puts(&quot;1&quot;);    puts(&quot;11&quot;);    puts(&quot;111&quot;);    puts(&quot;1111&quot;);}void context_test(){    char stack[1024*128];    ucontext_t child, main;    getcontext(&amp;child);    child.uc_stack.ss_sp = stack;    child.uc_stack.ss_size = sizeof(stack);    child.uc_link = &amp;main;    makecontext(&amp;child, (void (*)(void))func1, 0);    swapcontext(&amp;main, &amp;child);    puts(&quot;main&quot;);}int main(){    context_test();    return 0;}</code></pre><p>运行结果如下图所示。<br><img src="http://i.imgur.com/FbFrKKW.png" alt=""><br>在context_test中，创建了一个用户线程（协程）child，其运行的函数为func1，指定后继上下文为main，当func1返回后激活后继上下文，继续执行main函数。</p><h3 id="开源C-C-协程库"><a href="#开源C-C-协程库" class="headerlink" title="开源C/C++协程库"></a>开源C/C++协程库</h3><p>最后，罗列几个比较出名的开源C/C++协程库，后面争取再深入学习下。<br>libco： <a href="http://code.tencent.com/libco.html" target="_blank" rel="noopener">腾讯的开源协程库</a><br>coroutine: <a href="https://github.com/cloudwu/coroutine/" target="_blank" rel="noopener">云风大牛的作品</a><br>Protothreads: <a href="http://coolshell.cn/articles/10975.html" target="_blank" rel="noopener">一个”蝇量级”C语言协程库</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;协程，顾名思义，是“协作的例程”。跟具有操作系统概念的线程不一样，协程是在用户空间利用程序语言的语法语义就能实现逻辑上类似多任务的编程技巧。协程可以在运行期间的某个点上暂停执行，并在恢复运行时从暂停的点上继续执行。协程已经被证明是一种非常有用的程序组件，不仅被Python、lua、ruby等脚本语言广泛语言，还被新一代面多核的编程语言如golang等作为并发的基本单位。&lt;br&gt;
    
    </summary>
    
    
      <category term="进程/线程/并发" scheme="http://blog.jonnydu.me/tags/%E8%BF%9B%E7%A8%8B-%E7%BA%BF%E7%A8%8B-%E5%B9%B6%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>使用STL算法时需注意的问题</title>
    <link href="http://blog.jonnydu.me/2016/12/19/Effective-STL-2/"/>
    <id>http://blog.jonnydu.me/2016/12/19/Effective-STL-2/</id>
    <published>2016-12-19T12:21:14.000Z</published>
    <updated>2016-12-29T13:46:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>接着上一篇，对STL中非常有用而又容易犯错的一些算法做总结和记录。希望在后面的使用中更加注重STL算法。<br><a id="more"></a></p><h3 id="确保目标空间足够大"><a href="#确保目标空间足够大" class="headerlink" title="确保目标空间足够大"></a>确保目标空间足够大</h3><p>如上篇所述，当有新的对象加入容器时，STL容器会自动扩充存储空间以容纳这些对象。但是，需要注意的是，STL容器并不总是能够正确地管理其存储空间。比如下面这个例子：     </p><pre><code>int trans(int x){    return -x;}vector&lt;int&gt; ivec{1,2};vector&lt;int&gt; res;std::transform(ivec.begin(), ivec.end(), res.begin(), trans);    // error</code></pre><p>这段代码中，transform首先以ivec[0]为参数调用trans，并将结果赋给<em>\res.end()。然后，再以ivec[1]为参数调用trans，并将结果赋给\</em>(res.end()+1)。这可能会引起灾难性的后果，因为在*res.end()中并没有对象，*(res.end()+1)就更没有对象了。这种transform调用时错误的，因为导致了对无效对象的赋值操作。<br>犯这种错误的程序员总是希望他们所调用的算法的结果会被插入到目标容器中。事实上，必须向STL明确表达意图。所以，上面的例子需要通过调用back_insert生成一个迭代器来指定目标区间的起始位置：      </p><pre><code>int trans(int x){    return -x;    }vector&lt;int&gt; ivec{1,2};vector&lt;int&gt; res;std::transform(ivec.begin(), ivec.end(), back_inserter(res), trans);</code></pre><p>在内部，back_insert返回的迭代器将使得push_back被调用，所以back_insert可使用于所有提供push_back方法的容器。而如果想让一个算法在容器的头部而不是尾部插入对象可以使用front_inserter。<br>因此，无论何时，如果所使用的算法需要指定一个目标区间，那么必须确保目标区间足够大，或者确保它会随着算法的运行而增大。要么在算法执行过程中增大目标区间，请使用插入型迭代器，比如ostream_iterator，或者由back_inserter、front_inserter返回的迭代器。</p><h3 id="与排序有关的选择"><a href="#与排序有关的选择" class="headerlink" title="与排序有关的选择"></a>与排序有关的选择</h3><p>提到排序，首先想到的是std::sort。但需要注意的是，std::sort并非在任何场合下都是完美无缺的。有些场景并不需要完全的排序操作。比如使用partial_sort来选择前n个最好的商品送给n个最重要的客户，或者当只需要将最好的20个商品送给最重要的20个客户，而不关心哪个商品送给哪位客户的时候选用nth_element。   </p><pre><code>vector&lt;int&gt; ivec{2,5,10,23,12,56,34,100};std::partial_sort(ivec.begin(), ivec.begin()+4, ivec.end(), greater&lt;int&gt;());for(auto n : ivec){    cout &lt;&lt; n &lt;&lt; endl}</code></pre><p>运行结果：<br><img src="http://i.imgur.com/5BIVF20.png" alt="">   </p><pre><code>vector&lt;int&gt; ivec{2,5,10,23,12,56,34,100};std::nth_element(ivec.begin(), ivec.begin()+3, ivec.end(), greater&lt;int&gt;());for(auto n : ivec){    cout &lt;&lt; n &lt;&lt; endl}</code></pre><p>运行结果:<br><img src="http://i.imgur.com/aZpaEcc.png" alt=""><br>可以看出，对nth_element的调用与partial_sort几乎一样。在效果上唯一不同之处在于，partial_sort对前20个元素进行了排序，而nth_element没有对他们进行排序。<br>sort、partial_sort、nth_element和stable_sort都属于非稳定的排序算法，有一个名为stable_sort的算法可以提供稳定排序特性。<br>另外，sort、partial_sort、nth_element和stable_sort算法都要求随机访问迭代器，所以这些算法只能被应用于vector、string、deque和数组。对标准关联容器中的元素进行排序并没有实际意义，因为这样的容器总是使用比较函数来维护内部元素的有序性。list是唯一需要排序却无法使用这些排序算法的容器，为此，list特别提供了sort成员函数（稳定排序）。    </p><h3 id="remove算法后调用erase删除元素"><a href="#remove算法后调用erase删除元素" class="headerlink" title="remove算法后调用erase删除元素"></a>remove算法后调用erase删除元素</h3><p>我们需要知道，从容器中删除元素唯一的办法是调用容器的成员函数，几乎总是erase的某种形式。因此，remove并不知道它操作的元素所在的容器，所以remove不可能从容器中删除元素。那么remove究竟做了什么？<br>其实，remove移动了区间中的元素，其结果是，“不用背删除”的元素移到了区间的前部（保持原来的相对顺序）。返回的是一个迭代器指向最后一个“不用背删除”的元素之后的元素。这个返回值相当于该区间“新的逻辑结尾”。但是，需要注意的是，通常来说，当调用了remove以后，从区间中被删除的那些元素可能在也可能不在区间中，这是算法操作的附带结果。<br>所以，如果真想删除元素，那就必须在remove之后使用erase。要删除的元素的位置从“新的逻辑结尾”一直到原区间的结尾。比如:</p><pre><code>vector&lt;int&gt; ivec;...v.erase(remove(v.begin(), v.end(), 99), v.end());    </code></pre><p>此外，remove并不是唯一一个适用于这种情形的算法，其他还有两个属于“remove类”的算法：remove_if和unique。</p><h3 id="使用accumulate或者for-each进行区间统计"><a href="#使用accumulate或者for-each进行区间统计" class="headerlink" title="使用accumulate或者for_each进行区间统计"></a>使用accumulate或者for_each进行区间统计</h3><p>有时候，我们需要按照某种自定义的方式对区间进行统计处理。这个时候，必须自己定义统计方法。STL通过算法accumulate来提供。<br>accumulate有两种形式：第一种形式有两个迭代器和一个初始值，它返回该初始值加上由迭代器标识的区间中的值的总和。如下面的例子：      </p><pre><code>vector&lt;int&gt; ivec{1,2,3};std::accumulate(ivec.begin(), ivec.end(), 0);</code></pre><p>而另一种形式是accumulate带一个初始值和一个任意的统计函数。</p><pre><code>vector&lt;int&gt; ivec{2,2,3};std::accumulate(ivec.begin(), ivec.end(), 2, multiplies&lt;int&gt;());  //24    </code></pre><p>另一个可用来统计区间的算法时for_each，如图accumulate一样，for_each也带两个参数：一个是区间，另一个是函数（通常是函数对象）—对区间中的每个元素都要调用这个函数，但是，传给for_each的这个函数只接受一个实参（即当前的区间元素）；for_each执行完毕后会返回它的函数。</p><h3 id="使用排序的区间作为参数的算法"><a href="#使用排序的区间作为参数的算法" class="headerlink" title="使用排序的区间作为参数的算法"></a>使用排序的区间作为参数的算法</h3><p>有些算法既可以与排序的区间一起工作，也可以与未排序的区间一起工作，但是当它们作用在排序的区间上时，算法会更加有效。<br>首先，罗列出要求排序区间的STL算法:<br>binary_search, lower_bound, upper_bound, equal_range, set_union, set_intersection, set_difference, set_symmetric_difference, merge, inplace_merge, includes<br>还有一些算法并不一定要求排序的区间，但通常情况下会与排序区间一起使用。<br>unique, unique_copy<br>这其中，用于查找的算法binary_search, lower_bound, upper_bound, equal_range要求排序的区间，因为它们用二分法查找数据。<br>set_union, set_intersection, set_difference, set_symmetric_difference这四个算法提供了线性时间效率的集合操作。它们要求排序的区间，因为如果不满足，它们就无法在线性时间内完成工作。<br>merge和inplace_merge实际上实现了合并和排序的联合操作：它们读入两个排序的区间，然后合并成一个新的排序区间。如果源区间没有排过序，就不可能在线性时间内完成。<br>同样，includes也要求两个区间是排序的，承诺线性时间的效率。<br>而unique和unique_copy与上述讨论过的算法有所不同，它们即使对于未排序的区间也有很好的行为。以unique为例，unique用于删除区间中所有重复的元素，所以必须保证所有相等的元素都是连续存放的。因此，总是要确保传给unique的区间是排序的。<br>顺便提一下，unique使用了与remove类似的办法来删除区间中的元素，而并非真正意义上的删除。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;接着上一篇，对STL中非常有用而又容易犯错的一些算法做总结和记录。希望在后面的使用中更加注重STL算法。&lt;br&gt;
    
    </summary>
    
    
      <category term="C++" scheme="http://blog.jonnydu.me/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>使用vector和string时需注意的问题</title>
    <link href="http://blog.jonnydu.me/2016/12/11/Effective-STL-1/"/>
    <id>http://blog.jonnydu.me/2016/12/11/Effective-STL-1/</id>
    <published>2016-12-11T09:39:38.000Z</published>
    <updated>2017-01-03T02:05:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>随着对STL的频繁使用和着迷，越发意识到更深层次理解STL的重要性。使用STL正确得到结果是一回事，而高效地使用STL得到结果又是另一回事。因此，拿起了Scott Meyers大神的《Effective STL》，结合自己平时遇到的问题，作一些总结和记录。<br><a id="more"></a></p><h3 id="使用reserve避免不必要的重新分配"><a href="#使用reserve避免不必要的重新分配" class="headerlink" title="使用reserve避免不必要的重新分配"></a>使用reserve避免不必要的重新分配</h3><p>我们知道，STL容器最大的进步之一是它们会自动增长以便容纳下所放入的数据，只要没有超出它们的最大限制（可以使用max_size()成员函数查看）就可以。vector和string的增长过程是：<br>（1）分配一块大小为当前容量的某个倍数的新内存，在大多数实现中，vector和string的容量每次以2的倍数增长，即每当容器需要扩张时，它们的容量即加倍。<br>（2）把容器的所有元素从旧的内存复制到新的内存中。<br>（3）析构掉旧内存中的对象。<br>（4）释放旧内存。<br>可以看出，这个过程会非常耗时。因此，可以使用reserve成员函数来讲重新分配的次数减少到最低限度，从而避免了重新分配和迭代器/引用失效带来的开销。<br>因此，当一个元素需要被插入而容器的容量不够时，就会发生重新分配过程。比如：创建一个1到1000之间的vector<int>，如果采用这样的方式：</int></p><pre><code>vector&lt;int&gt; ivec;for(int i=1;i&lt;=1000;i++)    ivec.push_back(i);</code></pre><p>对于大多数STL实现，该循环在进行过程中将导致2到10次重新分配。而如果改用reserve。</p><pre><code>vector&lt;int&gt; v;v.reserve(1000);for(int i=1;i&lt;=1000;++i)    v.push_back(i);</code></pre><p>那么，在循环过程中，将不会再发生重新分配。<br>所以，当能确切知道或大致预计容器中最终会有多少元素时，使用reserve。如果不知道，可以先预留足够大的空间，然后，当所有数据都加入以后，再去除多余的容量。   </p><h3 id="把vector和string数据传递给旧的C-API"><a href="#把vector和string数据传递给旧的C-API" class="headerlink" title="把vector和string数据传递给旧的C API"></a>把vector和string数据传递给旧的C API</h3><p>在使用C/C++时，我们发现很多时候还有旧的C API的身影，它们使用数组和char*指针而不是vector或string对象来进行数据交换。因此，在使用STL时，也必须处理好与旧的C API之间的关系。<br>比如，有一个vector v，而需要得到一个指向v中数据的指针，从而可把v中的数据作为数组来对待，那么只需要使用&amp;v[0]就可以了。而对于string s,对应的形式是s.c_str()。<br>所以，对于给定的C API：        </p><pre><code>void doSomething(const int* pInts, size_t numInts);</code></pre><p>可以这样使用：</p><pre><code>if(!v.empty()){    doSomething(&amp;v[0], v.size());}</code></pre><p>请注意，不要使用v.begin()来代替&amp;v[0]，因为begin的返回值是一个迭代器，不是指针；当需要一个指向vector中的数据的指针时，永远不应该使用begin()。（可以使用&amp;*v.begin()）<br>但是，上述方法对string却是不可靠的。原因如下：<br>（1）string中的数据不一定存储在连续的内存中；<br>（2）string的内部表示不一定是以空字符结尾的。<br>所以，对于给定的C API：</p><pre><code>void doSomething(const char* pString)</code></pre><p>可以这样使用： </p><pre><code>doSomething(s.c_str())    </code></pre><h3 id="避免使用vector"><a href="#避免使用vector" class="headerlink" title="避免使用vector"></a>避免使用vector<bool></bool></h3><p>作为一个STL容器，vector<bool>有两点问题。首先，它不是一个STL容器。其次，它并不存储bool。除此之外，一切正常。<br>vector<bool>不是一个STL容器，原因是一个对象要成为STL容器，必须满足的一个条件是，支持operator[]。也就是当使用operator[]取得了容器Container<t>中的一个T对象，那么可以通过取它的地址得到一个指向该对象的指针。<br>因此，如果vector<bool>是一个容器，那么下面这段代码必须可以被编译：</bool></t></bool></bool></p><pre><code>vector&lt;bool&gt; v;bool *p = &amp;v[0];</code></pre><p>但是，它不能通过编译。原因是，vector<bool>是一个假的容器，它并不真的储存bool，相反，为了节省空间，它储存的是bool的紧凑表示。在一个典型的实现中，储存在”vector”中的每个”bool”仅占一个二进制位，一个8位的字节可容纳8个”bool”。在内部，vector<bool>使用了与位域一样的思想，来表示它所存储的那些bool；实际上它只是假装存储了这些bool。<br>位域与bool相似，它只能表示两个可能的值，但是在bool与看似bool的位域之间有一个重要的区别：可以创建一个指向bool的指针，而指向单个位的指针则是不允许的。<br>所以，在上述的实验中，vector<bool>::operator[]需要返回一个指向单个位的引用，而这样的引用却不存在。<br>那么。当需要vector<bool>的时候，应该使用什么呢？<br>标准库提供了两种选择，可以满足绝大多数的需求。第一种是deque<bool>。deque几乎提供了vector所提供的一切，而deque<bool>是一个STL容器，并且确实存储bool。只是需要注意的是，deque中的元素的内存不是连续的，所以不能把deque<bool>中的数据传递给一个期望bool数组的C API。<br>第二种方案时bitset。bitset不是STL容器，但它是标准C++库的一部分。与STL容器不同的是，它的大小（即元素的个数）在编译时就确定了，所以它不支持插入和删除元素。而且，它不支持迭代器。但是，与vector<bool>一样，它使用了一种紧凑表示，只为包含所包含的每个值提高一位空间，它提供了vector<bool>所特有的flip成员函数，以及其他一些特有的、对位的集合有意义的成员函数。在不需要迭代器和动态改变大小的环境下，bitset很适合需要。</bool></bool></bool></bool></bool></bool></bool></bool></bool></p><h3 id="string实现的多样性"><a href="#string实现的多样性" class="headerlink" title="string实现的多样性"></a>string实现的多样性</h3><p>几乎每个string实现都包含如下信息：<br>（1）字符串的大小，即所包含的字符的个数。<br>（2）用于存储该字符串中字符的内存的容量。<br>（3）字符串的值。<br>除此之外，一个string可能还包含：<br>（1）它的分配子的一份拷贝。<br>（2）对值的引用计数。<br>不同的string实现以不同的方式来组织这些信息。下面以4种不同的string实现方式来说明。它们来源于四种STL实现。    </p><h4 id="实现A"><a href="#实现A" class="headerlink" title="实现A"></a>实现A</h4><p>每个string对象包含其分配子的一份拷贝、该字符串的大小、它的容量以及一个指针，该指针指向一块动态分配的内存，其中包含了引用计数和字符串的值。<br><img src="http://i.imgur.com/5ySzRwe.png" alt=""><br>在该实现中，使用默认分配子的string对象是一个指针的4倍。若使用了自定义的分配子，则string对象会更大一些，多出的部分取决于分配子对象的大小。    </p><h4 id="实现B"><a href="#实现B" class="headerlink" title="实现B"></a>实现B</h4><p>在实现B中，string对象与指针大小相同，因为它只包含一个指向结构的指针。B的string所指向的对象中包含了该字符串的大小、容量和引用计数，以及一个指向动态分配的内存的指针，该内存中存放了字符串的值。<br><img src="http://i.imgur.com/NhnOLNi.png" alt="">      </p><h4 id="实现C"><a href="#实现C" class="headerlink" title="实现C"></a>实现C</h4><p>在实现C中，string对象的大小总是与指针的相同，该指针指向一块动态分配的内存，其中包含了与该字符串相关的一切数据：它的大小、容量、引用计数和值。<br><img src="http://i.imgur.com/pdowvMe.png" alt="">    </p><h4 id="实现D"><a href="#实现D" class="headerlink" title="实现D"></a>实现D</h4><p>实现D的string对象是指针大小的7倍（仍然假定使用的是默认的分配子）。这一实现不使用引用计数，但是每个string对象内部包含一块内存，最大可容纳15个字符的字符串。因此，小的字符串可以完整地存放在string对象中，这通常被称为“小字符串优化”特性。而当一个string的容量超过15时，该内存的起始部分被当做一个指向一块动态分配的内存的指针，而该string的值就在这块内存中。<br><img src="http://i.imgur.com/dD1svMe.png" alt=""></p><h4 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h4><p>（1）在以引用计数为基础的设计方案中，string对象之外的一切可以被多个string对象（如果它们有同样的值）。所以，实现A比实现B或C提供了较小的共享能力。尤其是，实现B和C可以共享string的大小和容量，从而减少每个对象存储这些数据的平均开销。<br>（2）创建一个新的字符串值可能需要零次、一次或两次动态分配。      </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;随着对STL的频繁使用和着迷，越发意识到更深层次理解STL的重要性。使用STL正确得到结果是一回事，而高效地使用STL得到结果又是另一回事。因此，拿起了Scott Meyers大神的《Effective STL》，结合自己平时遇到的问题，作一些总结和记录。&lt;br&gt;
    
    </summary>
    
    
      <category term="C++" scheme="http://blog.jonnydu.me/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>《C++编程思想》之字符串学习笔记</title>
    <link href="http://blog.jonnydu.me/2016/12/08/String/"/>
    <id>http://blog.jonnydu.me/2016/12/08/String/</id>
    <published>2016-12-08T13:10:07.000Z</published>
    <updated>2016-12-07T08:33:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>字符串是C++标准库的一个重要组成部分，本文参照《C++编程思想》中字符串的相关内容，对该部分进行简单总结。<br><a id="more"></a></p><h3 id="C字符型数组和C-字符串"><a href="#C字符型数组和C-字符串" class="headerlink" title="C字符型数组和C++字符串"></a>C字符型数组和C++字符串</h3><p>在C语言中，字符串基本上就是字符串数组，并且总是以二进制零（空结束符）作为最末元素。C++ string与它们在C语言中的前身截然不同。首先，也是最重要的不同点，C++隐藏了它所包含的字符序列的物理表示。程序设计人员不必关心数组的维数或空结束符方面的问题。C++ string对象知道自己在内存中的开始位置、包含的内容、包含的字符长度以及在必需重新调整内部缓冲区的大小之前自己可以增长到的最大字符长度。C++字符串极大地减少了C语言编程中最常见且最具破坏性的错误：（1）超越数组边界；（2）通过未被初始化或被赋以错误值的指针来访问数组元素；（3）在释放了某一数组原先所分配的存储单元后仍保留了“悬挂”指针。<br>C++标准没有定义字符串类内存布局的确切实现。采用这种体系结构是为了获得足够的灵活性，从而允许不同的编译器厂商能够提供不同的实现，并且向用户保证提供可预测的行为。</p><h3 id="创建并初始化C-字符串"><a href="#创建并初始化C-字符串" class="headerlink" title="创建并初始化C++字符串"></a>创建并初始化C++字符串</h3><p>创建C++ string一般有以下几种方式：<br>（1）<code>string temp;</code>  创建空string对象，且并不立即用字符数据对其初始化；<br>（2）<code>string temp(&quot;hello world&quot;);</code>  将一个文字的引用数组作为参数传递给构造函数，以此来对一个string对象进行初始化；<br>（3）<code>string temp = &quot;hello world&quot;;</code>  用等号来初始化一个string对象;<br>（4）<code>string temp(str);</code>        用一个string对象看来初始化另一个string对象。<br>另外，还有一些相关的成员函数和特性：<br>（1）用operator+来将不同的初始化数据源结合在一起；</p><pre><code>string s1(&quot;hello&quot;);std::cout &lt;&lt; s1 + &quot; world&quot; &lt;&lt; std::endl;        //hello world</code></pre><p>（2）用string对象的成员函数substr()来创建一个子串。</p><pre><code>string s1(&quot;hello world, hello&quot;);std::cout &lt;&lt; s1.substr(0, 11) &lt;&lt; std::endl;        //hello world   \</code></pre><p>string类对象的成员函数substr()将开始位置作为其第一个参数，而将待选字符的个数作为其第2个参数。<br>（3）将string看做容器对象，利用string类的迭代器string::begin()和string::end()。</p><pre><code>string s1(&quot;hello world&quot;);string s2(s1.begin(), s2.end());std::cout &lt;&lt; s2 &lt;&lt; std::endl;        //hello world</code></pre><p>不可以使用单个的字符、ASCII码或其他整数值来初始化C++字符串。</p><h3 id="字符串的操作"><a href="#字符串的操作" class="headerlink" title="字符串的操作"></a>字符串的操作</h3><p>标准C语言的char型数组工具中存在着其固有的误区，那就是它们都显示地依赖一种假设：字符数组包括一个空结束符。若由于疏忽或是其他差错，这个空结束符被忽略或重写，这个小小的差错就会使C语言的char数组处理函数几乎不可避免地操作其已分配空间之外的内存，有时会带来灾难性后果。<br>C++提供的string类对象，在使用的便利性和安全性上都有很大的提高。     </p><h4 id="追加、插入和连接字符串"><a href="#追加、插入和连接字符串" class="headerlink" title="追加、插入和连接字符串"></a>追加、插入和连接字符串</h4><p>C++字符串一个便于使用的特色是：无需程序员干预，可根据需要自行扩充规模。这样程序员不再需要跟踪字符串的存储边界。此外，当字符串增长时，字符串成员函数append()–(追加)和insert()–(插入)很明显地重新分配了存储空间。<br>此外，还有一些成员函数：capacity()返回当前分配的存储空间的规模；reserve(int size)提供一种优化机制，按照程序员的意图，预留一定数量的存储空间，以便将来使用。而如果要生成的新字符串的规模比当前的字符串大或者需要截断原字符串，resize()函数就会在字符串的末尾追加空格或是指定的字符。</p><h4 id="替换字符串的字符"><a href="#替换字符串的字符" class="headerlink" title="替换字符串的字符"></a>替换字符串的字符</h4><p>insert()使程序员放心地向字符串中插入字符，而不必担心会使存储空间越界，或者会改写插入点之后紧跟的字符。存储空间增大了，原有的字符会该改变其存储位置，以便安置新元素。<br>replace()用于替换（删除）字符串中的字符，如果对replace()的调用使“替换”超出了原有序列的边界，这与追加操作是等价的，这样replace()也会增加存储空间。<br>此外，我们知道，STL中的算法功能很强大，由于string类几乎可以与STL容器等价，因此，string的很多解决方案可以由标准库的算法完成。比如，使用<code>std::replace(s.begin(), s.end(), &#39;X&#39;, &#39;Y&#39;)</code>将字符串中出现的某个字符（’X’）全部用另一个字符(‘Y’)替换掉。这里的replace()是STL通用算法，不是string的成员函数。</p><h4 id="使用非成员重载运算符连接"><a href="#使用非成员重载运算符连接" class="headerlink" title="使用非成员重载运算符连接"></a>使用非成员重载运算符连接</h4><p>C++ string类另一个令人欣喜的特性是可以借助operator+和operator+=来轻易地实现字符串的合并与追加操作。</p><pre><code>string str(&quot;hello&quot;);str += &quot; world&quot;;cout &lt;&lt; str &lt;&lt; endl;        //hello world</code></pre><h3 id="字符串的查找"><a href="#字符串的查找" class="headerlink" title="字符串的查找"></a>字符串的查找</h3><p>string成员函数中的find族是用来在给定字符串中定位某个或某组字符的。包括：<br>（1）find()：在字符串中查找一个指定的单个字符或字符组；<br>（2）find_first_of()：查找第一个与指定字符组任何字符匹配的字符位置；<br>（3）find_first_not_of()：查找第一个与指定字符组任何字符都不匹配的字符位置；<br>（4）find_last_of();<br>（5）find_last_not_of();<br>（6）rfind()：对一个字符串从尾到头查找指定的字符或字符组，返回首次匹配的开始位置。<br>string类中没有改变字符串大小写的函数，可以借助于标准C语言的toupper()和tolower()实现，这两个函数以此只能改变一个字符的大小写。</p><h3 id="字符串的删除"><a href="#字符串的删除" class="headerlink" title="字符串的删除"></a>字符串的删除</h3><p>使用erase()成员函数删除字符串中的字符，该函数有两个参数：一个参数表示开始删除字符的位置（默认值是0）；另一个表示要删除多少个字符（默认值是string::npos）。如果指定删除的字符个数比字符串中剩余的字符还多，那么剩余的字符将全部删除，所以，调用不含参数的erase()函数将删除字符串中的所有字符。</p><h3 id="字符串的比较"><a href="#字符串的比较" class="headerlink" title="字符串的比较"></a>字符串的比较</h3><p>字符串的比较，简而言之就是：当两个字符串比较遇到第一对不同的字符时，字符串s1中第一个不同的字符比字符串s2中同样位置的字符在ASCII表中的位置更靠后，那么s1“大于”s2。<br>C++提供了多种字符串比较方法，它们各具特色。其中最简单的就是使用非成员的重载运算符函数：operator==、operator!=、operator&gt;、operator&lt;、operator&gt;=、operator&lt;=。<br>此外，string类还提供compare()成员函数，用以提供更复杂紧密的比较手段。用多种重载版本来对比较的字串做控制。<br>C语言中获取字符串中的某个字符都是采用数组索引法（[]）,C++ string提供一种s[n]表示法的替代方法：at()成员函数。如果不出异常，这两种索引机制在C++中产生的结果是一样的。而有异常的情况时，比如程序员引用了一个超过边界的数组元素，at()将会友好地抛出一个异常，而普通的[]下标语法将让程序员自行决策。at()成员函数抛出的是一个out_of_range类对象，它(最终)派生于std::exception。程序可在一个异常处理器中捕获该对象，并采取适当的补救措施，比如重新计算越界下标或扩充数组。而string::operator<a href=""></a>不会有那样的保护性，它的危险性等同于C语言中国对char型数组的处理。    </p><h3 id="C语言函数处理时的转换"><a href="#C语言函数处理时的转换" class="headerlink" title="C语言函数处理时的转换"></a>C语言函数处理时的转换</h3><p>C++ string提供了c_str()函数返回一个const char*，它指向一个C语言风格的具有“空结束符”的字符串，此字符串与string对象的内容等价。当想将一个字符串传送给一个标准C语言函数时，比如atoi()、printf()，const char*可派上用场。不过，用c_str()的返回值作为非const参数应用于任一函数都是错误的。    </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;字符串是C++标准库的一个重要组成部分，本文参照《C++编程思想》中字符串的相关内容，对该部分进行简单总结。&lt;br&gt;
    
    </summary>
    
    
      <category term="C++" scheme="http://blog.jonnydu.me/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>Redis两种持久化方式比较</title>
    <link href="http://blog.jonnydu.me/2016/12/03/Redis-RDB-AOF/"/>
    <id>http://blog.jonnydu.me/2016/12/03/Redis-RDB-AOF/</id>
    <published>2016-12-03T09:14:03.000Z</published>
    <updated>2017-03-27T07:02:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文对前面所述的Redis两种持久化方式RDB和AOF做简单总结和比较。<br><a id="more"></a>  </p><h3 id="RDB和AOF简单总结"><a href="#RDB和AOF简单总结" class="headerlink" title="RDB和AOF简单总结"></a>RDB和AOF简单总结</h3><p>RDB持久化可以在指定的时间间隔内生成数据集的时间点快照，保存的是该时间点Redis在内存中的数据库状态。<br>AOF持久化记录的是服务器执行的所有写操作命令，并在服务器启动时，通过重新执行这些命令来还原数据集。AOF文件中的命令全部以Redis协议的格式来保存，新命令会被追加到文件的末尾。Redis还可以在后台对AOF文件进行重写，使得AOF文件的体积不会超出保存数据集状态所需的实际大小。Redis可以同时使用AOF持久化和RDB持久化，在这种情况下，当Redis重启下，它会优先使用AOF来还原数据集，因为AOF文件保存的数据集通常比RDB文件所保存的数据集更完整。           </p><h3 id="RDB"><a href="#RDB" class="headerlink" title="RDB"></a>RDB</h3><h4 id="RDB的优点"><a href="#RDB的优点" class="headerlink" title="RDB的优点"></a>RDB的优点</h4><p>RDB是一个非常紧凑的文件，它保存了Redis在某个时间点上的数据集，这种文件非常适合进行备份。比如，可以在一天内，每小时备份RDB文件，并在每月的每一天，也备份一个RDB文件。这样的话，即使遇上问题，也可以随时将数据集还原到不同的版本。RDB非常适用于灾难恢复，它只有一个文件，并且内容都非常紧凑，可以（在加密后）将它传送到别的数据中心。RDB可以最大化Redis的性能：父进程在保存RDB文件时唯一要做的就是fork出一个子进程，然后这个子进程就会处理接下来的所有保存工作，父进程无须执行任何磁盘I/O操作。RDB在恢复大数据集时的速度比AOF的恢复速度要快。          </p><h4 id="RDB的缺点"><a href="#RDB的缺点" class="headerlink" title="RDB的缺点"></a>RDB的缺点</h4><p>如果需要尽量避免在服务器故障时丢失数据，那么RDB不适合。因为，虽然Redis允许设置不同的保存点来控制保存RDB文件的频率，但是，因为RDB文件需要保存整个数据集的状态，所以它并不是一个轻松的操作。因此，可能会至少需要5分钟才保存一次RDB文件。这种情况下，一旦出现故障停机，那么便会丢失几分钟的数据。此外，每次保存RDB的时候，Redis都要fork出一个子进程，并由子进程来进行实际的持久化工作。当数据集比较庞大的时候，fork过程会非常耗时，造成服务器端在短时间内停止处理客户端。虽然AOF重写也需要进行fork，但无论fork重写的执行间隔有多长，数据的耐久性都不会有任何损失。</p><h3 id="AOF"><a href="#AOF" class="headerlink" title="AOF"></a>AOF</h3><h4 id="AOF的优点"><a href="#AOF的优点" class="headerlink" title="AOF的优点"></a>AOF的优点</h4><p>使用AOF持久化会让Redis变得非常耐久，AOF支持设置不同的fsync策略，比如每秒钟fsync、每次执行写命令时fsync。AOF的默认策略为每秒钟fsync一次，在这种配置下，Redis仍然可以保持良好的性能，并且就算发生故障停机，也最多只会丢失一秒钟的数据（fsync会在后台进程执行，所以主进程可以继续处理命令请求）。另外，AOF文件是一个只进行追加操作的日志文件，因此对AOF文件的写入不需要进行seek，即使日志因为某些原因而包含了未写入完整的命令（比如写入时磁盘已满，写入中途停机），redis-check-aof工具也可以轻易地修复这种问题。Redis可以在AOF文件体积变得过大时，自动地在后台对AOF进行重写，重写后的新AOF文件包含了恢复当前数据集所需的最小命令集合。整个重写操作是绝对安全的，因为Redis在创建AOF文件的过程中，会继续将命令追加到现有的AOF文件里面，即使重写过程中发生停机，现有的AOF文件也不会丢失。而一旦新AOF文件创建完毕，Redis就会从旧AOF文件切换到新AOF文件，并开始对新AOF文件进行追加操作。AOF文件有序地保存了对数据库执行的所有写操作，这些写入操作以Redis协议的格式保存，因此AOF文件的内容非常容易被人读懂，对文件进行分析也很轻松。比如，如果不小心执行了FLUSHALL命令，但只要AOF文件未被重写，那么只要停止服务器，移除AOF文件末尾的FLUSHALL命令，并重启Redis，就可以将数据集恢复到执行FLUSHALL之前的状态。           </p><h4 id="AOF的缺点"><a href="#AOF的缺点" class="headerlink" title="AOF的缺点"></a>AOF的缺点</h4><p>对同样的数据集，AOF文件通常要大于等价的RDB文件。<br>AOF可能比RDB慢，这取决于具体的fsync策略。<br>另外，Redis AOF是通过递增地更新一个已经存在的状态，而RDB快照则是一次又一次地从头开始创造一切，概念上更健壮。</p><h3 id="二者的使用"><a href="#二者的使用" class="headerlink" title="二者的使用"></a>二者的使用</h3><p>一般来说，如果想达到很高的数据安全性，那么应该同时使用两种持久化功能。如果可以承受数分钟以内的数据丢失，那么可以只使用RDB持久化。但是不建议只是用AOF持久化：因为定时生成RDB快照非常便于进行数据库备份，并且RDB恢复数据集的速度也要比AOF恢复的速度要快。</p><h3 id="其他一些问题"><a href="#其他一些问题" class="headerlink" title="其他一些问题"></a>其他一些问题</h3><h4 id="RDB设置"><a href="#RDB设置" class="headerlink" title="RDB设置"></a>RDB设置</h4><p>在默认情况下，Redis将数据库快照保存dump.rdb的二进制文件中。可以对Redis进行设置，让它在“N秒内数据集至少有M个改动”这样的条件满足时，自动保存一次数据集。此外，还可以使用SAVE或BGSAVE手动对Redis进行数据集保存操作。</p><h4 id="AOF文件出错"><a href="#AOF文件出错" class="headerlink" title="AOF文件出错"></a>AOF文件出错</h4><p>服务器可能在程序正在对AOF文件进行写入时停机，如果停机造成了AOF文件出错，那么Redis在重启时会拒绝载入这个AOF文件，从而确保数据的一致性不会被破坏。<br>当发生这种情况时，可以为现有的AOF文件创建一个备份，然后使用Redis附带的redis-check-aof程序，对原来的AOF文件进行修复，并查看两个文件的不同之处，最后重启Redis服务器，等待服务器载入修复后的AOF文件，并进行数据恢复。</p><h4 id="RDB和AOF之间的相互作用"><a href="#RDB和AOF之间的相互作用" class="headerlink" title="RDB和AOF之间的相互作用"></a>RDB和AOF之间的相互作用</h4><p>在版本大于等于2.4的Redis中，BGSAVE执行的过程中，不可以执行BGREWRITEAOF。同样，在执行BGREWRITEAOF的过程中，也不可执行BGSAVE。这样主要是防止两个Redis后台进程同时对磁盘进行大量的I/O操作。<br>如果BGSAVE正在执行，并且用户显示地调用BGREWRITEAOF命令， 那么服务器将向用户回复一个OK状态，并告知用户，BGREWRITEAOF已经被预定执行：一旦BGSAVE执行完毕，BGREWRITEAOF就会正式开始。<br>当Redis启动时，如果RDB持久化和AOF持久化都打开了，那么程序会优先使用AOF文件来恢复数据集，因为AOF文件所保存的数据通常是最完整的。</p><h4 id="RDB备份数据"><a href="#RDB备份数据" class="headerlink" title="RDB备份数据"></a>RDB备份数据</h4><p>Redis对于数据备份是非常友好的，可以在服务器运行的时候对 RDB 文件进行复制：RDB文件一旦被创建，就不会进行任何修改。当服务器要创建一个新的RDB文件时，它先将文件的内容保存在一个临时文件里，当临时文件写入完毕时，程序才原子地使用临时文件替换原来的RDB文件。即，无论何时，复制RDB文件都是绝对安全的。</p><p>参考： <a href="https://segmentfault.com/a/1190000005052628" target="_blank" rel="noopener">Redis持久化</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文对前面所述的Redis两种持久化方式RDB和AOF做简单总结和比较。&lt;br&gt;
    
    </summary>
    
    
      <category term="Redis" scheme="http://blog.jonnydu.me/tags/Redis/"/>
    
  </entry>
  
</feed>
